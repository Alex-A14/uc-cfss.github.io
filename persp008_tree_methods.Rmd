---
title: "Statistical learning: tree-based methods"
author: "MACS 30100 - Perspectives on Computational Modeling"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Objectives

* Define a decision tree
* Identify the steps to estimating a decision tree
* Demonstrate how to estimate a decision tree for regression and classification problems
* Define and estimate bagging models
* Define and estimate random forest models
* Define and estimate boosting models

```{r packages, cache = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(modelr)
library(broom)
library(tree)
library(stringr)
library(ISLR)
library(gridExtra)
library(grid)
library(titanic)
library(rcfss)
# to get the tree graphs with the labels and values, use the forked
# version of ggdendro
# devtools::install_github("bensoltoff/ggdendro")
library(ggdendro)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())
```

# Decision trees

![](https://s-media-cache-ak0.pinimg.com/originals/7a/89/ff/7a89ff67b4ce34204c23135cbf35acfa.jpg)

![](https://eight2late.files.wordpress.com/2016/02/7214525854_733237dd83_z1.jpg?w=700)

![](https://s-media-cache-ak0.pinimg.com/564x/0b/87/df/0b87df1a54474716384f8ec94b52eab9.jpg)

**Decision trees** are intuitive concepts for making decisions. They are also useful methods for regression and classification. They work by splitting the observations into a number of regions, and predictions are made based on the mean or mode of the training observations in that region.

## Regression trees

### Single predictor

Let's first consider a basic linear regression model of the relationship between horsepower and highway mileage from the `Auto` dataset.

```{r auto-lm}
# add 95% confidence intervals to fitted values from augment()
add_ci <- function(df_augment) {
  df_augment %>%
    mutate(.fitted.low = .fitted - 1.96 * .se.fit,
           .fitted.high = .fitted + 1.96 * .se.fit)
}

# draw 95% confidence interval plot using results of add_ci()
plot_ci <- function(df_ci, x){
  ggplot(df_ci, aes_string(x, ".fitted")) +
  geom_line() +
  geom_line(aes(y = .fitted.low), linetype = 2) +
  geom_line(aes(y = .fitted.high), linetype = 2)
}

auto_lm <- glm(mpg ~ horsepower, data = Auto)

augment(auto_lm, newdata = data_grid(Auto, horsepower)) %>%
  add_ci() %>%
  plot_ci("horsepower") +
  geom_point(data = Auto, aes(y = mpg), alpha = .2) +
  labs(title = "Linear model of highway mileage",
       x = "Horsepower",
       y = "Highway mileage")
```

As we recall, a strictly linear model is a poor fit for the data since the relationship actually appears to be quadratic. But unless we [relax our linear assumption](persp007_nonlinear.html), this is the best OLS model we can estimate.

Let's compare this instead to a decision tree using horsepower to predict highway mileage. Decision trees work through a process of **stratification**:

1. Divide the predictor space ($X_1, X_2, \dots, X_p$) into $J$ distinct and non-overlapping regions $R_1, R_2, \dots, R_J$.
1. For every observation in region $R_j$, we make the same prediction which is the mean of the response variable $Y$ for all observations in $R_j$.

This process is iterative: during the first iteration, we segment the predictor space $X$ into two regions $R_1, R_2$. In the context of a decision tree with a single predictor, that process results in decision trees like the following:

```{r part-tree-data}
# hackish function to get line segment coordinates for ggplot
partition.tree.data <- function (tree, label = "yval", add = FALSE, ordvars, ...) 
{
  ptXlines <- function(x, v, xrange, xcoord = NULL, ycoord = NULL, 
                       tvar, i = 1L) {
    if (v[i] == "<leaf>") {
      y1 <- (xrange[1L] + xrange[3L])/2
      y2 <- (xrange[2L] + xrange[4L])/2
      return(list(xcoord = xcoord, ycoord = c(ycoord, y1, 
                                              y2), i = i))
    }
    if (v[i] == tvar[1L]) {
      xcoord <- c(xcoord, x[i], xrange[2L], x[i], xrange[4L])
      xr <- xrange
      xr[3L] <- x[i]
      ll2 <- Recall(x, v, xr, xcoord, ycoord, tvar, i + 
                      1L)
      xr <- xrange
      xr[1L] <- x[i]
      return(Recall(x, v, xr, ll2$xcoord, ll2$ycoord, tvar, 
                    ll2$i + 1L))
    }
    else if (v[i] == tvar[2L]) {
      xcoord <- c(xcoord, xrange[1L], x[i], xrange[3L], 
                  x[i])
      xr <- xrange
      xr[4L] <- x[i]
      ll2 <- Recall(x, v, xr, xcoord, ycoord, tvar, i + 
                      1L)
      xr <- xrange
      xr[2L] <- x[i]
      return(Recall(x, v, xr, ll2$xcoord, ll2$ycoord, tvar, 
                    ll2$i + 1L))
    }
    else stop("wrong variable numbers in tree.")
  }
  if (inherits(tree, "singlenode")) 
    stop("cannot plot singlenode tree")
  if (!inherits(tree, "tree")) 
    stop("not legitimate tree")
  frame <- tree$frame
  leaves <- frame$var == "<leaf>"
  var <- unique(as.character(frame$var[!leaves]))
  if (length(var) > 2L || length(var) < 1L) 
    stop("tree can only have one or two predictors")
  nlevels <- sapply(attr(tree, "xlevels"), length)
  if (any(nlevels[var] > 0L)) 
    stop("tree can only have continuous predictors")
  x <- rep(NA, length(leaves))
  x[!leaves] <- as.double(substring(frame$splits[!leaves, "cutleft"], 
                                    2L, 100L))
  m <- model.frame(tree)
  if (length(var) == 1L) {
    x <- sort(c(range(m[[var]]), x[!leaves]))
    if (is.null(attr(tree, "ylevels"))) 
      y <- frame$yval[leaves]
    else y <- frame$yprob[, 1L]
    y <- c(y, y[length(y)])
    if (add) {
      # lines(x, y, type = "s", ...)
    }
    else {
      a <- attributes(attr(m, "terms"))
      yvar <- as.character(a$variables[1 + a$response])
      xo <- m[[yvar]]
      if (is.factor(xo)) 
        ylim <- c(0, 1)
      else ylim <- range(xo)
      # plot(x, y, ylab = yvar, xlab = var, type = "s", ylim = ylim,
      #      xaxs = "i", ...)
    }
    data_frame(x = x, y = y)
  }
  else {
    if (!missing(ordvars)) {
      ind <- match(var, ordvars)
      if (any(is.na(ind))) 
        stop("unmatched names in vars")
      var <- ordvars[sort(ind)]
    }
    lab <- frame$yval[leaves]
    if (is.null(frame$yprob)) 
      lab <- format(signif(lab, 3L))
    else if (match(label, attr(tree, "ylevels"), nomatch = 0L)) 
      lab <- format(signif(frame$yprob[leaves, label], 
                           3L))
    rx <- range(m[[var[1L]]])
    rx <- rx + c(-0.025, 0.025) * diff(rx)
    rz <- range(m[[var[2L]]])
    rz <- rz + c(-0.025, 0.025) * diff(rz)
    xrange <- c(rx, rz)[c(1, 3, 2, 4)]
    xcoord <- NULL
    ycoord <- NULL
    xy <- ptXlines(x, frame$var, xrange, xcoord, ycoord, 
                   var)
    xx <- matrix(xy$xcoord, nrow = 4L)
    yy <- matrix(xy$ycoord, nrow = 2L)

    return(list(data_frame(xmin = xx[1L,],
                           ymin = xx[2L,],
                           xmax = xx[3L,],
                           ymax = xx[4L,]),
                data_frame(x = yy[1L,],
                           y = yy[2L,],
                           label = lab)))
    # if (!add) 
    #   plot(rx, rz, xlab = var[1L], ylab = var[2L], type = "n", 
    #        xaxs = "i", yaxs = "i", ...)
    # segments(xx[1L, ], xx[2L, ], xx[3L, ], xx[4L, ])
    # text(yy[1L, ], yy[2L, ], as.character(lab), ...)
  }
}
```

```{r auto-tree2}
# estimate model
auto_tree <- tree(mpg ~ horsepower, data = Auto,
     control = tree.control(nobs = nrow(Auto),
                            mindev = 0))

mod <- prune.tree(auto_tree, best = 2)

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(alpha = .2) +
  geom_step(data = partition.tree.data(mod), aes(x, y), size = 1.5)

# display plots side by side
grid.arrange(ptree, preg, ncol = 2)
```

On the left is the decision tree after the first iteration, and on the right is the decision tree estimation of the relationship between horsepower and highway mileage. The tree consists of three different components:

* Each outcome (survived or died) is a **terminal node** or a **leaf**
* Splits occur at **internal nodes**
* The segments connecting each node are called **branches**

This model has two terminal nodes (`r mod$frame$yval[[2]]` and `r mod$frame$yval[[3]]`), one internal node (`horsepower` $`r mod$frame$splits[1,1]`$), and two branches. For observations with horsepower $`r mod$frame$splits[1,1]`$, the model estimates highway mileage of `r mod$frame$yval[[2]]`. For observations with horsepower $`r mod$frame$splits[1,2]`$, the model estimates highway mileage of `r mod$frame$yval[[3]]`. The resulting relationship "curve" (see right) looks like a step function. Each segment of the function is the `mean()` of the observations inside that region.

If we proceed to the next iteration, the decision tree segments $R_1$ further.

```{r auto-tree3}
mod <- prune.tree(auto_tree, best = 3)

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(alpha = .2) +
  geom_step(data = partition.tree.data(mod), aes(x, y), size = 1.5)

# display plots side by side
grid.arrange(ptree, preg, ncol = 2)
```

Now there are three terminal nodes (`r mod$frame$yval[3:5]`), two internal nodes (`horsepower` $`r mod$frame$splits[1,1]`$ and `horsepower` $`r mod$frame$splits[2,1]`$), and three branches. Interpreting the decision tree is still relatively intuitive:

* If `horsepower` $`r mod$frame$splits[1,2]`$, then the model estimates highway mileage to be `r mod$frame$yval[5]`.
* If `horsepower` $`r mod$frame$splits[1,1]`$, then we proceed down the left branch to the next internal node.
    * If `horsepower` $`r mod$frame$splits[2,1]`$, then the model estimates highway mileage to be `r mod$frame$yval[3]`.
    * If `horsepower` $`r mod$frame$splits[2,2]`$, then the model estimates highway mileage to be `r mod$frame$yval[4]`.

If we continued the iterative process many many times, we'd get a decision tree that looks like this:

```{r auto-treeall}
mod <- auto_tree

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(horsepower, mpg)) +
  geom_point(alpha = .2) +
  geom_step(data = partition.tree.data(mod), aes(x, y), size = 1.5) +
  geom_smooth(data = partition.tree.data(mod), aes(x, y), se = FALSE)

# display plots side by side
grid.arrange(ptree, preg, ncol = 2)
```

There are `r nrow(mod$frame)` nodes in this decision tree, with `r nrow(mod$frame)` different regions and `r nrow(mod$frame)` different predicted values depending on the observation's value for horsepower. Notice though that the step function actually looks similar to a quadratic smoothing line, matching our expectations of the relationship. In fact, compared to the linear model (`r mse(auto_lm, Auto)`) the decision tree generates a far lower training MSE (`r mse(auto_tree, Auto)`).^[Yes, we know [the pitfalls of using training MSE for model comparison](persp006_resampling.html#training_vs_test_data). It's just an example because we haven't split the data into a validation set.]

### Multiple predictors

With just a single predictor, the regions are a function of that one predictor. If we add a second predictor (say, vehicle weight), the regions become a function of **both** predictors and can be visualized as grids or boxes.

```{r auto-tree-weight}
auto_tree <- tree(mpg ~ horsepower + weight, data = Auto,
     control = tree.control(nobs = nrow(Auto),
                            mindev = 0))

mod <- prune.tree(auto_tree, best = 3)

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(weight, horsepower)) +
  geom_point(alpha = .2) +
  geom_segment(data = partition.tree.data(mod)[[1]],
               aes(x = xmin, xend = xmax, y = ymin, yend = ymax)) +
  geom_text(data = partition.tree.data(mod)[[2]],
            aes(x = x, y = y, label = label)) +
  coord_cartesian(xlim = c(min(Auto$weight), max(Auto$weight)),
                  ylim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
grid.arrange(ptree, preg, ncol = 2,
             top = textGrob(str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)),
                            gp = gpar(fontsize = 20)))
```

* If `weight` $`r mod$frame$splits[1,2]`$, then the model estimates highway mileage to be approximately `r mod$frame$yval[5]`.
* If `weight` $`r mod$frame$splits[1,1]`$, then we proceed down the left branch to the next internal node.
    * If `horsepower` $`r mod$frame$splits[2,1]`$, then the model estimates highway mileage to be `r mod$frame$yval[3]`.
    * If `horsepower` $`r mod$frame$splits[2,2]`$, then the model estimates highway mileage to be `r mod$frame$yval[4]`.

We can continue to build the tree up by adding additional nodes:

```{r auto-tree-weight-i}
for(i in c(4:10, 20, 50)){
  mod <- prune.tree(auto_tree, best = i)
  
  # plot tree
  tree_data <- dendro_data(mod)
  ptree <- ggplot(segment(tree_data)) +
    geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
                 alpha = 0.5) +
    geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
    geom_text(data = leaf_label(tree_data), 
              aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
    theme_dendro()
  
  # plot region space
  preg <- ggplot(Auto, aes(weight, horsepower)) +
    geom_point(alpha = .2) +
    geom_segment(data = partition.tree.data(mod)[[1]],
                 aes(x = xmin, xend = xmax, y = ymin, yend = ymax)) +
    geom_text(data = partition.tree.data(mod)[[2]],
              aes(x = x, y = y, label = label)) +
    coord_cartesian(xlim = c(min(Auto$weight), max(Auto$weight)),
                    ylim = c(min(Auto$horsepower), max(Auto$horsepower)),
                    expand = FALSE) +
    theme(panel.border = element_rect(fill = NA, size = 1))
  
  # display plots side by side
  grid.arrange(ptree, preg, ncol = 2,
               top = textGrob(str_c("Terminal Nodes = ", i),
                              gp = gpar(fontsize = 20))) 
}
```

### Estimation procedure

We have already identified that decision trees use stratification to divide the observations into $R_J$ regions. Like in linear regression, our goal is to minimize the residual sum of the squared errors (RSS), defined for a decision tree as:

$$\sum_{j=1}^J \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2$$

where $\hat{y}_{R_j}$ is the mean response for the observations in the $j$th region. In order to do this, decision trees implement a **recursive binary strategy**. The process begins at the top of the tree (**top-down**) and successively splits the data into a new region. This split generates two new branches in the tree. Rather than looking forward to select the optimal split among all future possibilities, this approach is **greedy** in that it selects the best split **at that particular step**. Given all the potential splits that could be performed on one of the predictors $X_1, X_2, \dots, X_p$ predictors, the algorithm assigns a cutpoint $s$ that splits the data in the manner that reduces the RSS by the largest amount. As the number of predictors $p$ and observations $N$ increases, the more potential cutpoints the algorithm must consider. However even with relatively large numbers of predictors and observations, the computational process is quite efficient.

This process continues until some designated stopping criteria is reached, otherwise it could continue until each training observation is sorted into its own node (i.e. overfitting). For example, the `tree()` function in R (from the `tree` library) will not split a node if it would contain fewer than 10 training observations. Once this iterative process stops, we can generate predicted values for the response of a given test observation by calculating the mean of the training observations for the region in which the test observation belongs.

### Pruning the tree

Notice that we stop splitting the tree in order to prevent overfitting. Even with the above process, decision trees are highly susceptible to overfitting due to its natural complexity. And if we simply set the stopping criteria at a higher level, we may miss crucial branches later on in the process. Instead we want a method that allows us to grow a large tree and preserve the most important branches or elements.

![](https://growingtogether.areavoices.com/files/2015/11/pruning.jpg)

In essence, we want to **prune** the tree. **Cost complexity pruning** is one predominant method for achieving this goal. While I leave the mathematics of this operation to [ISL](https://link-springer-com.proxy.uchicago.edu/book/10.1007%2F978-1-4614-7138-7), cost complexity pruning uses a **tuning parameter** to selectively prune or snip branches that do not contribute significant predictive accuracy, resulting in a subtree generated from the full tree. Different tuning parameter values will lead to different trade-offs between model complexity and model accuracy, so functions such as `prune.tree()` use [$k$-fold cross-validation](persp006_resampling.html#k-fold_cross-validation) to select a cost complexity parameter that optimally balances the trade-off for the specific dataset.

For example, here is the full tree grown for the horsepower + weight decision tree using the default control parameters for `tree()`:

```{r auto-tree-default}
auto_tree <- tree(mpg ~ horsepower + weight, data = Auto,
     control = tree.control(nobs = nrow(Auto),
                            mindev = 0))
mod <- auto_tree

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(weight, horsepower)) +
  geom_point(alpha = .2) +
  geom_segment(data = partition.tree.data(mod)[[1]],
               aes(x = xmin, xend = xmax, y = ymin, yend = ymax)) +
  geom_text(data = partition.tree.data(mod)[[2]],
            aes(x = x, y = y, label = label)) +
  coord_cartesian(xlim = c(min(Auto$weight), max(Auto$weight)),
                  ylim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
grid.arrange(ptree, preg, ncol = 2,
             top = textGrob(str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)),
                            gp = gpar(fontsize = 20)))
```

Let's use $10$-fold CV to select the optimal tree size:

```{r auto-tree-default-prune}
# generate 10-fold CV trees
auto_cv <- crossv_kfold(Auto, k = 10) %>%
  mutate(tree = map(train, ~ tree(mpg ~ horsepower + weight, data = .,
     control = tree.control(nobs = nrow(Auto),
                            mindev = 0))))

# calculate each possible prune result for each fold
auto_cv <- expand.grid(auto_cv$.id, 2:10) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(.id = Var1,
         k = Var2) %>%
  left_join(auto_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.tree(.x, best = .y)),
         mse = map2_dbl(prune, test, mse))

auto_cv %>%
  select(k, mse) %>%
  group_by(k) %>%
  summarize(test_mse = mean(mse),
            sd = sd(mse, na.rm = TRUE)) %>%
  ggplot(aes(k, test_mse)) +
  geom_point() +
  geom_line() +
  labs(x = "Number of terminal nodes",
       y = "Test MSE")
```

The minimum cross-validated test MSE is for 7 terminal nodes. Here's what that tree looks like:

```{r auto-tree-7}
mod <- prune.tree(auto_tree, best = 7)

# plot tree
tree_data <- dendro_data(mod)
ptree <- ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro()

# plot region space
preg <- ggplot(Auto, aes(weight, horsepower)) +
  geom_point(alpha = .2) +
  geom_segment(data = partition.tree.data(mod)[[1]],
               aes(x = xmin, xend = xmax, y = ymin, yend = ymax)) +
  geom_text(data = partition.tree.data(mod)[[2]],
            aes(x = x, y = y, label = label)) +
  coord_cartesian(xlim = c(min(Auto$weight), max(Auto$weight)),
                  ylim = c(min(Auto$horsepower), max(Auto$horsepower)),
                  expand = FALSE) +
  theme(panel.border = element_rect(fill = NA, size = 1))

# display plots side by side
grid.arrange(ptree, preg, ncol = 2,
             top = textGrob(str_c("Terminal Nodes = ", ceiling(length(mod$frame$yval) / 2)),
                            gp = gpar(fontsize = 20)))
```

## Classification trees

A **classification tree** is similar to a regression tree, except that the response variable is qualitative. In making predictions, we would predict for a test set observation the most commonly occurring class value in the given region. However we will also consider the **class proportions**, or the proportion of training observations in the region $R_j$ that fall into a given class.

Rather than using RSS to grow the tree, we have three options for minimizing error. An obvious choice might be the **classification error rate**, or the proportion of training observations in a given region that do not belong to the most common class:

$$E = 1 - \max_{k}(\hat{p}_{mk})$$

where $\hat{p}_{mk}$ is the proportion of training observations in region $m$ that do not belong to the most common class $k$.

In practice, two other methods grow better and more accurate trees. The **Gini index** is defined as:

$$G = \sum_{k = 1}^k \hat{p}_{mk} (1 - \hat{p}_{mk})$$

and is a measure of node **purity**. The higher the proportion of observations belonging to a single class, the closer this value will be to 0.

The alternative is **cross-entropy**:

$$D = - \sum_{k = 1}^K \hat{p}_{mk} \log(\hat{p}_{mk})$$

As more observations are closer to or near 0 or 1, cross-entropy will shrink towards zero. So for classification trees, each split can be evaluated using one of these criteria, though again it is typically the Gini index or cross-entropy.^[`tree()` uses the Gini index for classification trees.]

Let's return to our running Titanic example. I want to predict who lives and who dies during this event. Instead of using [logistic regression](persp004_logistic_regression.html), I'm going to calculate a decision tree based on a passenger's age and gender. Here's what that decision tree looks like:

```{r titanic_tree}
titanic <- titanic_train %>%
  as_tibble() %>%
  mutate(Survived = factor(Survived, levels = 0:1, labels = c("Died", "Survived")),
         Female = factor(Sex, levels = c("male", "female")))

# estimate model
titanic_tree <- tree(Survived ~ Age + Female, data = titanic,
                     control = tree.control(nobs = nrow(titanic),
                            mindev = .001))

# plot unpruned tree
mod <- titanic_tree

tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Titanic survival tree",
       subtitle = "Age + Gender")
```

```{r titanic-tree-prune, dependson="titanic-tree"}
mse.tree <- function (model, data){
  residuals.glm <- function(model, data) {
    modelr:::response(model, data) - stats::predict(model, data, type = "response")
  }

  x <- residuals(model, data, type = "usual")
  mean(x^2, na.rm = TRUE)
}

# generate 10-fold CV trees
titanic_cv <- titanic %>%
  na.omit() %>%
  crossv_kfold(k = 10) %>%
  mutate(tree = map(train, ~ tree(Survived ~ Age + Female, data = .,
     control = tree.control(nobs = nrow(titanic),
                            mindev = .001))))

# calculate each possible prune result for each fold
titanic_cv <- expand.grid(titanic_cv$.id,
                          seq(from = 2, to = ceiling(length(mod$frame$yval) / 2))) %>%
  as_tibble() %>%
  mutate(Var2 = as.numeric(Var2)) %>%
  rename(.id = Var1,
         k = Var2) %>%
  left_join(titanic_cv) %>%
  mutate(prune = map2(tree, k, ~ prune.misclass(.x, best = .y)),
         mse = map2_dbl(prune, test, mse.tree))

titanic_cv %>%
  group_by(k) %>%
  summarize(test_mse = mean(mse),
            sd = sd(mse, na.rm = TRUE)) %>%
  ggplot(aes(k, test_mse)) +
  geom_point() +
  geom_line() +
  labs(title = "Titanic survival tree",
       subtitle = "Age + Gender",
       x = "Number of terminal nodes",
       y = "Test error rate")
```

Here I select 7 as the optimal number of nodes; while the error rate continues to decline, there are diminishing marginal returns.

```{r titanic-tree-7, dependson="titanic-tree"}
mod <- prune.tree(titanic_tree, best = 10)

tree_data <- dendro_data(mod)
ggplot(segment(tree_data)) +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend), 
               alpha = 0.5) +
  geom_text(data = label(tree_data), 
            aes(x = x, y = y, label = label_full), vjust = -0.5, size = 3) +
  geom_text(data = leaf_label(tree_data), 
            aes(x = x, y = y, label = label), vjust = 0.5, size = 3) +
  theme_dendro() +
  labs(title = "Titanic survival tree",
       subtitle = "Age + Gender")
```

Notice that some branches split and lead to the same outcome. For instance, the bottom-left branch assigns individuals with an age both greater than and less than $.96$ to `Survived`. This is because splitting the node leads to increased **node purity** where we are even more confident in our predictions. Think about it. Here are the outcomes in the training observations for females less than 13 years old:

```{r titanic-f-13}
titanic %>%
  filter(Female == "female", Age < 13) %>%
  count(Survived)
```

We would predict for all of these observations that the individual survived, being incorrect 13 times. What happens if we split this subset even further?

```{r titanic-f-13-split}
titanic %>%
  filter(Female == "female", Age < 13) %>%
  mutate(age96 = Age < .96) %>%
  count(age96, Survived) %>%
  complete(age96, Survived, fill = list(n = 0)) %>%
  knitr::kable(col.names = c("Less than .96 years old", "Outcome", "Number of training observations"))
```

For individuals less than $.96$ years old, the decision tree prediction achieves complete node purity - it perfectly predicts all of the training observations as survivors. If we had a test observation for a female infant (i.e. less than a year old), we'd be more confident in our prediction than if we had terminated the node at $\text{age} < 13$. While this does not improve our error rate (we would have made the same prediction regardless), it does improve our Gini index and cross-entropy which are the measures used to grow the tree.

## Trees vs. regression

## Benefits/drawbacks to decision trees

Decision trees are an entirely different method of estimating functional forms as compared to linear regression. There are some benefits to trees:

* They are easy to explain. Most people, even if they lack statistical training, can understand decision trees.
* They are easily presented as visualizations, and pretty interpretable.
* Qualitative predictors are easily handled without the need to create a long series of dummy variables.

However there are also drawbacks to trees:

* Their accuracy rates are generally lower than other regression and classification approaches.
* Trees can be non-robust. That is, a small change in the data or inclusion/exclusion of a handful of observations can dramatically alter the final estimated tree. For example, let's estimate a decision tree for the highway mileage example ($N = `r nrow(Auto)`) by splitting the data into a training/test set (70/30%) and estimating the test MSE, and repeat this process 1000 times using random combinations of training/test sets:

    ```{r auto-tree-val}
    auto_val_test <- function(){
      # split data
      auto_split <- resample_partition(Auto, p = c(test = 0.3, train = 0.7))
      
      # estimate model
      val <- tree(mpg ~ horsepower + weight, data = auto_split$train)
      
      # estimate test mse
      mse(val, auto_split$test)
    }
    
    # repeat the procedure 100 times
    val_mse <- data_frame(id = 1:1000,
                          mse = map_dbl(id, ~ auto_val_test()))
    
    # mean of the mse
    mean(val_mse$mse)
    
    # distribution of the mse
    ggplot(val_mse, aes(mse)) +
      geom_histogram()
    ```
    
    Even on larger datasets, this same trend will hold:
    
    ```{r sim-tree-val}
    simdata <- data_frame(x = runif(10000, 0, 10),
                          y = 2 + 3 * x + ifelse(x < 3, -x, 0) + rnorm(10000, 0, 3))
    
    simdata_val_test <- function(){
      # split data
      simdata_split <- resample_partition(simdata, p = c(test = 0.3, train = 0.7))
      
      # estimate model
      val <- tree(y ~ x, data = simdata_split$train)
      
      # estimate test mse
      mse(val, simdata_split$test)
    }
    
    # repeat the procedure 100 times
    val_mse <- data_frame(id = 1:1000,
                          mse = map_dbl(id, ~ simdata_val_test()))
    ```
    
    ```{r sim-tree-val-plot}
    # mean of the mse
    mean(val_mse$mse)
    
    # distribution of the mse
    ggplot(val_mse, aes(mse)) +
      geom_histogram()
    ```



Fortuntately, there is an easy way to improve on these poor predictions: by aggregating many decision trees and averaging across them, we can substantially improve performance.

# Bagging

# Random forests

# Boosting

# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```




