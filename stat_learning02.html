<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Statistical learning: resampling and decision trees</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/htmlwidgets-0.8/htmlwidgets.js"></script>
<script src="site_libs/d3-3.5.6/d3.min.js"></script>
<link href="site_libs/profvis-0.3.2/profvis.css" rel="stylesheet" />
<script src="site_libs/profvis-0.3.2/profvis.js"></script>
<link href="site_libs/highlight-6.2.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlight-6.2.0/highlight.js"></script>
<script src="site_libs/profvis-binding-0.3.2/profvis.js"></script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-45631879-2', 'auto');
  ga('send', 'pageview');

</script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.9em;
  padding-left: 5px;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Computing for the Social Sciences</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="faq.html">FAQ</a>
</li>
<li>
  <a href="syllabus.html">Syllabus</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Statistical learning: resampling and decision trees</h1>

</div>


<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="kw">theme_set</span>(<span class="kw">theme_bw</span>())</code></pre></div>
<div id="objectives" class="section level1">
<h1>Objectives</h1>
<ul>
<li>Define resampling methods</li>
<li>Review the validation set approach using linear regression</li>
<li>Explain leave-one-out cross-validation</li>
<li>Explain <span class="math inline">\(k\)</span>-fold cross-validation</li>
<li>Demonstrate how to conduct cross-validation on generalized linear models</li>
<li>Define a decision tree</li>
<li>Demonstrate how to estimate a decision tree</li>
<li>Define and estimate a random forest</li>
<li>Introduce the <code>caret</code> package for statistical learning in R</li>
</ul>
</div>
<div id="resampling-methods" class="section level1">
<h1>Resampling methods</h1>
<p>Resampling methods are essential to test and evaluate statistical models. Because you likely do not have the resources or capabilities to repeatedly sample from your population of interest, instead you can repeatedly draw from your original sample obtain additional information about your model. For instance, you could repeatedly draw samples from your data, estimate a linear regression model on each sample, and then examine how the estimated model differs across each sample. This allows you to assess the variability and stability of your model in a way not possible if you can only fit the model once.</p>
<div id="validation-set" class="section level2">
<h2>Validation set</h2>
<p>We have already seen the <em>validation set</em> approach in the <a href="stat_learning01.html">previous class</a>. By splitting our data into a <em>training set</em> and <em>test set</em>, we can evaluate the model’s effectiveness at predicting the response variable (in the context of either regression or classification) independently of the data used to estimate the model in the first place.</p>
<div id="classification" class="section level3">
<h3>Classification</h3>
<p>Recall how we used this approach to evaluate the accuracy of our <a href="stat_learning01.html#interactive_terms">interactive model predicting survival during the sinking of the Titanic</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;data/titanic_train.csv&quot;</span>)

titanic %&gt;%
<span class="st">  </span><span class="kw">head</span>() %&gt;%
<span class="st">  </span>knitr::<span class="kw">kable</span>()</code></pre></div>
<table>
<thead>
<tr class="header">
<th align="right">PassengerId</th>
<th align="right">Survived</th>
<th align="right">Pclass</th>
<th align="left">Name</th>
<th align="left">Sex</th>
<th align="right">Age</th>
<th align="right">SibSp</th>
<th align="right">Parch</th>
<th align="left">Ticket</th>
<th align="right">Fare</th>
<th align="left">Cabin</th>
<th align="left">Embarked</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Braund, Mr. Owen Harris</td>
<td align="left">male</td>
<td align="right">22</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">A/5 21171</td>
<td align="right">7.2500</td>
<td align="left">NA</td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>
<td align="left">female</td>
<td align="right">38</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">PC 17599</td>
<td align="right">71.2833</td>
<td align="left">C85</td>
<td align="left">C</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">1</td>
<td align="right">3</td>
<td align="left">Heikkinen, Miss. Laina</td>
<td align="left">female</td>
<td align="right">26</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">STON/O2. 3101282</td>
<td align="right">7.9250</td>
<td align="left">NA</td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
<td align="left">female</td>
<td align="right">35</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="left">113803</td>
<td align="right">53.1000</td>
<td align="left">C123</td>
<td align="left">S</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Allen, Mr. William Henry</td>
<td align="left">male</td>
<td align="right">35</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">373450</td>
<td align="right">8.0500</td>
<td align="left">NA</td>
<td align="left">S</td>
</tr>
<tr class="even">
<td align="right">6</td>
<td align="right">0</td>
<td align="right">3</td>
<td align="left">Moran, Mr. James</td>
<td align="left">male</td>
<td align="right">NA</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="left">330877</td>
<td align="right">8.4583</td>
<td align="left">NA</td>
<td align="left">Q</td>
</tr>
</tbody>
</table>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">survive_age_woman_x &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age *<span class="st"> </span>Sex, <span class="dt">data =</span> titanic,
                           <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(survive_age_woman_x)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age * Sex, family = binomial, data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9401  -0.7136  -0.5883   0.7626   2.2455  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.59380    0.31032   1.913  0.05569 . 
## Age          0.01970    0.01057   1.863  0.06240 . 
## Sexmale     -1.31775    0.40842  -3.226  0.00125 **
## Age:Sexmale -0.04112    0.01355  -3.034  0.00241 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 740.40  on 710  degrees of freedom
##   (177 observations deleted due to missingness)
## AIC: 748.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">logit2prob &lt;-<span class="st"> </span>function(x){
  <span class="kw">exp</span>(x) /<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span><span class="kw">exp</span>(x))
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(modelr)

titanic_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(titanic, <span class="kw">c</span>(<span class="dt">test =</span> <span class="fl">0.3</span>, <span class="dt">train =</span> <span class="fl">0.7</span>))
<span class="kw">map</span>(titanic_split, dim)</code></pre></div>
<pre><code>## $test
## [1] 267  12
## 
## $train
## [1] 624  12</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_model &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age +<span class="st"> </span>Sex, <span class="dt">data =</span> titanic_split$train,
                   <span class="dt">family =</span> binomial)
<span class="kw">summary</span>(train_model)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age + Sex, family = binomial, data = titanic_split$train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6710  -0.7157  -0.6920   0.7874   1.7882  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  1.118708   0.265525   4.213 2.52e-05 ***
## Age         -0.003546   0.007447  -0.476    0.634    
## Sexmale     -2.271914   0.217131 -10.463  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 662.67  on 487  degrees of freedom
## Residual deviance: 533.83  on 485  degrees of freedom
##   (136 observations deleted due to missingness)
## AIC: 539.83
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x_test_accuracy &lt;-<span class="st"> </span>titanic_split$test %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>() %&gt;%
<span class="st">  </span><span class="kw">add_predictions</span>(train_model) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">logit2prob</span>(pred),
         <span class="dt">pred =</span> <span class="kw">as.numeric</span>(pred &gt;<span class="st"> </span>.<span class="dv">5</span>))

<span class="kw">mean</span>(x_test_accuracy$Survived ==<span class="st"> </span>x_test_accuracy$pred, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.8185841</code></pre>
</div>
<div id="regression" class="section level3">
<h3>Regression</h3>
<p>This method also works for regression analysis. Here we will examine the relationship between horsepower and car mileage in the <code>Auto</code> dataset (found in <code>library(ISLR)</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ISLR)

Auto &lt;-<span class="st"> </span>Auto %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>()
Auto</code></pre></div>
<pre><code>## # A tibble: 392 × 9
##      mpg cylinders displacement horsepower weight acceleration  year
## *  &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;
## 1     18         8          307        130   3504         12.0    70
## 2     15         8          350        165   3693         11.5    70
## 3     18         8          318        150   3436         11.0    70
## 4     16         8          304        150   3433         12.0    70
## 5     17         8          302        140   3449         10.5    70
## 6     15         8          429        198   4341         10.0    70
## 7     14         8          454        220   4354          9.0    70
## 8     14         8          440        215   4312          8.5    70
## 9     14         8          455        225   4425         10.0    70
## 10    15         8          390        190   3850          8.5    70
## # ... with 382 more rows, and 2 more variables: origin &lt;dbl&gt;, name &lt;fctr&gt;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(Auto, <span class="kw">aes</span>(horsepower, mpg)) +
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="stat_learning02_files/figure-html/auto_plot-1.png" width="672" /></p>
<p>The relationship does not appear to be strictly linear:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(Auto, <span class="kw">aes</span>(horsepower, mpg)) +
<span class="st">  </span><span class="kw">geom_point</span>() +
<span class="st">  </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/auto_plot_lm-1.png" width="672" /></p>
<p>Perhaps by adding <a href="stat_learning01.html#quadratic_terms">quadratic terms</a> to the linear regression we could improve overall model fit. To evaluate the model, we will split the data into a training set and test set, estimate a series of higher-order models, and calculate a test statistic summarizing the accuracy of the estimated <code>mpg</code>. Rather than relying on the raw error rate (which makes sense in a classification model), we will instead use <em>mean squared error</em> (<span class="math inline">\(MSE\)</span>), defined as</p>
<p><span class="math display">\[MSE = \frac{1}{n} \sum_{i = 1}^{n}{(y_i - \hat{f}(x_i))^2}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y_i =\)</span> the observed response value for the <span class="math inline">\(i\)</span>th observation</li>
<li><span class="math inline">\(\hat{f}(x_i) =\)</span> the predicted response value for the <span class="math inline">\(i\)</span>th observation given by <span class="math inline">\(\hat{f}\)</span></li>
<li><span class="math inline">\(n =\)</span> the total number of observations</li>
</ul>
<p>Boo math! Actually this is pretty intuitive. All we’re doing is for each observation, calculating the difference between the actual and predicted values for <span class="math inline">\(y\)</span>, squaring that difference, then calculating the average across all observations. An <span class="math inline">\(MSE\)</span> of 0 indicates the model perfectly predicted each observation. The larger the <span class="math inline">\(MSE\)</span>, the more error in the model.</p>
<p>For this task, first we can use <code>modelr::resample_partition</code> to create training and test sets (using a 50/50 split), then estimate a linear regression model without any quadratic terms.</p>
<ul>
<li>I use <code>set.seed()</code> in the beginning - whenever you are writing a script that involves randomization (here, random subsetting of the data), always set the seed at the beginning of the script. This ensures the results can be reproduced precisely.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
<li>I also use the <code>glm</code> function rather than <code>lm</code> - if you don’t change the <code>family</code> parameter, the results of <code>lm</code> and <code>glm</code> are exactly the same. I do this because I want to use a cross-validation function later that only works with results from a <code>glm</code> function.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1234</span>)

auto_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(Auto, <span class="kw">c</span>(<span class="dt">test =</span> <span class="fl">0.5</span>, <span class="dt">train =</span> <span class="fl">0.5</span>))
auto_train &lt;-<span class="st"> </span>auto_split$train %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>()
auto_test &lt;-<span class="st"> </span>auto_split$test %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">auto_lm &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg ~<span class="st"> </span>horsepower, <span class="dt">data =</span> auto_train)
<span class="kw">summary</span>(auto_lm)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = mpg ~ horsepower, data = auto_train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -12.892   -2.864   -0.545    2.793   13.298  
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 38.005404   0.921129   41.26   &lt;2e-16 ***
## horsepower  -0.140459   0.007968  -17.63   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for gaussian family taken to be 20.48452)
## 
##     Null deviance: 10359.4  on 196  degrees of freedom
## Residual deviance:  3994.5  on 195  degrees of freedom
## AIC: 1157.9
## 
## Number of Fisher Scoring iterations: 2</code></pre>
<p>To estimate the <span class="math inline">\(MSE\)</span>, I write a brief function that requires two inputs: the dataset and the linear model.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MSE_mpg &lt;-<span class="st"> </span>function(model, data){
  <span class="kw">mean</span>((<span class="kw">predict</span>(model, data) -<span class="st"> </span>data$mpg)^<span class="dv">2</span>)
}

<span class="kw">MSE_mpg</span>(auto_lm, auto_test)</code></pre></div>
<pre><code>## [1] 28.57255</code></pre>
<p>For a strictly linear model, the <span class="math inline">\(MSE\)</span> for the test set is 28.57. How does this compare to a quadratic model? We can use the <code>poly</code> function in conjunction with a <code>map</code> iteration to estimate the <span class="math inline">\(MSE\)</span> for a series of models with higher-order polynomial terms:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">auto_poly &lt;-<span class="st"> </span>function(i){
  <span class="kw">glm</span>(mpg ~<span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> auto_train)
}

auto_poly_results &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">terms =</span> <span class="dv">1</span>:<span class="dv">5</span>,
           <span class="dt">model =</span> <span class="kw">map</span>(terms, auto_poly),
           <span class="dt">MSE =</span> <span class="kw">map_dbl</span>(model, MSE_mpg, <span class="dt">data =</span> auto_test))

<span class="kw">ggplot</span>(auto_poly_results, <span class="kw">aes</span>(terms, MSE)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Comparing quadratic linear models&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Using validation set&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Highest-order polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/mse_poly-1.png" width="672" /></p>
<p>Based on the <span class="math inline">\(MSE\)</span> for the validation (test) set, a polynomial model with a quadratic term (<span class="math inline">\(\text{horsepower}^2\)</span>) produces the lowest average error. Adding cubic or higher-order terms is just not necessary.</p>
</div>
<div id="drawbacks-to-validation-sets" class="section level3">
<h3>Drawbacks to validation sets</h3>
<p>There are two main problems with validation sets:</p>
<ol style="list-style-type: decimal">
<li><p>Validation estimates of the test error rates can be highly variable depending on which observations are sampled into the training and test sets. See what happens if we repeat the sampling, estimation, and validation procedure for the <code>Auto</code> data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mse_variable &lt;-<span class="st"> </span>function(Auto){
  auto_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(Auto, <span class="kw">c</span>(<span class="dt">test =</span> <span class="fl">0.5</span>, <span class="dt">train =</span> <span class="fl">0.5</span>))
  auto_train &lt;-<span class="st"> </span>auto_split$train %&gt;%
<span class="kw">tbl_df</span>()
  auto_test &lt;-<span class="st"> </span>auto_split$test %&gt;%
<span class="kw">tbl_df</span>()

  auto_poly &lt;-<span class="st"> </span>function(i){
<span class="kw">glm</span>(mpg ~<span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> auto_train)
  }

  results &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">terms =</span> <span class="dv">1</span>:<span class="dv">5</span>,
                    <span class="dt">model =</span> <span class="kw">map</span>(terms, auto_poly),
                    <span class="dt">MSE =</span> <span class="kw">map_dbl</span>(model, MSE_mpg, <span class="dt">data =</span> auto_test))

  <span class="kw">return</span>(results)
}

<span class="kw">rerun</span>(<span class="dv">10</span>, <span class="kw">mse_variable</span>(Auto)) %&gt;%
<span class="st">  </span><span class="kw">bind_rows</span>(<span class="dt">.id =</span> <span class="st">&quot;id&quot;</span>) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(terms, MSE, <span class="dt">color =</span> id)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Variability of MSE estimates&quot;</span>,
   <span class="dt">subtitle =</span> <span class="st">&quot;Using the validation set approach&quot;</span>,
   <span class="dt">x =</span> <span class="st">&quot;Degree of Polynomial&quot;</span>,
   <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>) +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;none&quot;</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/auto_variable_mse-1.png" width="672" /></p></li>
<li><p>If you don’t have a large data set, you’ll have to dramatically shrink the size of your training set. Most statistical learning methods perform better with more observations - if you don’t have enough data in the training set, you might overestimate the error rate in the test set.</p></li>
</ol>
</div>
</div>
<div id="leave-one-out-cross-validation" class="section level2">
<h2>Leave-one-out cross-validation</h2>
<p>An alternative method is <em>leave-one-out cross validation</em> (LOOCV). Like with the validation set approach, you split the data into two parts. However the difference is that you only remove one observation for the test set, and keep all remaining observations in the training set. The statistical learning method is fit on the <span class="math inline">\(n-1\)</span> training set. You then use the held-out observation to calculate the <span class="math inline">\(MSE = (y_1 - \hat{y}_1)^2\)</span> which should be an unbiased estimator of the test error. Because this <span class="math inline">\(MSE\)</span> is highly dependent on which observation is held out, <em>we repeat this process for every single observation in the data set</em>. Mathematically, this looks like:</p>
<p><span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i = 1}^{n}{MSE_i}\]</span></p>
<p>This method produces estimates of the error rate that have minimal bias and relatively steady (i.e. non-varying), unlike the validation set approach where the <span class="math inline">\(MSE\)</span> estimate is highly dependent on the sampling process for training/test sets. LOOCV is also highly flexible and works with any kind of predictive modeling.</p>
<p>Of course the downside is that this method is computationally difficult. You have to estimate <span class="math inline">\(n\)</span> different models - if you have a large <span class="math inline">\(n\)</span> or each individual model takes a long time to compute, you may be stuck waiting a long time for the computer to finish its calculations.</p>
<div id="loocv-in-linear-regression" class="section level3">
<h3>LOOCV in linear regression</h3>
<p>We can use the <code>cv.glm</code> function in the <code>boot</code> library to compute the LOOCV of any linear or logistic regression model. For the <code>Auto</code> dataset, this looks like:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(boot)

auto_lm &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg ~<span class="st"> </span>horsepower, <span class="dt">data =</span> Auto)
auto_lm_err &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(Auto, auto_lm)
auto_lm_err$delta[<span class="dv">1</span>]</code></pre></div>
<pre><code>## [1] 24.23151</code></pre>
<p><code>cv.glm</code> produces a list with several components. The two numbers in the <code>delta</code> vector contain the results of the LOOCV. The first number is what we care about the most, and is the LOOCV estimate of the <span class="math inline">\(MSE\)</span> for the dataset.</p>
<p>We can also use this method to compare the optimal number of polynomial terms as before.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_error &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dv">5</span>)
terms &lt;-<span class="st"> </span><span class="dv">1</span>:<span class="dv">5</span>

for(i in terms){
  glm_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg ~<span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> Auto)
  cv_error[[i]] &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(Auto, glm_fit)$delta[[<span class="dv">1</span>]]
}

cv_mse &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">terms =</span> terms,
           <span class="dt">cv_MSE =</span> cv_error)
cv_mse</code></pre></div>
<pre><code>## # A tibble: 5 × 2
##   terms   cv_MSE
##   &lt;int&gt;    &lt;dbl&gt;
## 1     1 24.23151
## 2     2 19.24821
## 3     3 19.33498
## 4     4 19.42443
## 5     5 19.03321</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(cv_mse, <span class="kw">aes</span>(terms, cv_MSE)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Comparing quadratic linear models&quot;</span>,
       <span class="dt">subtitle =</span> <span class="st">&quot;Using LOOCV&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Highest-order polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/loocv_poly-1.png" width="672" /></p>
<p>And arrive at the same conclusion.</p>
</div>
<div id="loocv-in-classification" class="section level3">
<h3>LOOCV in classification</h3>
<p>Let’s use classification to validate the interactive terms model from before. Before we can estimate the LOOCV, we need to first remove any observations from <code>titanic</code> which have missing values for <code>Survived</code>, <code>Age</code>, or <code>Sex</code> (<code>glm</code> does this for us automatically, whereas <code>cv.glm</code> does not. And since <code>cv.glm</code> requires the data frame as its first argument, we can use the pipe <code>%&gt;%</code>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_model &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age *<span class="st"> </span>Sex, <span class="dt">data =</span> titanic,
                     <span class="dt">family =</span> binomial)

titanic_loocv &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">is.na</span>(Survived), !<span class="kw">is.na</span>(Age), !<span class="kw">is.na</span>(Sex)) %&gt;%
<span class="st">  </span><span class="kw">cv.glm</span>(titanic_model)
titanic_loocv$delta[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>## [1] 0.1703518</code></pre>
<p>In a classification problem, the LOOCV tells us the average error rate based on our predictions. So here, it tells us that the interactive <code>Age * Sex</code> model has a 17% error rate. This is similar to the validation set result (<span class="math inline">\(81.9\%\)</span>)</p>
</div>
</div>
<div id="k-fold-cross-validation" class="section level2">
<h2>k-fold cross-validation</h2>
<p>A less computationally-intensive approach to cross validation is <span class="math inline">\(k\)</span>-fold cross-validation. Rather than dividing the data into <span class="math inline">\(n\)</span> groups, one divides the observations into <span class="math inline">\(k\)</span> groups, or <em>folds</em>, of approximately equal size. The first fold is treated as the validation set, the model is estimated on the remaining <span class="math inline">\(k-1\)</span> folds. This process is repeated <span class="math inline">\(k\)</span> times, with each fold serving as the validation set precisely once. The <span class="math inline">\(k\)</span>-fold CV estimate is calculated by averaging the <span class="math inline">\(MSE\)</span> values for each fold:</p>
<p><span class="math display">\[CV_{(k)} = \frac{1}{k} \sum_{i = 1}^{k}{MSE_i}\]</span></p>
<p>LOOCV is the special case of <span class="math inline">\(k\)</span>-fold cross-validation where <span class="math inline">\(k = n\)</span>. More typically researchers will use <span class="math inline">\(k=5\)</span> or <span class="math inline">\(k=10\)</span> depending on the size of the data set and the complexity of the statistical model.</p>
<div id="k-fold-cv-in-linear-regression" class="section level3">
<h3>k-fold CV in linear regression</h3>
<p>Let’s go back to the <code>Auto</code> data set. Instead of LOOCV, let’s use 10-fold CV to compare the different polynomial models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cv_error_fold10 &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dv">5</span>)
terms &lt;-<span class="st"> </span><span class="dv">1</span>:<span class="dv">5</span>

for(i in terms){
  glm_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg ~<span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> Auto)
  cv_error_fold10[[i]] &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(Auto, glm_fit, <span class="dt">K =</span> <span class="dv">10</span>)$delta[[<span class="dv">1</span>]]
}

cv_error_fold10</code></pre></div>
<pre><code>## [1] 24.42374 19.28760 19.20702 19.42017 18.96962</code></pre>
<p>How do these results compare to the LOOCV values?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data_frame</span>(<span class="dt">terms =</span> terms,
           <span class="dt">loocv =</span> cv_error,
           <span class="dt">fold10 =</span> cv_error_fold10) %&gt;%
<span class="st">  </span><span class="kw">gather</span>(method, MSE, loocv:fold10) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(terms, MSE, <span class="dt">color =</span> method)) +
<span class="st">  </span><span class="kw">geom_line</span>() +
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;MSE estimates&quot;</span>,
       <span class="dt">x =</span> <span class="st">&quot;Degree of Polynomial&quot;</span>,
       <span class="dt">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span>,
       <span class="dt">color =</span> <span class="st">&quot;CV Method&quot;</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/10_fold_auto_loocv-1.png" width="672" /></p>
<p>Pretty much the same results. But computationally, how long does it take to estimate the 10-fold CV versus LOOCV? We can use the <code>profvis</code> package to profile our code and determine how long it takes to run.</p>
<div id="loocv" class="section level4">
<h4>LOOCV</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(profvis)

<span class="kw">profvis</span>({
  cv_error &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dv">5</span>)
  terms &lt;-<span class="st"> </span><span class="dv">1</span>:<span class="dv">5</span>
  
  for (i in terms) {
    glm_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg ~<span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> Auto)
    cv_error[[i]] &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(Auto, glm_fit)$delta[[<span class="dv">1</span>]]
  }
})</code></pre></div>
<div id="htmlwidget-e806828ac0a3497cfb73" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-e806828ac0a3497cfb73">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,12,12,12,12,12,12,12,12,12,12,12,12,12,13,13,13,13,13,13,13,13,13,13,13,14,14,14,14,14,14,14,14,14,15,15,15,15,15,15,15,16,16,16,16,16,16,16,16,16,16,16,17,17,17,17,17,17,17,17,18,18,18,18,18,18,18,18,18,19,19,19,19,19,19,19,19,19,20,20,20,20,20,20,20,20,20,20,20,21,21,21,21,21,21,21,22,22,22,22,22,22,22,22,23,23,23,23,23,23,23,23,24,24,24,24,24,24,24,25,25,25,25,26,26,26,26,26,26,26,26,26,26,26,26,26,27,27,27,27,27,27,27,27,27,28,28,28,28,28,28,28,28,28,29,29,29,29,29,29,29,29,29,29,30,30,30,30,30,30,30,30,30,30,31,31,31,31,31,31,31,31,31,32,32,32,32,32,32,32,32,32,32,32,32,32,32,32,33,33,33,33,33,33,33,33,33,33,33,33,34,34,34,34,34,34,34,34,34,35,35,35,35,35,35,35,35,36,36,36,37,37,37,37,37,37,37,37,37,37,37,38,38,38,38,38,39,39,39,39,39,39,39,39,39,39,39,39,40,40,40,40,40,40,40,40,40,40,40,41,41,41,41,41,41,41,41,41,42,42,42,42,42,42,42,42,42,42,42,42,42,42,43,43,43,43,43,43,44,44,44,44,44,44,45,45,45,45,45,45,45,45,45,45,45,45,45,46,46,46,47,47,47,47,47,47,47,47,48,48,48,48,48,48,48,48,48,48,48,48,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,49,50,50,50,50,50,50,50,51,51,51,51,51,51,51,51,51,51,51,51,51,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,52,53,53,53,53,53,53,53,53,53,54,54,54,54,54,54,54,54,54,54,54,54,54,55,55,55,55,55,55,55,55,55,56,56,56,56,56,56,56,56,56,56,56,57,57,57,57,57,57,57,57,57,57,58,58,58,58,58,58,58,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,60,60,60,60,60,60,60,61,61,61,61,61,61,61,61,61,61,61,61,61,61,61,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,62,63,63,63,63,63,63,63,63,63,63,63,63,63,63,64,64,64,64,64,64,64,64,64,64,64,64,64,64,65,65,65,65,65,65,65,65,65,65,66,66,66,66,66,66,66,66,66,67,67,67,67,67,67,67,67,67,67,67,67,67,67,67,68,68,68,68,68,68,68,68,68,68,68,69,69,69,69,69,69,69,69,70,70,70,70,70,70,70,70,70,70,70,70,70,70,71,71,71,71,71,71,71,71,71,71,71,72,72,72,72,72,72,72,72,72,73,73,73,73,73,73,74,74,74,74,74,74,74,74,74,74,74,74,74,75,75,75,75,75,75,75,75,75,75,75,76,76,76,76,76,76,76,76,76,76,76,76,77,77,77,77,77,77,77,77,77,77,77,77,77,77,78,78,78,78,78,78,78,78,78,78,79,79,79,79,79,79,79,79,79,80,80,80,80,80,80,80,80,80,80,80,80,80,80,80,81,81,81,82,82,82,82,82,83,83,83,83,83,83,83,83,83,84,84,84,84,84,84,84,84,85,85,85,85,85,85,85,85,85,85,86,86,86,86,86,86,86,86,86,86,86,87,87,87,88,88,88,89,89,89,89,89,89,89,90,90,90,90,90,91,91,91,91,91,91,91,91,91,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,92,93,93,93,93,93,93,93,93,94,94,94,94,94,94,94,94,94,94,94,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,95,96,96,96,96,96,96,96,96,96,96,96,97,97,97,97,97,97,97,97,97,98,98,98,98,98,98,98,98,98,98,98,98,98,98,98,99,99,99,99,99,99,99,99,100,100,100,100,100,100,100,100,100,101,101,101,101,101,101,101,101,101,101,101,101,101,101,102,102,102,102,102,102,102,102,102,102,102,102,103,103,103,103,103,103,103,103,103,104,104,104,104,104,104,104,104,105,105,105,105,105,105,105,106,106,106,106,106,106,106,106,106,106,106,106,107,107,107,107,107,108,108,108,108,108,108,109,109,109,109,109,109,109,110,110,110,110,110,110,110,110,110,111,111,111,111,111,111,111,111,111,111,111,112,112,112,112,112,112,112,112,112,113,113,113,113,113,113,113,113,113,113,113,113,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,114,115,115,115,115,115,115,115,116,116,116,116,116,116,117,117,117,117,117,117,117,117,117,117,117,117,117,118,118,118,118,118,118,118,118,118,118,118,118,118,119,119,119,119,119,119,119,119,119,119,119,119,119,120,120,120,120,120,120,120,120,120,120,120,120,120,121,121,121,121,121,121,121,121,121,121,121,121,121,122,122,122,122,122,122,122,122,122,122,122,122,122,123,123,123,123,123,123,123,123,123,123,123,123,123,124,124,124,124,124,124,124,124,124,124,124,124,124,125,125,125,125,125,125,125,126,126,126,126,126,126,126,126,126,126,126,127,127,127,127,127,127,127,127,127,127,127,127,127,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,128,129,129,129,129,129,129,129,129,129,129,129,130,130,130,130,130,130,130,130,130,130,130,130,131,131,131,131,131,131,131,131,132,132,132,132,132,133,133,133,133,133,133,133,133,133,133,133,133,134,134,134,134,134,134,134,134,135,135,135,135,135,135,135,135,136,136,136,136,136,136,136,137,137,137,137,137,137,138,138,138,138,138,139,139,139,139,139,139,139,139,140,140,140,140,140,140,140,140,141,141,141,141,141,141,141,141,141,141,141,141,141,141,141,142,142,142,142,142,142,142,142,143,143,143,143,143,143,143,143,143,143,143,143,143,143,144,144,144,144,144,144,144,145,145,145,145,145,145,145,145,146,146,146,146,146,146,146,146,146,146,146,146,146,147,147,147,147,147,147,148,149,149,149,149,149,149,149,149,149,149,150,150,150,150,150,150,150,150,151,151,151,151,151,151,151,151,151,152,152,152,152,152,152,152,152,152,152,153,153,153,153,153,153,153,153,154,154,154,154,154,154,154,155,156,156,156,156,156,156,156,156,156,156,156,156,156,156,156,157,157,157,157,157,157,157,157,158,158,158,158,158,158,158,158,158,158,158,159,159,159,159,159,159,159,159,159,159,159,159,159,160,160,160,160,160,160,160,160,160,160,160,160,160,160,160,161,161,161,161,161,161,161,162,162,162,162,162,162,162,162,162,163,163,163,163,163,163,163,163,163,164,165,165,165,165,165,165,165,166,166,166,166,166,167,167,167,167,167,167,167,167,167,167,167,167,168,168,168,168,168,168,168,168,168,168,169,169,169,169,169,169,169,169,169,169,169,169,169,169,169,170,170,170,170,170,170,170,171,171,171,171,171,171,171,171,171,172,172,172,172,172,172,172,172,172,172,172,172,172,172,173,173,173,173,173,173,173,173,173,173,173,173,174,174,174,174,174,174,174,174,174,175,175,175,175,175,175,175,175,176,176,176,176,176,176,176,176,176,176,176,176,176,177,177,177,177,177,177,177,177,177,178,178,178,178,178,178,178,179,179,179,179,179,179,179,179,179,179,179,180,180,180,180,180,180,180,180,180,180,180,181,181,181,181,181,181,182,182,182,182,182,182,182,182,182,182,182,182,182,182,183,183,183,183,183,183,183,183,183,183,184,184,184,184,184,184,184,184,184,184,184,184,184,184,184,184,185,185,185,185,185,185,185,185,185,185,185,185,185,185,185,186,186,186,186,186,186,186,186,186,186,186,186,186,186,186,187,187,187,187,187,187,187,188,188,188,188,188,188,188,188,188,188,188,189,189,189,189,189,189,189,189,189,190,190,190,190,190,190,190,190,190,191,191,191,191,191,191,191,192,192,192,192,192,192,192,192,192,193,193,193,193,193,193,193,193,193,193,193,193,193,193,194,194,194,194,194,194,195,195,195,195,195,195,196,196,196,196,196,197,197,197,197,197,197,197,197,197,197,197,197,198,198,198,198,198,198,198,198,198,199,199,199,199,199,199,199,199,199,199,200,200,200,200,200,200,200,200,200,200,200,201,201,201,201,201,201,201,201,202,202,202,202,202,202,202,202,202,202,202,203,203,203,203,203,203,203,203,203,204,204,204,204,204,204,204,205,205,205,205,205,205,205,205,206,206,206,206,206,206,206,206,206,206,206,206,207,207,207,207,208,208,208,208,208,208,208,208,208,208,208,208,208,208,209,209,209,209,209,209,209,209,209,209,209,210,210,210,210,210,210,210,210,210,210,210,210,210,210,211,211,211,211,211,211,211,211,212,212,212,212,212,212,212,212,212,212,212,212,212,213,213,213,213,213,213,213,213,213,214,214,214,214,214,214,214,214,214,215,215,215,215,215,215,215,215,215,215,215,215,215,216,216,216,216,216,216,216,216,217,217,217,217,217,217,217,217,217,217,217,217,218,218,218,218,218,218,218,218,218,219,219,219,219,219,219,219,219,219,220,220,220,220,220,220,220,220,220,220,220,220,220,220,221,221,221,221,221,222,222,222,222,222,222,222,222,222,222,222,223,223,223,223,223,223,223,223,223,223,223,223,223,223,223,223,223,223,224,224,224,224,224,224,224,224,224,224,224,224,224,224,224,225,225,225,225,225,225,225,226,226,226,226,226,226,226,226,226,226,226,226,227,227,227,227,227,227,227,227,227,228,228,228,228,228,228,228,228,228,228,228,228,228,228,229,229,229,229,229,229,229,229,230,230,230,230,230,230,230,230,230,230,230,230,231,231,231,231,231,231,231,231,232,232,232,232,232,232,232,232,233,233,233,233,233,233,233,233,233,234,234,234,234,234,234,234,234,234,235,235,235,235,235,235,235,235,236,236,236,236,236,236,236,236,236,236,236,236,237,237,237,237,238,238,238,238,238,238,238,238,238,238,238,239,239,239,239,239,239,240,240,240,240,240,240,240,240,240,241,241,241,241,241,241,241,241,241,241,241,241,241,241,242,242,242,242,242,242,242,242,242,242,242,242,242,242,242,242,243,243,243,243,243,243,243,243,243,243,243,243,243,243,244,244,244,244,244,244,244,245,245,245,245,245,245,245,245,245,245,245,245,245,245,245,246,246,246,246,246,246,246,246,246,247,247,247,247,247,247,247,247,247,247,247,247,247,247,248,248,248,248,248,248,249,249,249,249,249,250,250,250,250,250,250,250,250,250,250,250,251,251,251,251,251,251,251,251,251,251,251,252,252,252,252,252,252,252,252,252,252,252,252,252,252,253,253,253,253,253,253,253,253,253,253,253,253,253,253,253,254,254,254,254,254,254,254,255,255,255,255,255,255,255,255,256,256,256,256,256,256,256,256,256,256,257,257,257,257,257,257,257,257,257,257,257,257,258,258,258,258,258,258,258,258,258,258,258,259,259,259,259,259,259,259,259,259,259,259,259,260,260,260,260,260,260,260,260,260,261,261,261,261,261,261,261,262,262,262,262,262,262,262,262,263,263,263,263,263,263,264,264,264,264,264,264,264,264,265,265,265,265,265,265,265,265,265,265,265,265,265,265,265,266,266,266,266,266,266,266,266,266,266,266,266,266,267,267,267,267,267,267,267,268,268,268,268,268,268,268,268,268,268,268,268,268,269,269,269,269,269,269,270,270,270,270,270,270,270,270,270,270,270,271,271,271,271,271,271,271,271,271,271,271,271,271,272,272,272,272,272,272,272,273,273,273,273,273,273,273,273,273,273,273,273,273,273,273,273,273,274,274,274,274,274,274,274,275,275,275,275,275,275,275,275,275,275,275,275,276,276,276,276,276,276,276,276,276,276,276,276,277,277,277,277,277,277,277,277,277,277,277,277,277,277,277,277,277,277,277,277,278,278,278,279,279,279,279,279,279,279,279,279,279,280,280,280,280,280,280,280,280,280,280,281,281,281,281,281,281,281,281,282,282,282,282,282,282,282,282,283,283,283,283,283,283,283,283,283,283,283,283,284,284,284,284,285,285,285,285,285,285,285,285,285,286,286,286,286,286,286,286,286,286,287,287,287,287,287,287,287,287,287,287,288,288,288,288,288,288,288,289,289,289,289,289,289,289,289,289,290,290,290,290,290,290,290,290,290,290,291,291,291,291,291,291,291,291,291,291,292,292,292,292,292,292,292,293,293,293,293,293,293,293,293,293,294,294,294,294,294,294,294,294,294,294,294,294,295,295,295,295,295,295,295,295,295,295,296,296,296,296,296,296,296,296,296,296,296,296,296,296,296,297,297,297,297,297,297,297,297,298,298,298,298,298,298,298,299,299,299,299,299,299,299,299,299,299,300,300,300,300,300,300,300,300,300,300,301,301,301,301,301,301,301,301,301,301,302,302,302,302,302,303,303,303,303,303,303,303,303,303,304,304,304,304,304,304,304,304,304,305,305,305,305,305,305,305,305,306,306,306,306,306,306,306,307,307,307,307,307,307,307,307,307,307,307,307,307,307,307,308,308,308,308,308,309,309,309,309,309,309,309,309,309,309,309,309,309,309,310,310,310,310,310,310,310,310,310,310,310,311,311,311,311,311,311,311,311,312,312,312,312,312,312,312,312,312,312,312,312,313,313,313,313,313,313,313,313,314,314,314,314,314,314,314,314,315,315,315,315,315,315,315,315,315,315,315,315,315,316,316,316,316,316,316,316,316,316,316,316,317,317,317,317,317,317,317,317,317,317,317,317,317,318,318,318,318,318,318,318,318,319,319,319,319,319,319,319,319,319,319,319,319,320,320,320,320,320,320,320,320,320,320,320,320,320,320,320,320,320,321,321,321,321,321,321,321,322,322,322,322,322,322,322,322,322,322,322,322,322,323,323,323,323,323,323,323,323,323,323,323,323,324,324,324,324,324,324,324,324,324,324,324,324,324,325,325,325,325,325,325,325,325,325,326,326,326,326,326,326,326,326,326,326,326,327,327,327,327,327,327,327,327,328,328,328,328,328,328,329,329,329,329,329,329,329,329,329,329,329,330,330,330,330,330,330,330,330,331,331,331,331,331,331,332,332,332,332,332,332,332,332,333,333,333,333,333,333,333,333,333,333,333,333,334,334,334,334,334,334,334,334,334,334,334,335,335,335,335,335,335,335,336,336,336,336,336,336,336,336,336,336,336,336,337,337,337,337,337,337,337,337,337,338,338,338,338,338,338,339,339,339,339,339,339,339,339,339,339,339,339,340,340,340,340,340,340,340,340,340,340,340,340,341,341,341,341,341,341,341,341,341,341,341,341,341,342,342,342,342,342,342,342,342,343,343,343,343,343,343,343,343,343,343,344,344,344,344,344,344,344,344,344,344,345,345,345,345,345,345,345,345,345,345,345,346,346,346,346,346,346,346,346,346,346,346,346,346,346,346,347,347,347,347,347,347,348,348,348,348,348,348,348,348,349,349,349,349,349,349,349,349,349,349,349,350,350,350,350,350,350,350,350,350,350,351,351,351,351,351,351,351,351,351,351,351,352,352,352,352,352,352,352,352,352,352,352,352,352,352,353,353,353,353,353,353,353,353,354,354,354,354,354,354,354,354,354,355,355,355,355,355,355,355,355,355,356,356,356,356,356,356,356,356,356,356,356,356,356,356,356,356,357,357,357,357,357,357,358,358,358,358,358,358,358,359,359,359,359,359,359,359,359,359,360,360,360,360,360,360,361,361,361,361,361,361,361,361,361,361,361,361,361,361,362,362,362,362,363,363,363,363,363,363,363,363,363,363,363,363,364,364,364,364,364,364,364,364,364,365,365,365,365,365,365,365,365,365,365,365,366,366,366,366,366,366,366,366,366,366,367,367,367,367,367,367,367,367,367,368,368,368,368,368,368,368,368,368,368,368,368,368,368,369,369,369,369,369,369,369,369,369,370,370,370,370,370,370,370,370,370,370,370,371,371,371,371,371,371,371,371,371,371,371,371,372,372,372,372,372,372,372,372,372,372,372,372,372,372,373,373,373,373,373,373,373,373,374,374,374,374,374,374,374,374,374,375,375,375,375,375,375,376,376,376,376,376,376,376,376,376,376,377,377,377,377,377,377,377,377,377,377,377,378,378,378,378,378,378,378,378,378,378,378,378,378,379,379,380,380,380,380,380,380,380,380,380,380,380,380,380,381,381,381,381,381,381,381,381,381,381,381,381,381,382,382,382,382,382,382,382,382,382,382,382,382,382,383,383,383,383,383,383,383,383,383,383,383,383,383,384,384,384,384,384,384,384,384,384,384,384,384,384,385,385,385,385,385,385,385,385,385,385,385,385,385,386,386,386,386,386,386,386,386,386,386,386,386,386,387,387,387,387,387,387,387,387,387,387,387,387,387,388,388,388,388,388,388,388,388,388,388,388,388,388,389,389,389,389,389,389,389,389,389,390,390,390,390,390,390,391,391,391,391,391,391,391,391,391,392,392,392,392,392,392,392,392,392,392,393,393,393,393,393,393,393,393,393,393,394,394,394,394,394,394,394,394,394,395,395,395,395,395,395,395,395,395,396,396,396,396,396,396,396,396,396,396,396,396,396,396,396,396,396,397,397,397,397,397,397,397,398,398,398,398,398,398,398,398,398,398,398,398,398,398,399,399,399,399,399,399,399,399,399,399,399,399,399,400,400,400,400,400,400,400,400,400,401,401,401,401,401,401,401,401,401,401,401,401,401,402,402,402,402,402,402,402,402,402,402,403,403,403,403,403,403,403,403,403,403,403,403,403,403,403,404,404,404,404,404,404,405,405,405,405,405,406,406,406,406,407,407,407,407,407,408,408,408,408,408,408,408,408,408,408,408,409,409,409,409,409,409,409,409,410,410,410,411,411,411,411,411,411,412,412,412,412,412,412,412,412,412,412,412,412,413,413,413,414,414,414,414,414,414,414,414,414,414,414,415,415,415,415,415,415,415,415,415,415,415,415,416,416,416,416,416,416,416,416,417,417,417,417,417,417,417,418,418,418,418,418,418,418,419,419,419,419,419,419,419,419,419,419,419,420,420,420,420,420,420,421,421,421,421,421,421,421,421,421,421,421,421,421,422,422,422,422,422,422,422,422,422,422,422,423,423,423,423,423,423,423,423,423,423,424,424,424,424,424,424,424,424,424,425,425,425,425,425,425,425,425,426,426,426,426,426,426,426,426,426,426,426,427,427,427,427,427,427,427,427,428,428,428,428,428,428,428,428,428,429,429,430,430,430,430,430,430,430,430,431,431,431,431,431,431,432,432,432,432,432,432,433,433,433,433,433,433,433,433,433,434,434,434,434,434,434,435,435,435,435,435,435,435,435,435,435,435,435,435,435,435,435,435,436,436,436,436,436,436,436,436,436,436,436,436,436,437,437,437,437,437,437,437,437,437,438,438,438,438,438,438,438,438,438,438,438,438,439,439,439,439,439,439,439,440,440,440,440,440,440,440,440,440,440,440,440,440,440,440,440,440,441,441,441,441,441,441,441,441,442,442,442,442,442,442,442,442,442,442,443,443,443,443,443,443,443,444,444,444,444,444,444,444,444,445,445,445,445,445,445,445,445,445,445,446,446,446,446,446,446,446,446,446,446,447,447,447,447,447,447,447,447,447,448,448,448,448,448,448,448,448,448,448,448,448,449,449,449,449,449,449,449,449,450,450,450,450,450,450,450,450,450,450,451,451,451,451,451,451,451,451,452,452,452,452,452,452,452,452,452,452,452,453,453,453,453,453,453,453,453,453,453,453,454,454,454,454,454,454,454,454,454,454,454,454,455,455,455,455,455,455,455,455,455,455,455,455,455,455,456,456,456,456,456,456,456,456,456,456,456,456,456,457,457,457,457,457,457,457,457,457,457,457,458,458,458,458,458,458,458,458,459,459,459,459,459,459,459,459,459,459,459,459,459,460,460,460,460,460,460,460,460,461,461,461,461,461,461,461,461,461,461,461,461,462,462,462,462,462,462,462,462,462,462,462,463,463,463,463,463,463,463,464,464,464,464,464,464,464,464,464,465,465,465,465,465,465,465,465,465,465,466,466,466,466,466,466,466,466,466,466,466,467,467,467,467,467,467,467,467,467,467,467,467,467,468,468,468,468,468,468,468,468,469,469,469,469,469,469,469,469,470,470,470,470,470,470,470,470,470,471,471,471,471,471,471,471,471,471,472,472,472,472,472,472,472,472,472,472,472,472,472,472,472,472,472,472,472,473,473,473,473,473,473,473,473,473,473,473,473,474,474,474,474,474,474,474,475,475,475,475,475,475,475,475,475,475,475,475,475,475,476,476,476,476,476,476,476,476,476,476,476,476,476,476,477,477,477,477,477,477,477,477,477,477,477,477,477,477,478,478,478,478,478,478,478,478,478,478,478,478,478,478,478,479,479,479,479,479,479,479,479,479,479,479,480,480,480,480,480,480,480,480,480,480,480,480,480,480,480,480,481,481,481,481,481,481,481,482,482,482,482,482,482,482,482,483,483,483,483,483,484,484,484,484,484,484,484,484,484,484,484,484,485,485,485,485,485,485,485,485,485,485,485,485,485,485,485,485,486,486,486,486,486,486,486,486,486,486,486,486,487,487,487,487,487,487,487,487,487,487,487,488,488,488,488,488,489,489,489,489,489,489,489,490,490,490,490,490,490,490,490,490,491,491,491,491,491,491,491,491,492,492,492,492,492,492,492,492,493,493,493,493,493,493,493,493,494,494,494,494,494,494,494,494,494,495,495,495,495,495,495,495,495,495,495,495,496,496,496,496,496,496,496,496,496,497,497,497,497,497,497,497,497,497,497,497,497,497,497,497,498,498,498,498,498,498,498,498,498,498,498,498,498,498,498,499,499,499,499,499,499,499,499,499,499,499,499,500,500,500,500,500,500,500,501,501,501,501,501,501,501,501,501,502,502,502,502,502,502,502,502,502,503,503,503,503,503,503,503,503,503,504,504,504,504,504,504,505,505,505,505,505,505,505,505,505,505,505,505,506,506,506,506,506,506,506,506,506,507,507,507,507,507,507,507,507,507,507,508,508,508,508,508,508,508,508,509,509,509,509,509,509,509,509,509,509,509,509,509,509,510,510,510,510,510,510,510,510,510,510,510,510,511,511,511,511,511,511,511,511,511,511,511,511,511,511,511,512,512,512,512,512,512,512,513,513,513,513,513,514,514,514,514,514,514,514,514,515,515,515,515,515,515,515,515,515,515,515,515,515,515,515,516,516,516,516,516,516,516,516,516,517,517,517,517,517,517,517,517,518,518,518,518,518,518,518,518,518,519,519,519,519,519,519,520,520,520,520,520,520,520,520,520,520,520,521,521,521,521,521,522,522,522,522,522,522,522,522,522,523,523,523,523,523,523,523,523,523,523,523,523,524,524,524,524,524,524,524,524,524,524,524,524,525,525,525,525,525,525,525,525,525,526,526,526,526,526,526,526,526,526,527,527,527,527,527,528,528,528,528,528,528,528,528,528,528,528,528,528,528,528,529,529,529,529,529,529,529,529,529,529,529,529,529,530,530,530,530,530,530,530,531,531,531,531,531,531,531,531,531,531,531,531,532,532,532,532,532,532,532,532,532,533,533,533,533,533,533,533,533,533,533,533,533,533,534,534,534,534,534,534,534,535,535,535,535,535,535,535,535,535,535,535,535,535,535,536,536,536,536,536,536,536,536,536,536,536,536,536,536,536,537,537,537,537,537,537,537,537,538,538,538,538,538,538,538,539,539,539,539,539,539,539,540,540,540,540,540,540,540,541,541,541,541,541,541,541,542,542,542,542,542,542,542,543,543,543,543,543,543,543,544,544,544,544,544,544,544,545,545,545,545,545,545,545,546,546,546,546,546,546,546,547,547,547,547,547,547,547,547,547,547,547,547,548,548,548,548,548,549,549,549,549,549,549,549,549,549,549,549,549,549,550,550,550,550,550,550,550,550,550,550,550,551,551,551,551,551,551,551,551,551,551,552,552,552,552,552,552,552,552,552,552,552,552,552,552,553,553,553,553,553,553,553,553,553,553,553,553,553,553,553,554,554,554,554,554,554,555,555,555,555,555,555,555,555,555,556,556,556,556,556,557,557,557,557,557,557,557,557,557,557,558,558,558,558,558,558,558,559,559,559,559,559,559,559,559,560,560,560,560,560,560,560,560,560,561,561,561,561,561,561,561,561,562,562,562,562,562,562,562,562,563,563,563,563,563,563,563,563,563,563,564,564,564,565,565,565,565,565,565,565,565,565,565,565,565,566,566,566,566,566,566,566,566,566,566,567,567,567,567,567,567,567,568,568,568,568,568,568,568,568,568,569,569,569,569,569,569,569,569,569,569,569,569,569,569,569,569,569,570,570,570,570,570,570,570,570,570,570,570,570,571,571,571,571,571,571,571,571,572,572,572,572,572,572,572,572,572,573,573,573,573,573,573,573,573,573,574,574,574,574,574,574,575,575,575,575,575,575,575,575,576,576,576,576,576,576,576,577,577,577,577,577,577,577,577,577,577,577,577,578,578,578,578,578,578,578,578,578,579,579,579,579,579,579,579,579,579,579,579,579,579,579,579,580,580,580,580,580,580,580,580,581,581,581,581,581,581,581,581,581,582,582,582,582,582,582,582,582,582,582,582,582,583,583,583,583,583,583,583,583,583,584,584,584,584,584,584,584,584,584,584,584,584,584,584,584,584,585,585,585,585,585,585,585,585,585,585,585,586,586,586,586,586,586,586,586,587,587,587,587,587,587,587,587,587,587,587,587,588,588,588,588,588,588,588,588,588,588,588,588,588,588,588,588,588,589,589,589,589,589,589,589,589,589,589,590,590,590,590,590,590,590,591,591,591,591,591,591,591,592,592,592,592,592,592,592,593,593,593,593,593,593,593,593,593,593,593,594,594,594,594,594,594,594,595,595,595,595,595,595,595,595,595,595,595,596,596,596,596,596,596,597,597,597,597,597,597,597,597,597,597,597,597,597,597,597,597,598,598,598,598,598,598,598,598,598,598,598,598,598,598,599,599,599,599,599,599,599,599,599,599,599,599,599,599,599,600,600,600,600,600,600,600,600,600,600,600,601,601,601,601,601,601,601,601,602,602,602,602,602,602,602,602,603,603,603,603,603,603,603,603,603,603,603,604,604,604,604,604,604,604,604,605,605,606,606,606,606,606,606,606,606,606,606,607,607,607,607,607,607,607,607,607,607,607,607,607,607,607,608,608,608,608,608,608,608,608,609,609,609,609,609,609,609,609,609,609,609,609,610,610,610,610,610,610,610,610,611,611,611,611,611,611,611,611,611,611,611,611,611,611,612,612,612,612,612,612,612,612,612,612,612,613,613,613,613,613,613,613,613,613,613,613,613,613,613,613,614,614,614,614,614,614,614,614,614,614,614,614,614,614,614,614,615,615,615,615,615,615,615,615,615,615,615,615,615,615,615,615,615,615,615,615,616,616,616,616,616,616,616,617,617,617,617,617,617,618,618,618,618,618,618,618,619,619,619,619,619,619,619,619,619,619,619,620,620,620,620,620,620,621,621,621,621,621,621,621,621,621,621,622,622,622,622,622,622,622,622,622,622,623,623,623,623,623,623,623,624,624,624,624,624,624,624,624,624,624,624,625,625,625,625,625,625,625,625,625,625,625,625,625,626,626,626,626,626,626,626,626,626,626,626,626,626,627,627,627,627,627,627,627,627,627,627,627,628,628,628,628,628,628,628,628,628,628,628,629,629,629,629,629,629,629,629,629,630,630,630,630,630,630,630,630,630,630,630,630,630,630,630,631,631,631,631,631,631,631,631,632,632,632,632,632,632,632,632,632,632,632,632,632,632,632,633,633,633,633,633,633,633,633,633,634,634,634,634,634,634,634,634,634,634,634,634,634,634,635,635,635,635,635,635,635,635,635,635,635,635,635,636,636,636,636,636,636,636,637,637,638,638,638,638,638,638,638,638,639,639,639,639,639,640,640,640,640,640,640,640,640,640,640,641,641,641,641,641,641,641,641,641,641,642,642,642,642,642,642,642,642,642,642,642,642,643,643,643,643,643,643,643,643,643,643,643,643,643,643,644,644,644,644,644,644,644,644,644,645,645,645,645,645,645,645,645,645,645,645,645,645,646,646,646,646,646,646,646,646,647,647,647,647,647,647,647,647,647,647,647,647,647,648,648,648,648,648,648,648,649,649,649,649,649,649,650,650,650,650,650,650,650,650,650,650,650,650,650,650,651,651,651,651,651,651,651,651,651,651,651,651,652,652,652,652,652,652,653,653,653,653,653,653,653,653,653,653,653,654,654,654,654,654,654,654,655,655,655,655,655,655,655,655,655,655,655,655,655,655,655,656,656,656,656,656,656,656,656,656,657,657,657,657,657,657,657,657,657,657,657,657,657,657,657,657,657,658,658,658,658,658,658,658,658,658,659,659,659,659,659,659,659,659,659,659,659,660,660,660,660,660,660,660,660,660,660,660,660,660,660,661,661,661,661,661,661,661,661,661,661,661,661,661,661,661,661,661,661,661,662,662,662,662,662,662,662,662,662,662,662,663,663,663,663,663,663,663,663,663,663,663,663,663,664,664,664,664,664,664,664,664,664,664,664,664,664,664,665,665,665,665,665,665,665,665,665,665,665,665,666,666,666,666,666,666,666,666,666,666,666,666,666,666,667,667,668,668,668,668,668,668,668,668,668,668,668,668,668,668,668,669,669,669,669,669,669,669,669,669,669,669,669,669,669,669,670,670,670,670,670,670,670,670,670,670,670,670,670,670,670,671,671,671,671,671,671,671,671,671,671,671,671,671,671,671,672,672,672,672,672,672,672,672,672,672,672,672,672,672,672,673,673,673,673,673,673,673,673,673,673,673,673,673,673,673,674,674,674,674,674,674,674,674,674,674,675,675,675,675,675,675,675,675,675,675,675,675,676,676,676,676,676,676,676,676,676,677,677,677,677,677,677,677,677,678,678,678,678,678,678,678,678,678,678,678,679,679,679,679,679,679,679,679,679,679,679,679,679,679,679,680,680,680,680,680,680,680,680,680,680,680,680,681,681,681,681,681,681,681,681,682,682,682,682,682,682,682,682,683,683,683,683,683,683,684,684,684,684,684,684,684,684,684,684,684,684,684,684,685,685,685,685,685,685,685,685,685,685,685,685,685,685,686,686,686,686,686,686,686,686,686,686,686,686,686,686,687,687,687,687,687,687,687,687,688,688,688,688,688,688,688,688,688,688,688,688,688,688,688,688,689,689,689,689,689,689,689,689,689,689,689,689,690,690,690,690,690,690,690,690,690,691,691,691,691,691,691,691,691,691,691,691,691,692,692,692,692,692,692,692,692,692,692,692,692,692,692,693,693,693,693,693,693,693,693,694,694,694,694,694,694,694,694,694,694,694,695,695,695,695,695,695,696,696,696,696,696,696,696,696,696,696,696,696,696,696,696,697,697,697,697,697,697,697,697,697,698,698,698,698,698,698,699,699,699,699,699,699,699,699,699,699,699,699,699,699,699,699,699,699,700,700,700,700,700,700,700,700,700,700,700,701,701,701,701,701,701,701,701,701,701,701,702,702,702,702,702,702,702,702,702,702,702,702,702,702,703,703,703,703,703,703,703,703,703,703,703,703,703,703,704,704,704,704,704,704,704,705,705,705,705,705,705,706,706,706,706,706,706,706,706,707,707,707,707,707,707,707,707,707,707,707,707,708,708,708,708,708,709,709,709,709,709,709,709,709,709,709,709,710,710,710,710,710,710,710,710,710,710,710,711,711,711,711,711,711,711,711,711,711,711,712,712,713,713,713,713,713,714,714,714,714,714,714,714,714,714,714,714,714,714,714,715,715,715,715,715,715,715,716,716,716,716,716,716,716,717,717,717,717,717,717,717,717,718,718,718,718,718,718,718,718,719,719,719,719,719,719,719,719,720,720,720,720,720,720,721,721,721,721,721,721,722,722,722,722,722,722,722,722,722,722,722,722,722,722,722,722,722,723,723,723,723,723,723,723,723,723,724,724,724,724,724,724,724,724,724,724,724,724,724,725,725,725,725,725,725,725,726,726,726,726,726,726,726,726,727,727,727,727,727,727,727,727,727,728,728,728,728,728,728,728,728,728,728,728,728,728,728,729,729,729,729,729,729,729,729,729,729,729,729,729,729,729,729,729,729],"depth":[9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,3,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,3,2,1,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,1,7,6,5,4,3,2,1,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,5,4,3,2,1,4,3,2,1,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,3,2,1,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,5,4,3,2,1,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,2,1,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,20,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,2,1,8,7,6,5,4,3,2,1,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,19,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,6,5,4,3,2,1,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,2,1,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,6,5,4,3,2,1,6,5,4,3,2,1,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,13,12,11,10,9,8,7,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,14,13,12,11,10,9,8,7,6,5,4,3,2,1,18,17,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1],"label":["dev.resids","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","sys.parent","sys.call","match.call","glm","eval","eval","eval.parent","cv.glm","<Anonymous>",".External2","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","mode","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list.data.frame","as.list","vapply","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","%*%","predict.lm","predict.glm","predict","mean","cost","cv.glm","$","family.glm","family","predict.glm","predict","mean","cost","cv.glm","as.list","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","any",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.fun","lapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.control","do.call","glm","eval","eval","eval.parent","cv.glm","nrow","[.tbl_df","[","cv.glm","<GC>","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","unique","simplify2array","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.array","is.array","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","parent.frame","match.fun","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","[.tbl_df","[","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm","eval","eval","eval.parent","cv.glm",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm","model.matrix","glm","eval","eval","eval.parent","cv.glm","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","structure","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","attributes","structure","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","colnames","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","$","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","length","length","FUN","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","dim","dim","nrow","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","getOption","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","unlist","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.response","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","is.matrix","is.matrix","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","length","length","dim.data.frame","dim","dim","nrow","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","sys.parent","sys.function","formals","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","match","%in%","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","mean","cost","cv.glm","dim","nrow","[.tbl_df","[","cv.glm","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","mean","cost","cv.glm","[.tbl_df","[","cv.glm","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm",".row_names_info","%||%","raw_rownames","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","cv.glm","as.list","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","c","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list.data.frame","as.list","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","any","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","$","structure","family","glm","eval","eval","eval.parent","cv.glm","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","aic","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","lapply","[.tbl_df","[","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","NextMethod","[.factor","FUN","lapply","[.tbl_df","[","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","dim","dim","ncol","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","linkinv","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","unique","simplify2array","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","<GC>","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","names","names","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.glm","predict","mean","cost","cv.glm",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","eval","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","family","glm","eval","eval","eval.parent","cv.glm","as.list","lapply","[.tbl_df","[","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","attributes","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","%*%","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","cv.glm","cv.glm","<GC>","rep.int","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","$","predict.lm","predict.glm","predict","mean","cost","cv.glm","cv.glm","dim","dim","ncol","paste","FUN","vapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","sum",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","cv.glm","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","glm","eval","eval","eval.parent","cv.glm",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list.default","as.list","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","vapply","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","make.link","family","glm","eval","eval","eval.parent","cv.glm","as.list","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","paste","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix","glm","eval","eval","eval.parent","cv.glm","length","length","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","paste0","<Anonymous>","[[.data.frame","[[","model.extract","glm","eval","eval","eval.parent","cv.glm","dim","dim","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<Anonymous>","[[.data.frame","[[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","dim","dim","nrow","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","length","length","dim.data.frame","dim","dim","nrow","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm","glm","eval","eval","eval.parent","cv.glm","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","[[","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","names","names","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.array","is.array","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list","vapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","ncol","paste","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","-","mean","cost","cv.glm","length","length","dim.data.frame","dim","dim","nrow","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","dim","dim","ncol","paste","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","[[<-","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","-","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","match","%in%","[[.data.frame","[[","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","terms","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%","[[.data.frame","[[","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","NextMethod","[.table","[","cv.glm","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","$<-","predict.glm","predict","mean","cost","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm",".Fortran","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","eval","glm","eval","eval","eval.parent","cv.glm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","as.list","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","family","glm","eval","eval","eval.parent","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<Anonymous>","[[.data.frame","[[","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","FUN","vapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","attr","predict.lm","predict.glm","predict","mean","cost","cv.glm","[[.data.frame","[[","model.extract","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.matrix","is.matrix","FUN","vapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","<GC>","!=","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","mean","cost","cv.glm","paste0","<Anonymous>","[[.data.frame","[[","model.extract","glm","eval","eval","eval.parent","cv.glm","is.ordered","FUN","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","mean.default","mean","cost","cv.glm","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","eval","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","structure","make.link","family","glm","eval","eval","eval.parent","cv.glm","sum",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","sys.parent","sys.function","match.call","glm","eval","eval","eval.parent","cv.glm","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","%*%","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","names","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","makepredictcall","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.glm","predict","mean","cost","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.matrix","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","lapply","[.tbl_df","[","cv.glm","match.fun","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","c","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","colnames<-","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","unique.default","unique","simplify2array","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","matrix","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","mean","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","$","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","family","glm","eval","eval","eval.parent","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","matrix","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","$","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list.default","as.list","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.matrix","is.matrix","FUN","vapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list","lapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","match","%in%","structure","make.link","family","glm","eval","eval","eval.parent","cv.glm","[[","$.data.frame","$","model.offset","as.vector","glm","eval","eval","eval.parent","cv.glm","match","%in%","structure","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","sum",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","make.link","family","glm","eval","eval","eval.parent","cv.glm","anyDuplicated.default","anyDuplicated","[.data.frame","[","lapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","makepredictcall","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","names","names","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","names","names","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","NextMethod","[.factor","FUN","lapply","[.tbl_df","[","cv.glm","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","lapply","[.tbl_df","[","cv.glm","dim.data.frame","dim","dim","ncol","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","unique","simplify2array","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","rep.int","mu.eta","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.character","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","dim","dim","nrow","[.tbl_df","[","cv.glm","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","[","cv.glm","<GC>","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","c","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","names","names","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","sum",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","integer","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","do.call","glm","eval","eval","eval.parent","cv.glm","<GC>","lapply","[.tbl_df","[","cv.glm","lapply","[.tbl_df","[","cv.glm","<GC>","lapply","[.tbl_df","[","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","matrix","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","parent.frame","eval.parent","cv.glm","is.matrix","is.matrix","FUN","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","$","family.glm","family","predict.glm","predict","mean","cost","cv.glm","$","predict.lm","predict.glm","predict","mean","cost","cv.glm","$","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.numeric","is.numeric","FUN","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.arg","predict.glm","predict","mean","cost","cv.glm","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","nargs","[[.data.frame","[[","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","structure","make.link","family","glm","eval","eval","eval.parent","cv.glm","<GC>","cv.glm","ifelse","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","family","glm","eval","eval","eval.parent","cv.glm","do.call","glm","eval","eval","eval.parent","cv.glm","dev.resids","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","do.call","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","linkinv","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","asNamespace","getExportedValue","::","eval","eval","glm","eval","eval","eval.parent","cv.glm","paste","FUN","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","length","FUN","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","NextMethod","[.data.frame","[","lapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","rep.int","variance","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.array","is.array","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","c","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","dim","dim","ncol","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","structure","make.link","family","glm","eval","eval","eval.parent","cv.glm","colnames<-","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","paste","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","ncol","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.matrix.default","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","[[.data.frame","[[","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","formals","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","any",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Fortran","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","structure","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","length","length","dim.data.frame","dim","dim","nrow","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","is.array","is.array","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","lapply","[.tbl_df","[","cv.glm","NextMethod","[.factor","FUN","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","c","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.numeric","is.numeric","FUN","vapply",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.glm","predict","mean","cost","cv.glm",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","NROW","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","==","eval","eval","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","anyDuplicated","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","<Anonymous>","[[.data.frame","[[","$.data.frame","$","model.weights","as.vector","glm","eval","eval","eval.parent","cv.glm","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.matrix","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.character","makepredictcall.poly","makepredictcall","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","[[","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","anyDuplicated","[.data.frame","[","lapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","$","structure","family","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.character","makepredictcall.poly","makepredictcall","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","double","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","lapply","[.tbl_df","[","cv.glm","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Fortran","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","terms","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","$","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.fun","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","glm","eval","eval","eval.parent","cv.glm","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","unique.default","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.data.frame","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","lapply","[.tbl_df","[","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.factor","unique.default","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","as.list","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","sum",".deparseOpts","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","anyDuplicated","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","formals","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm","unique.default","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.glm","predict","mean","cost","cv.glm","sys.call","match","%in%","[[.data.frame","[[","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.array","is.array","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".Fortran","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.glm","predict","mean","cost","cv.glm","as.list","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","qr.lm","predict.lm","predict.glm","predict","mean","cost","cv.glm","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","FUN","vapply","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","mean","cost","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","$","predict.lm","predict.glm","predict","mean","cost","cv.glm","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","anyDuplicated.default","anyDuplicated","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.list","eval","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.response","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","$","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.character","makepredictcall.poly","makepredictcall","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","all","[[.data.frame","[[","$.data.frame","$","model.offset","as.vector","glm","eval","eval","eval.parent","cv.glm",".Call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","any",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","eval","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","<GC>","rep.int","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","terms","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm",".checkMFClasses","predict.lm","predict.glm","predict","mean","cost","cv.glm","terms.formula","terms","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","model.matrix","glm","eval","eval","eval.parent","cv.glm",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","<GC>","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","[[<-","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","cv.glm","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","length","length","FUN","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","matrix","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","[.factor","FUN","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","family","predict.glm","predict","mean","cost","cv.glm","<GC>",".External2","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.array","is.array","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","paste","FUN","vapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","[.factor","FUN","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","as.list","vapply","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","[","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","isTRUE","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","colnames<-","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","[","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","<GC>","lapply","[.tbl_df","[","cv.glm","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","is.factor","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","unique.default","unique","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","<GC>","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","[[","model.extract","glm","eval","eval","eval.parent","cv.glm","c","glm","eval","eval","eval.parent","cv.glm",".Fortran","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.extract","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","max","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","cv.glm","<GC>","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","lapply","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","%in%",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","dev.resids","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","dim.data.frame","dim","dim","ncol","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm",".Fortran","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","unique","simplify2array","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","structure","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","nrow","logical","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","rep.int","outer","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","eval.parent","cv.glm","<GC>","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","is.data.frame","colSums","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.matrix","[.data.frame","[","lapply",".getXlevels","glm","eval","eval","eval.parent","cv.glm","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".Fortran","qr.default","qr","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","NROW","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.factor","unique.default","unique","simplify2array","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".Fortran","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","pmatch",".deparseOpts","deparse","paste","FUN","lapply","sapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","do.call","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","as.list","lapply","sapply","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","getNamespace","asNamespace","make.link","family","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","deparse","mode","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","is.na","is.na","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","$","qr.qy","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","%*%","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.call","glm","eval","eval","eval.parent","cv.glm","structure","make.link","family","glm","eval","eval","eval.parent","cv.glm","match.fun","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","glm","eval","eval","eval.parent","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","poly","eval","eval","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","rep.int","variance","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","eval.parent","cv.glm","glm","eval","eval","eval.parent","cv.glm","as.list.default","as.list","vapply","recycle_columns","list_to_tibble","as_tibble.data.frame","[.tbl_df","[","predict.lm","predict.glm","predict","mean","cost","cv.glm","<GC>","match.call","glm","eval","eval","eval.parent","cv.glm","as.character","model.response","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","match.arg","predict.glm","predict","mean","cost","cv.glm","match","%in%",".deparseOpts","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","<GC>","poly","eval","eval","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","model.frame.default","model.frame","predict.lm","predict.glm","predict","mean","cost","cv.glm","vapply","model.matrix.default","model.matrix","predict.lm","predict.glm","predict","mean","cost","cv.glm","[[.data.frame","[[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm",".subset2","<Anonymous>","[[.data.frame","[[","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm"],"filenum":[null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,1,null,null,1,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,1,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,1,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,1,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1],"linenum":[null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,9,null,null,9,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,9,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,9,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,9,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9],"memalloc":[11.6954803466797,11.6954803466797,11.6954803466797,11.6954803466797,11.6954803466797,11.6954803466797,11.6954803466797,11.6954803466797,11.6954803466797,12.869026184082,12.869026184082,12.869026184082,12.869026184082,12.869026184082,12.869026184082,12.869026184082,12.869026184082,14.1435699462891,14.1435699462891,14.1435699462891,14.1435699462891,14.1435699462891,14.1435699462891,14.1435699462891,14.1435699462891,14.1435699462891,14.1435699462891,15.7921829223633,15.7921829223633,15.7921829223633,15.7921829223633,15.7921829223633,15.7921829223633,15.7921829223633,15.7921829223633,15.7921829223633,15.7921829223633,17.7745132446289,17.7745132446289,17.7745132446289,17.7745132446289,17.7745132446289,17.7745132446289,17.7745132446289,17.7745132446289,17.7745132446289,17.7745132446289,17.7745132446289,10.3588409423828,10.3588409423828,10.3588409423828,10.3588409423828,10.3588409423828,10.3588409423828,10.3588409423828,10.3588409423828,10.3588409423828,10.3588409423828,12.1788940429688,12.1788940429688,12.1788940429688,12.1788940429688,12.1788940429688,12.1788940429688,12.1788940429688,13.6129989624023,13.6129989624023,13.6129989624023,13.6129989624023,13.6129989624023,13.6129989624023,13.6129989624023,13.6129989624023,15.0421981811523,15.0421981811523,15.0421981811523,15.0421981811523,15.0421981811523,15.0421981811523,15.0421981811523,15.0421981811523,15.0421981811523,15.0421981811523,16.1651992797852,16.1651992797852,16.1651992797852,16.1651992797852,16.1651992797852,16.1651992797852,16.1651992797852,16.1651992797852,17.1996154785156,17.1996154785156,17.1996154785156,17.1996154785156,17.1996154785156,17.1996154785156,17.1996154785156,17.1996154785156,17.1996154785156,17.1996154785156,17.1996154785156,17.1996154785156,17.1996154785156,17.1996154785156,17.1996154785156,18.1514358520508,18.1514358520508,18.1514358520508,18.1514358520508,18.1514358520508,18.1514358520508,18.1514358520508,18.1514358520508,18.1514358520508,18.1514358520508,18.1514358520508,18.1514358520508,18.1514358520508,19.3345184326172,19.3345184326172,19.3345184326172,19.3345184326172,19.3345184326172,19.3345184326172,19.3345184326172,19.3345184326172,19.3345184326172,19.3345184326172,19.3345184326172,10.9006118774414,10.9006118774414,10.9006118774414,10.9006118774414,10.9006118774414,10.9006118774414,10.9006118774414,10.9006118774414,10.9006118774414,12.5828857421875,12.5828857421875,12.5828857421875,12.5828857421875,12.5828857421875,12.5828857421875,12.5828857421875,14.7180633544922,14.7180633544922,14.7180633544922,14.7180633544922,14.7180633544922,14.7180633544922,14.7180633544922,14.7180633544922,14.7180633544922,14.7180633544922,14.7180633544922,17.5469589233398,17.5469589233398,17.5469589233398,17.5469589233398,17.5469589233398,17.5469589233398,17.5469589233398,17.5469589233398,19.4717483520508,19.4717483520508,19.4717483520508,19.4717483520508,19.4717483520508,19.4717483520508,19.4717483520508,19.4717483520508,19.4717483520508,12.8502655029297,12.8502655029297,12.8502655029297,12.8502655029297,12.8502655029297,12.8502655029297,12.8502655029297,12.8502655029297,12.8502655029297,15.0080337524414,15.0080337524414,15.0080337524414,15.0080337524414,15.0080337524414,15.0080337524414,15.0080337524414,15.0080337524414,15.0080337524414,15.0080337524414,15.0080337524414,17.1579513549805,17.1579513549805,17.1579513549805,17.1579513549805,17.1579513549805,17.1579513549805,17.1579513549805,11.760986328125,11.760986328125,11.760986328125,11.760986328125,11.760986328125,11.760986328125,11.760986328125,11.760986328125,14.1024856567383,14.1024856567383,14.1024856567383,14.1024856567383,14.1024856567383,14.1024856567383,14.1024856567383,14.1024856567383,15.9197387695312,15.9197387695312,15.9197387695312,15.9197387695312,15.9197387695312,15.9197387695312,15.9197387695312,17.7752532958984,17.7752532958984,17.7752532958984,17.7752532958984,11.3439178466797,11.3439178466797,11.3439178466797,11.3439178466797,11.3439178466797,11.3439178466797,11.3439178466797,11.3439178466797,11.3439178466797,11.3439178466797,11.3439178466797,11.3439178466797,11.3439178466797,13.8510131835938,13.8510131835938,13.8510131835938,13.8510131835938,13.8510131835938,13.8510131835938,13.8510131835938,13.8510131835938,13.8510131835938,15.6755981445312,15.6755981445312,15.6755981445312,15.6755981445312,15.6755981445312,15.6755981445312,15.6755981445312,15.6755981445312,15.6755981445312,17.4385757446289,17.4385757446289,17.4385757446289,17.4385757446289,17.4385757446289,17.4385757446289,17.4385757446289,17.4385757446289,17.4385757446289,17.4385757446289,18.8807983398438,18.8807983398438,18.8807983398438,18.8807983398438,18.8807983398438,18.8807983398438,18.8807983398438,18.8807983398438,18.8807983398438,18.8807983398438,12.2560653686523,12.2560653686523,12.2560653686523,12.2560653686523,12.2560653686523,12.2560653686523,12.2560653686523,12.2560653686523,12.2560653686523,14.1207733154297,14.1207733154297,14.1207733154297,14.1207733154297,14.1207733154297,14.1207733154297,14.1207733154297,14.1207733154297,14.1207733154297,14.1207733154297,14.1207733154297,14.1207733154297,14.1207733154297,14.1207733154297,14.1207733154297,17.6917190551758,17.6917190551758,17.6917190551758,17.6917190551758,17.6917190551758,17.6917190551758,17.6917190551758,17.6917190551758,17.6917190551758,17.6917190551758,17.6917190551758,17.6917190551758,19.470573425293,19.470573425293,19.470573425293,19.470573425293,19.470573425293,19.470573425293,19.470573425293,19.470573425293,19.470573425293,13.3946914672852,13.3946914672852,13.3946914672852,13.3946914672852,13.3946914672852,13.3946914672852,13.3946914672852,13.3946914672852,14.9904861450195,14.9904861450195,14.9904861450195,17.0678939819336,17.0678939819336,17.0678939819336,17.0678939819336,17.0678939819336,17.0678939819336,17.0678939819336,17.0678939819336,17.0678939819336,17.0678939819336,17.0678939819336,18.6714935302734,18.6714935302734,18.6714935302734,18.6714935302734,18.6714935302734,12.8156585693359,12.8156585693359,12.8156585693359,12.8156585693359,12.8156585693359,12.8156585693359,12.8156585693359,12.8156585693359,12.8156585693359,12.8156585693359,12.8156585693359,12.8156585693359,14.7807083129883,14.7807083129883,14.7807083129883,14.7807083129883,14.7807083129883,14.7807083129883,14.7807083129883,14.7807083129883,14.7807083129883,14.7807083129883,14.7807083129883,16.9003295898438,16.9003295898438,16.9003295898438,16.9003295898438,16.9003295898438,16.9003295898438,16.9003295898438,16.9003295898438,16.9003295898438,18.5497894287109,18.5497894287109,18.5497894287109,18.5497894287109,18.5497894287109,18.5497894287109,18.5497894287109,18.5497894287109,18.5497894287109,18.5497894287109,18.5497894287109,18.5497894287109,18.5497894287109,18.5497894287109,12.8428649902344,12.8428649902344,12.8428649902344,12.8428649902344,12.8428649902344,12.8428649902344,14.5261459350586,14.5261459350586,14.5261459350586,14.5261459350586,14.5261459350586,14.5261459350586,16.33056640625,16.33056640625,16.33056640625,16.33056640625,16.33056640625,16.33056640625,16.33056640625,16.33056640625,16.33056640625,16.33056640625,16.33056640625,16.33056640625,16.33056640625,18.4387512207031,18.4387512207031,18.4387512207031,12.8994522094727,12.8994522094727,12.8994522094727,12.8994522094727,12.8994522094727,12.8994522094727,12.8994522094727,12.8994522094727,14.7399291992188,14.7399291992188,14.7399291992188,14.7399291992188,14.7399291992188,14.7399291992188,14.7399291992188,14.7399291992188,14.7399291992188,14.7399291992188,14.7399291992188,14.7399291992188,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,16.7205047607422,18.8694686889648,18.8694686889648,18.8694686889648,18.8694686889648,18.8694686889648,18.8694686889648,18.8694686889648,10.6261825561523,10.6261825561523,10.6261825561523,10.6261825561523,10.6261825561523,10.6261825561523,10.6261825561523,10.6261825561523,10.6261825561523,10.6261825561523,10.6261825561523,10.6261825561523,10.6261825561523,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,12.5397644042969,15.0165634155273,15.0165634155273,15.0165634155273,15.0165634155273,15.0165634155273,15.0165634155273,15.0165634155273,15.0165634155273,15.0165634155273,17.5419158935547,17.5419158935547,17.5419158935547,17.5419158935547,17.5419158935547,17.5419158935547,17.5419158935547,17.5419158935547,17.5419158935547,17.5419158935547,17.5419158935547,17.5419158935547,17.5419158935547,19.4690780639648,19.4690780639648,19.4690780639648,19.4690780639648,19.4690780639648,19.4690780639648,19.4690780639648,19.4690780639648,19.4690780639648,12.1944580078125,12.1944580078125,12.1944580078125,12.1944580078125,12.1944580078125,12.1944580078125,12.1944580078125,12.1944580078125,12.1944580078125,12.1944580078125,12.1944580078125,14.3447189331055,14.3447189331055,14.3447189331055,14.3447189331055,14.3447189331055,14.3447189331055,14.3447189331055,14.3447189331055,14.3447189331055,14.3447189331055,16.5059661865234,16.5059661865234,16.5059661865234,16.5059661865234,16.5059661865234,16.5059661865234,16.5059661865234,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,18.6619720458984,11.3212509155273,11.3212509155273,11.3212509155273,11.3212509155273,11.3212509155273,11.3212509155273,11.3212509155273,13.005744934082,13.005744934082,13.005744934082,13.005744934082,13.005744934082,13.005744934082,13.005744934082,13.005744934082,13.005744934082,13.005744934082,13.005744934082,13.005744934082,13.005744934082,13.005744934082,13.005744934082,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,15.6631240844727,17.7694396972656,17.7694396972656,17.7694396972656,17.7694396972656,17.7694396972656,17.7694396972656,17.7694396972656,17.7694396972656,17.7694396972656,17.7694396972656,17.7694396972656,17.7694396972656,17.7694396972656,17.7694396972656,19.4706954956055,19.4706954956055,19.4706954956055,19.4706954956055,19.4706954956055,19.4706954956055,19.4706954956055,19.4706954956055,19.4706954956055,19.4706954956055,19.4706954956055,19.4706954956055,19.4706954956055,19.4706954956055,12.8724975585938,12.8724975585938,12.8724975585938,12.8724975585938,12.8724975585938,12.8724975585938,12.8724975585938,12.8724975585938,12.8724975585938,12.8724975585938,15.6942367553711,15.6942367553711,15.6942367553711,15.6942367553711,15.6942367553711,15.6942367553711,15.6942367553711,15.6942367553711,15.6942367553711,17.8394012451172,17.8394012451172,17.8394012451172,17.8394012451172,17.8394012451172,17.8394012451172,17.8394012451172,17.8394012451172,17.8394012451172,17.8394012451172,17.8394012451172,17.8394012451172,17.8394012451172,17.8394012451172,17.8394012451172,19.274040222168,19.274040222168,19.274040222168,19.274040222168,19.274040222168,19.274040222168,19.274040222168,19.274040222168,19.274040222168,19.274040222168,19.274040222168,12.8356018066406,12.8356018066406,12.8356018066406,12.8356018066406,12.8356018066406,12.8356018066406,12.8356018066406,12.8356018066406,14.9525756835938,14.9525756835938,14.9525756835938,14.9525756835938,14.9525756835938,14.9525756835938,14.9525756835938,14.9525756835938,14.9525756835938,14.9525756835938,14.9525756835938,14.9525756835938,14.9525756835938,14.9525756835938,17.456413269043,17.456413269043,17.456413269043,17.456413269043,17.456413269043,17.456413269043,17.456413269043,17.456413269043,17.456413269043,17.456413269043,17.456413269043,19.4713592529297,19.4713592529297,19.4713592529297,19.4713592529297,19.4713592529297,19.4713592529297,19.4713592529297,19.4713592529297,19.4713592529297,13.459098815918,13.459098815918,13.459098815918,13.459098815918,13.459098815918,13.459098815918,16.6503677368164,16.6503677368164,16.6503677368164,16.6503677368164,16.6503677368164,16.6503677368164,16.6503677368164,16.6503677368164,16.6503677368164,16.6503677368164,16.6503677368164,16.6503677368164,16.6503677368164,18.8007965087891,18.8007965087891,18.8007965087891,18.8007965087891,18.8007965087891,18.8007965087891,18.8007965087891,18.8007965087891,18.8007965087891,18.8007965087891,18.8007965087891,12.5909576416016,12.5909576416016,12.5909576416016,12.5909576416016,12.5909576416016,12.5909576416016,12.5909576416016,12.5909576416016,12.5909576416016,12.5909576416016,12.5909576416016,12.5909576416016,14.7016372680664,14.7016372680664,14.7016372680664,14.7016372680664,14.7016372680664,14.7016372680664,14.7016372680664,14.7016372680664,14.7016372680664,14.7016372680664,14.7016372680664,14.7016372680664,14.7016372680664,14.7016372680664,17.5660247802734,17.5660247802734,17.5660247802734,17.5660247802734,17.5660247802734,17.5660247802734,17.5660247802734,17.5660247802734,17.5660247802734,17.5660247802734,12.2355117797852,12.2355117797852,12.2355117797852,12.2355117797852,12.2355117797852,12.2355117797852,12.2355117797852,12.2355117797852,12.2355117797852,14.72216796875,14.72216796875,14.72216796875,14.72216796875,14.72216796875,14.72216796875,14.72216796875,14.72216796875,14.72216796875,14.72216796875,14.72216796875,14.72216796875,14.72216796875,14.72216796875,14.72216796875,17.3393707275391,17.3393707275391,17.3393707275391,12.1992111206055,12.1992111206055,12.1992111206055,12.1992111206055,12.1992111206055,14.9749755859375,14.9749755859375,14.9749755859375,14.9749755859375,14.9749755859375,14.9749755859375,14.9749755859375,14.9749755859375,14.9749755859375,17.5318069458008,17.5318069458008,17.5318069458008,17.5318069458008,17.5318069458008,17.5318069458008,17.5318069458008,17.5318069458008,12.5035552978516,12.5035552978516,12.5035552978516,12.5035552978516,12.5035552978516,12.5035552978516,12.5035552978516,12.5035552978516,12.5035552978516,12.5035552978516,15.044563293457,15.044563293457,15.044563293457,15.044563293457,15.044563293457,15.044563293457,15.044563293457,15.044563293457,15.044563293457,15.044563293457,15.044563293457,17.1910552978516,17.1910552978516,17.1910552978516,19.4654693603516,19.4654693603516,19.4654693603516,14.5903778076172,14.5903778076172,14.5903778076172,14.5903778076172,14.5903778076172,14.5903778076172,14.5903778076172,17.4410629272461,17.4410629272461,17.4410629272461,17.4410629272461,17.4410629272461,15.4673004150391,15.4673004150391,15.4673004150391,15.4673004150391,15.4673004150391,15.4673004150391,15.4673004150391,15.4673004150391,15.4673004150391,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,15.1581726074219,17.4362182617188,17.4362182617188,17.4362182617188,17.4362182617188,17.4362182617188,17.4362182617188,17.4362182617188,17.4362182617188,13.4019470214844,13.4019470214844,13.4019470214844,13.4019470214844,13.4019470214844,13.4019470214844,13.4019470214844,13.4019470214844,13.4019470214844,13.4019470214844,13.4019470214844,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,16.0934066772461,18.7436141967773,18.7436141967773,18.7436141967773,18.7436141967773,18.7436141967773,18.7436141967773,18.7436141967773,18.7436141967773,18.7436141967773,18.7436141967773,18.7436141967773,14.2863693237305,14.2863693237305,14.2863693237305,14.2863693237305,14.2863693237305,14.2863693237305,14.2863693237305,14.2863693237305,14.2863693237305,16.33251953125,16.33251953125,16.33251953125,16.33251953125,16.33251953125,16.33251953125,16.33251953125,16.33251953125,16.33251953125,16.33251953125,16.33251953125,16.33251953125,16.33251953125,16.33251953125,16.33251953125,19.0305938720703,19.0305938720703,19.0305938720703,19.0305938720703,19.0305938720703,19.0305938720703,19.0305938720703,19.0305938720703,14.6357955932617,14.6357955932617,14.6357955932617,14.6357955932617,14.6357955932617,14.6357955932617,14.6357955932617,14.6357955932617,14.6357955932617,17.8027496337891,17.8027496337891,17.8027496337891,17.8027496337891,17.8027496337891,17.8027496337891,17.8027496337891,17.8027496337891,17.8027496337891,17.8027496337891,17.8027496337891,17.8027496337891,17.8027496337891,17.8027496337891,13.7443313598633,13.7443313598633,13.7443313598633,13.7443313598633,13.7443313598633,13.7443313598633,13.7443313598633,13.7443313598633,13.7443313598633,13.7443313598633,13.7443313598633,13.7443313598633,16.2702331542969,16.2702331542969,16.2702331542969,16.2702331542969,16.2702331542969,16.2702331542969,16.2702331542969,16.2702331542969,16.2702331542969,18.4066772460938,18.4066772460938,18.4066772460938,18.4066772460938,18.4066772460938,18.4066772460938,18.4066772460938,18.4066772460938,14.7489242553711,14.7489242553711,14.7489242553711,14.7489242553711,14.7489242553711,14.7489242553711,14.7489242553711,17.4395446777344,17.4395446777344,17.4395446777344,17.4395446777344,17.4395446777344,17.4395446777344,17.4395446777344,17.4395446777344,17.4395446777344,17.4395446777344,17.4395446777344,17.4395446777344,19.4697265625,19.4697265625,19.4697265625,19.4697265625,19.4697265625,16.1839828491211,16.1839828491211,16.1839828491211,16.1839828491211,16.1839828491211,16.1839828491211,18.4547729492188,18.4547729492188,18.4547729492188,18.4547729492188,18.4547729492188,18.4547729492188,18.4547729492188,15.8558654785156,15.8558654785156,15.8558654785156,15.8558654785156,15.8558654785156,15.8558654785156,15.8558654785156,15.8558654785156,15.8558654785156,18.2123565673828,18.2123565673828,18.2123565673828,18.2123565673828,18.2123565673828,18.2123565673828,18.2123565673828,18.2123565673828,18.2123565673828,18.2123565673828,18.2123565673828,15.3095474243164,15.3095474243164,15.3095474243164,15.3095474243164,15.3095474243164,15.3095474243164,15.3095474243164,15.3095474243164,15.3095474243164,17.6475296020508,17.6475296020508,17.6475296020508,17.6475296020508,17.6475296020508,17.6475296020508,17.6475296020508,17.6475296020508,17.6475296020508,17.6475296020508,17.6475296020508,17.6475296020508,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,19.4718704223633,16.5828475952148,16.5828475952148,16.5828475952148,16.5828475952148,16.5828475952148,16.5828475952148,16.5828475952148,19.4445343017578,19.4445343017578,19.4445343017578,19.4445343017578,19.4445343017578,19.4445343017578,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,19.4688720703125,10.1328048706055,10.1328048706055,10.1328048706055,10.1328048706055,10.1328048706055,10.1328048706055,10.1328048706055,10.1328048706055,10.1328048706055,10.1328048706055,10.1328048706055,10.1328048706055,10.1328048706055,11.2164993286133,11.2164993286133,11.2164993286133,11.2164993286133,11.2164993286133,11.2164993286133,11.2164993286133,13.6904983520508,13.6904983520508,13.6904983520508,13.6904983520508,13.6904983520508,13.6904983520508,13.6904983520508,13.6904983520508,13.6904983520508,13.6904983520508,13.6904983520508,16.9504776000977,16.9504776000977,16.9504776000977,16.9504776000977,16.9504776000977,16.9504776000977,16.9504776000977,16.9504776000977,16.9504776000977,16.9504776000977,16.9504776000977,16.9504776000977,16.9504776000977,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,12.0347900390625,12.0347900390625,12.0347900390625,12.0347900390625,12.0347900390625,12.0347900390625,12.0347900390625,12.0347900390625,12.0347900390625,12.0347900390625,12.0347900390625,15.4500350952148,15.4500350952148,15.4500350952148,15.4500350952148,15.4500350952148,15.4500350952148,15.4500350952148,15.4500350952148,15.4500350952148,15.4500350952148,15.4500350952148,15.4500350952148,17.9291000366211,17.9291000366211,17.9291000366211,17.9291000366211,17.9291000366211,17.9291000366211,17.9291000366211,17.9291000366211,10.5716781616211,10.5716781616211,10.5716781616211,10.5716781616211,10.5716781616211,12.9330139160156,12.9330139160156,12.9330139160156,12.9330139160156,12.9330139160156,12.9330139160156,12.9330139160156,12.9330139160156,12.9330139160156,12.9330139160156,12.9330139160156,12.9330139160156,15.3143997192383,15.3143997192383,15.3143997192383,15.3143997192383,15.3143997192383,15.3143997192383,15.3143997192383,15.3143997192383,17.7379837036133,17.7379837036133,17.7379837036133,17.7379837036133,17.7379837036133,17.7379837036133,17.7379837036133,17.7379837036133,10.8572692871094,10.8572692871094,10.8572692871094,10.8572692871094,10.8572692871094,10.8572692871094,10.8572692871094,13.4903717041016,13.4903717041016,13.4903717041016,13.4903717041016,13.4903717041016,13.4903717041016,17.7217864990234,17.7217864990234,17.7217864990234,17.7217864990234,17.7217864990234,11.2135162353516,11.2135162353516,11.2135162353516,11.2135162353516,11.2135162353516,11.2135162353516,11.2135162353516,11.2135162353516,13.4668731689453,13.4668731689453,13.4668731689453,13.4668731689453,13.4668731689453,13.4668731689453,13.4668731689453,13.4668731689453,15.6137847900391,15.6137847900391,15.6137847900391,15.6137847900391,15.6137847900391,15.6137847900391,15.6137847900391,15.6137847900391,15.6137847900391,15.6137847900391,15.6137847900391,15.6137847900391,15.6137847900391,15.6137847900391,15.6137847900391,17.7718734741211,17.7718734741211,17.7718734741211,17.7718734741211,17.7718734741211,17.7718734741211,17.7718734741211,17.7718734741211,12.3509368896484,12.3509368896484,12.3509368896484,12.3509368896484,12.3509368896484,12.3509368896484,12.3509368896484,12.3509368896484,12.3509368896484,12.3509368896484,12.3509368896484,12.3509368896484,12.3509368896484,12.3509368896484,13.2424163818359,13.2424163818359,13.2424163818359,13.2424163818359,13.2424163818359,13.2424163818359,13.2424163818359,16.7387313842773,16.7387313842773,16.7387313842773,16.7387313842773,16.7387313842773,16.7387313842773,16.7387313842773,16.7387313842773,19.2166061401367,19.2166061401367,19.2166061401367,19.2166061401367,19.2166061401367,19.2166061401367,19.2166061401367,19.2166061401367,19.2166061401367,19.2166061401367,19.2166061401367,19.2166061401367,19.2166061401367,14.5925140380859,14.5925140380859,14.5925140380859,14.5925140380859,14.5925140380859,14.5925140380859,17.8113327026367,19.4695587158203,19.4695587158203,19.4695587158203,19.4695587158203,19.4695587158203,19.4695587158203,19.4695587158203,19.4695587158203,19.4695587158203,19.4695587158203,14.6102905273438,14.6102905273438,14.6102905273438,14.6102905273438,14.6102905273438,14.6102905273438,14.6102905273438,14.6102905273438,17.0448989868164,17.0448989868164,17.0448989868164,17.0448989868164,17.0448989868164,17.0448989868164,17.0448989868164,17.0448989868164,17.0448989868164,19.4713973999023,19.4713973999023,19.4713973999023,19.4713973999023,19.4713973999023,19.4713973999023,19.4713973999023,19.4713973999023,19.4713973999023,19.4713973999023,14.827392578125,14.827392578125,14.827392578125,14.827392578125,14.827392578125,14.827392578125,14.827392578125,14.827392578125,16.814094543457,16.814094543457,16.814094543457,16.814094543457,16.814094543457,16.814094543457,16.814094543457,19.3734130859375,14.4908065795898,14.4908065795898,14.4908065795898,14.4908065795898,14.4908065795898,14.4908065795898,14.4908065795898,14.4908065795898,14.4908065795898,14.4908065795898,14.4908065795898,14.4908065795898,14.4908065795898,14.4908065795898,14.4908065795898,16.6128692626953,16.6128692626953,16.6128692626953,16.6128692626953,16.6128692626953,16.6128692626953,16.6128692626953,16.6128692626953,19.0935592651367,19.0935592651367,19.0935592651367,19.0935592651367,19.0935592651367,19.0935592651367,19.0935592651367,19.0935592651367,19.0935592651367,19.0935592651367,19.0935592651367,15.8304748535156,15.8304748535156,15.8304748535156,15.8304748535156,15.8304748535156,15.8304748535156,15.8304748535156,15.8304748535156,15.8304748535156,15.8304748535156,15.8304748535156,15.8304748535156,15.8304748535156,18.3732528686523,18.3732528686523,18.3732528686523,18.3732528686523,18.3732528686523,18.3732528686523,18.3732528686523,18.3732528686523,18.3732528686523,18.3732528686523,18.3732528686523,18.3732528686523,18.3732528686523,18.3732528686523,18.3732528686523,14.2415618896484,14.2415618896484,14.2415618896484,14.2415618896484,14.2415618896484,14.2415618896484,14.2415618896484,17.0261917114258,17.0261917114258,17.0261917114258,17.0261917114258,17.0261917114258,17.0261917114258,17.0261917114258,17.0261917114258,17.0261917114258,19.4708633422852,19.4708633422852,19.4708633422852,19.4708633422852,19.4708633422852,19.4708633422852,19.4708633422852,19.4708633422852,19.4708633422852,15.4501037597656,18.368896484375,18.368896484375,18.368896484375,18.368896484375,18.368896484375,18.368896484375,18.368896484375,14.0819549560547,14.0819549560547,14.0819549560547,14.0819549560547,14.0819549560547,17.4914169311523,17.4914169311523,17.4914169311523,17.4914169311523,17.4914169311523,17.4914169311523,17.4914169311523,17.4914169311523,17.4914169311523,17.4914169311523,17.4914169311523,17.4914169311523,14.2993240356445,14.2993240356445,14.2993240356445,14.2993240356445,14.2993240356445,14.2993240356445,14.2993240356445,14.2993240356445,14.2993240356445,14.2993240356445,16.850700378418,16.850700378418,16.850700378418,16.850700378418,16.850700378418,16.850700378418,16.850700378418,16.850700378418,16.850700378418,16.850700378418,16.850700378418,16.850700378418,16.850700378418,16.850700378418,16.850700378418,19.4691925048828,19.4691925048828,19.4691925048828,19.4691925048828,19.4691925048828,19.4691925048828,19.4691925048828,16.6172256469727,16.6172256469727,16.6172256469727,16.6172256469727,16.6172256469727,16.6172256469727,16.6172256469727,16.6172256469727,16.6172256469727,19.4512710571289,19.4512710571289,19.4512710571289,19.4512710571289,19.4512710571289,19.4512710571289,19.4512710571289,19.4512710571289,19.4512710571289,19.4512710571289,19.4512710571289,19.4512710571289,19.4512710571289,19.4512710571289,16.3099975585938,16.3099975585938,16.3099975585938,16.3099975585938,16.3099975585938,16.3099975585938,16.3099975585938,16.3099975585938,16.3099975585938,16.3099975585938,16.3099975585938,16.3099975585938,18.4268188476562,18.4268188476562,18.4268188476562,18.4268188476562,18.4268188476562,18.4268188476562,18.4268188476562,18.4268188476562,18.4268188476562,15.5052108764648,15.5052108764648,15.5052108764648,15.5052108764648,15.5052108764648,15.5052108764648,15.5052108764648,15.5052108764648,17.8403625488281,17.8403625488281,17.8403625488281,17.8403625488281,17.8403625488281,17.8403625488281,17.8403625488281,17.8403625488281,17.8403625488281,17.8403625488281,17.8403625488281,17.8403625488281,17.8403625488281,16.8501739501953,16.8501739501953,16.8501739501953,16.8501739501953,16.8501739501953,16.8501739501953,16.8501739501953,16.8501739501953,16.8501739501953,17.8515472412109,17.8515472412109,17.8515472412109,17.8515472412109,17.8515472412109,17.8515472412109,17.8515472412109,15.9848785400391,15.9848785400391,15.9848785400391,15.9848785400391,15.9848785400391,15.9848785400391,15.9848785400391,15.9848785400391,15.9848785400391,15.9848785400391,15.9848785400391,18.5221710205078,18.5221710205078,18.5221710205078,18.5221710205078,18.5221710205078,18.5221710205078,18.5221710205078,18.5221710205078,18.5221710205078,18.5221710205078,18.5221710205078,15.772590637207,15.772590637207,15.772590637207,15.772590637207,15.772590637207,15.772590637207,18.2446594238281,18.2446594238281,18.2446594238281,18.2446594238281,18.2446594238281,18.2446594238281,18.2446594238281,18.2446594238281,18.2446594238281,18.2446594238281,18.2446594238281,18.2446594238281,18.2446594238281,18.2446594238281,11.0720520019531,11.0720520019531,11.0720520019531,11.0720520019531,11.0720520019531,11.0720520019531,11.0720520019531,11.0720520019531,11.0720520019531,11.0720520019531,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,13.5571136474609,16.0669555664062,16.0669555664062,16.0669555664062,16.0669555664062,16.0669555664062,16.0669555664062,16.0669555664062,16.0669555664062,16.0669555664062,16.0669555664062,16.0669555664062,16.0669555664062,16.0669555664062,16.0669555664062,16.0669555664062,19.4710388183594,19.4710388183594,19.4710388183594,19.4710388183594,19.4710388183594,19.4710388183594,19.4710388183594,19.4710388183594,19.4710388183594,19.4710388183594,19.4710388183594,19.4710388183594,19.4710388183594,19.4710388183594,19.4710388183594,12.8432769775391,12.8432769775391,12.8432769775391,12.8432769775391,12.8432769775391,12.8432769775391,12.8432769775391,15.3729705810547,15.3729705810547,15.3729705810547,15.3729705810547,15.3729705810547,15.3729705810547,15.3729705810547,15.3729705810547,15.3729705810547,15.3729705810547,15.3729705810547,17.8315887451172,17.8315887451172,17.8315887451172,17.8315887451172,17.8315887451172,17.8315887451172,17.8315887451172,17.8315887451172,17.8315887451172,11.0713119506836,11.0713119506836,11.0713119506836,11.0713119506836,11.0713119506836,11.0713119506836,11.0713119506836,11.0713119506836,11.0713119506836,12.8923873901367,12.8923873901367,12.8923873901367,12.8923873901367,12.8923873901367,12.8923873901367,12.8923873901367,14.5866317749023,14.5866317749023,14.5866317749023,14.5866317749023,14.5866317749023,14.5866317749023,14.5866317749023,14.5866317749023,14.5866317749023,16.2714920043945,16.2714920043945,16.2714920043945,16.2714920043945,16.2714920043945,16.2714920043945,16.2714920043945,16.2714920043945,16.2714920043945,16.2714920043945,16.2714920043945,16.2714920043945,16.2714920043945,16.2714920043945,18.9182586669922,18.9182586669922,18.9182586669922,18.9182586669922,18.9182586669922,18.9182586669922,12.5637893676758,12.5637893676758,12.5637893676758,12.5637893676758,12.5637893676758,12.5637893676758,14.2995376586914,14.2995376586914,14.2995376586914,14.2995376586914,14.2995376586914,15.8097686767578,15.8097686767578,15.8097686767578,15.8097686767578,15.8097686767578,15.8097686767578,15.8097686767578,15.8097686767578,15.8097686767578,15.8097686767578,15.8097686767578,15.8097686767578,17.458869934082,17.458869934082,17.458869934082,17.458869934082,17.458869934082,17.458869934082,17.458869934082,17.458869934082,17.458869934082,19.2259140014648,19.2259140014648,19.2259140014648,19.2259140014648,19.2259140014648,19.2259140014648,19.2259140014648,19.2259140014648,19.2259140014648,19.2259140014648,12.4259490966797,12.4259490966797,12.4259490966797,12.4259490966797,12.4259490966797,12.4259490966797,12.4259490966797,12.4259490966797,12.4259490966797,12.4259490966797,12.4259490966797,13.9428253173828,13.9428253173828,13.9428253173828,13.9428253173828,13.9428253173828,13.9428253173828,13.9428253173828,13.9428253173828,15.1990051269531,15.1990051269531,15.1990051269531,15.1990051269531,15.1990051269531,15.1990051269531,15.1990051269531,15.1990051269531,15.1990051269531,15.1990051269531,15.1990051269531,16.4758758544922,16.4758758544922,16.4758758544922,16.4758758544922,16.4758758544922,16.4758758544922,16.4758758544922,16.4758758544922,16.4758758544922,18.9310150146484,18.9310150146484,18.9310150146484,18.9310150146484,18.9310150146484,18.9310150146484,18.9310150146484,13.0538864135742,13.0538864135742,13.0538864135742,13.0538864135742,13.0538864135742,13.0538864135742,13.0538864135742,13.0538864135742,15.7003555297852,15.7003555297852,15.7003555297852,15.7003555297852,15.7003555297852,15.7003555297852,15.7003555297852,15.7003555297852,15.7003555297852,15.7003555297852,15.7003555297852,15.7003555297852,19.009162902832,19.009162902832,19.009162902832,19.009162902832,13.4596786499023,13.4596786499023,13.4596786499023,13.4596786499023,13.4596786499023,13.4596786499023,13.4596786499023,13.4596786499023,13.4596786499023,13.4596786499023,13.4596786499023,13.4596786499023,13.4596786499023,13.4596786499023,15.9796295166016,15.9796295166016,15.9796295166016,15.9796295166016,15.9796295166016,15.9796295166016,15.9796295166016,15.9796295166016,15.9796295166016,15.9796295166016,15.9796295166016,17.6804504394531,17.6804504394531,17.6804504394531,17.6804504394531,17.6804504394531,17.6804504394531,17.6804504394531,17.6804504394531,17.6804504394531,17.6804504394531,17.6804504394531,17.6804504394531,17.6804504394531,17.6804504394531,19.3667068481445,19.3667068481445,19.3667068481445,19.3667068481445,19.3667068481445,19.3667068481445,19.3667068481445,19.3667068481445,13.1686859130859,13.1686859130859,13.1686859130859,13.1686859130859,13.1686859130859,13.1686859130859,13.1686859130859,13.1686859130859,13.1686859130859,13.1686859130859,13.1686859130859,13.1686859130859,13.1686859130859,15.3592071533203,15.3592071533203,15.3592071533203,15.3592071533203,15.3592071533203,15.3592071533203,15.3592071533203,15.3592071533203,15.3592071533203,17.9128646850586,17.9128646850586,17.9128646850586,17.9128646850586,17.9128646850586,17.9128646850586,17.9128646850586,17.9128646850586,17.9128646850586,19.4619674682617,19.4619674682617,19.4619674682617,19.4619674682617,19.4619674682617,19.4619674682617,19.4619674682617,19.4619674682617,19.4619674682617,19.4619674682617,19.4619674682617,19.4619674682617,19.4619674682617,13.7648239135742,13.7648239135742,13.7648239135742,13.7648239135742,13.7648239135742,13.7648239135742,13.7648239135742,13.7648239135742,17.4450836181641,17.4450836181641,17.4450836181641,17.4450836181641,17.4450836181641,17.4450836181641,17.4450836181641,17.4450836181641,17.4450836181641,17.4450836181641,17.4450836181641,17.4450836181641,13.0851364135742,13.0851364135742,13.0851364135742,13.0851364135742,13.0851364135742,13.0851364135742,13.0851364135742,13.0851364135742,13.0851364135742,15.125114440918,15.125114440918,15.125114440918,15.125114440918,15.125114440918,15.125114440918,15.125114440918,15.125114440918,15.125114440918,17.0812149047852,17.0812149047852,17.0812149047852,17.0812149047852,17.0812149047852,17.0812149047852,17.0812149047852,17.0812149047852,17.0812149047852,17.0812149047852,17.0812149047852,17.0812149047852,17.0812149047852,17.0812149047852,19.4690093994141,19.4690093994141,19.4690093994141,19.4690093994141,19.4690093994141,15.6015777587891,15.6015777587891,15.6015777587891,15.6015777587891,15.6015777587891,15.6015777587891,15.6015777587891,15.6015777587891,15.6015777587891,15.6015777587891,15.6015777587891,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,17.7608337402344,16.0296630859375,16.0296630859375,16.0296630859375,16.0296630859375,16.0296630859375,16.0296630859375,16.0296630859375,16.0296630859375,16.0296630859375,16.0296630859375,16.0296630859375,16.0296630859375,16.0296630859375,16.0296630859375,16.0296630859375,15.4989471435547,15.4989471435547,15.4989471435547,15.4989471435547,15.4989471435547,15.4989471435547,15.4989471435547,18.0326614379883,18.0326614379883,18.0326614379883,18.0326614379883,18.0326614379883,18.0326614379883,18.0326614379883,18.0326614379883,18.0326614379883,18.0326614379883,18.0326614379883,18.0326614379883,14.2085266113281,14.2085266113281,14.2085266113281,14.2085266113281,14.2085266113281,14.2085266113281,14.2085266113281,14.2085266113281,14.2085266113281,16.4824371337891,16.4824371337891,16.4824371337891,16.4824371337891,16.4824371337891,16.4824371337891,16.4824371337891,16.4824371337891,16.4824371337891,16.4824371337891,16.4824371337891,16.4824371337891,16.4824371337891,16.4824371337891,19.0236968994141,19.0236968994141,19.0236968994141,19.0236968994141,19.0236968994141,19.0236968994141,19.0236968994141,19.0236968994141,14.7153091430664,14.7153091430664,14.7153091430664,14.7153091430664,14.7153091430664,14.7153091430664,14.7153091430664,14.7153091430664,14.7153091430664,14.7153091430664,14.7153091430664,14.7153091430664,16.9382553100586,16.9382553100586,16.9382553100586,16.9382553100586,16.9382553100586,16.9382553100586,16.9382553100586,16.9382553100586,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,19.4718856811523,16.707145690918,16.707145690918,16.707145690918,16.707145690918,16.707145690918,16.707145690918,16.707145690918,16.707145690918,16.707145690918,19.4709930419922,19.4709930419922,19.4709930419922,19.4709930419922,19.4709930419922,19.4709930419922,19.4709930419922,19.4709930419922,19.4709930419922,16.4282913208008,16.4282913208008,16.4282913208008,16.4282913208008,16.4282913208008,16.4282913208008,16.4282913208008,16.4282913208008,19.2782592773438,19.2782592773438,19.2782592773438,19.2782592773438,19.2782592773438,19.2782592773438,19.2782592773438,19.2782592773438,19.2782592773438,19.2782592773438,19.2782592773438,19.2782592773438,16.9286727905273,16.9286727905273,16.9286727905273,16.9286727905273,19.4693145751953,19.4693145751953,19.4693145751953,19.4693145751953,19.4693145751953,19.4693145751953,19.4693145751953,19.4693145751953,19.4693145751953,19.4693145751953,19.4693145751953,16.5810470581055,16.5810470581055,16.5810470581055,16.5810470581055,16.5810470581055,16.5810470581055,18.8921432495117,18.8921432495117,18.8921432495117,18.8921432495117,18.8921432495117,18.8921432495117,18.8921432495117,18.8921432495117,18.8921432495117,16.9579162597656,16.9579162597656,16.9579162597656,16.9579162597656,16.9579162597656,16.9579162597656,16.9579162597656,16.9579162597656,16.9579162597656,16.9579162597656,16.9579162597656,16.9579162597656,16.9579162597656,16.9579162597656,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,19.2821044921875,16.7337265014648,16.7337265014648,16.7337265014648,16.7337265014648,16.7337265014648,16.7337265014648,16.7337265014648,16.7337265014648,16.7337265014648,16.7337265014648,16.7337265014648,16.7337265014648,16.7337265014648,16.7337265014648,19.470458984375,19.470458984375,19.470458984375,19.470458984375,19.470458984375,19.470458984375,19.470458984375,17.5033569335938,17.5033569335938,17.5033569335938,17.5033569335938,17.5033569335938,17.5033569335938,17.5033569335938,17.5033569335938,17.5033569335938,17.5033569335938,17.5033569335938,17.5033569335938,17.5033569335938,17.5033569335938,17.5033569335938,15.4682235717773,15.4682235717773,15.4682235717773,15.4682235717773,15.4682235717773,15.4682235717773,15.4682235717773,15.4682235717773,15.4682235717773,12.8358383178711,12.8358383178711,12.8358383178711,12.8358383178711,12.8358383178711,12.8358383178711,12.8358383178711,12.8358383178711,12.8358383178711,12.8358383178711,12.8358383178711,12.8358383178711,12.8358383178711,12.8358383178711,14.4966735839844,14.4966735839844,14.4966735839844,14.4966735839844,14.4966735839844,14.4966735839844,16.8012084960938,16.8012084960938,16.8012084960938,16.8012084960938,16.8012084960938,19.3356170654297,19.3356170654297,19.3356170654297,19.3356170654297,19.3356170654297,19.3356170654297,19.3356170654297,19.3356170654297,19.3356170654297,19.3356170654297,19.3356170654297,11.7340469360352,11.7340469360352,11.7340469360352,11.7340469360352,11.7340469360352,11.7340469360352,11.7340469360352,11.7340469360352,11.7340469360352,11.7340469360352,11.7340469360352,14.0257034301758,14.0257034301758,14.0257034301758,14.0257034301758,14.0257034301758,14.0257034301758,14.0257034301758,14.0257034301758,14.0257034301758,14.0257034301758,14.0257034301758,14.0257034301758,14.0257034301758,14.0257034301758,15.4601745605469,15.4601745605469,15.4601745605469,15.4601745605469,15.4601745605469,15.4601745605469,15.4601745605469,15.4601745605469,15.4601745605469,15.4601745605469,15.4601745605469,15.4601745605469,15.4601745605469,15.4601745605469,15.4601745605469,16.5033645629883,16.5033645629883,16.5033645629883,16.5033645629883,16.5033645629883,16.5033645629883,16.5033645629883,18.0008697509766,18.0008697509766,18.0008697509766,18.0008697509766,18.0008697509766,18.0008697509766,18.0008697509766,18.0008697509766,13.5912399291992,13.5912399291992,13.5912399291992,13.5912399291992,13.5912399291992,13.5912399291992,13.5912399291992,13.5912399291992,13.5912399291992,13.5912399291992,12.3247604370117,12.3247604370117,12.3247604370117,12.3247604370117,12.3247604370117,12.3247604370117,12.3247604370117,12.3247604370117,12.3247604370117,12.3247604370117,12.3247604370117,12.3247604370117,14.0155029296875,14.0155029296875,14.0155029296875,14.0155029296875,14.0155029296875,14.0155029296875,14.0155029296875,14.0155029296875,14.0155029296875,14.0155029296875,14.0155029296875,15.2146759033203,15.2146759033203,15.2146759033203,15.2146759033203,15.2146759033203,15.2146759033203,15.2146759033203,15.2146759033203,15.2146759033203,15.2146759033203,15.2146759033203,15.2146759033203,16.983268737793,16.983268737793,16.983268737793,16.983268737793,16.983268737793,16.983268737793,16.983268737793,16.983268737793,16.983268737793,19.442253112793,19.442253112793,19.442253112793,19.442253112793,19.442253112793,19.442253112793,19.442253112793,11.4447326660156,11.4447326660156,11.4447326660156,11.4447326660156,11.4447326660156,11.4447326660156,11.4447326660156,11.4447326660156,12.6961822509766,12.6961822509766,12.6961822509766,12.6961822509766,12.6961822509766,12.6961822509766,14.242561340332,14.242561340332,14.242561340332,14.242561340332,14.242561340332,14.242561340332,14.242561340332,14.242561340332,16.0778579711914,16.0778579711914,16.0778579711914,16.0778579711914,16.0778579711914,16.0778579711914,16.0778579711914,16.0778579711914,16.0778579711914,16.0778579711914,16.0778579711914,16.0778579711914,16.0778579711914,16.0778579711914,16.0778579711914,18.1816558837891,18.1816558837891,18.1816558837891,18.1816558837891,18.1816558837891,18.1816558837891,18.1816558837891,18.1816558837891,18.1816558837891,18.1816558837891,18.1816558837891,18.1816558837891,18.1816558837891,11.112548828125,11.112548828125,11.112548828125,11.112548828125,11.112548828125,11.112548828125,11.112548828125,13.1269073486328,13.1269073486328,13.1269073486328,13.1269073486328,13.1269073486328,13.1269073486328,13.1269073486328,13.1269073486328,13.1269073486328,13.1269073486328,13.1269073486328,13.1269073486328,13.1269073486328,15.2535705566406,15.2535705566406,15.2535705566406,15.2535705566406,15.2535705566406,15.2535705566406,17.7766265869141,17.7766265869141,17.7766265869141,17.7766265869141,17.7766265869141,17.7766265869141,17.7766265869141,17.7766265869141,17.7766265869141,17.7766265869141,17.7766265869141,19.4718399047852,19.4718399047852,19.4718399047852,19.4718399047852,19.4718399047852,19.4718399047852,19.4718399047852,19.4718399047852,19.4718399047852,19.4718399047852,19.4718399047852,19.4718399047852,19.4718399047852,13.1501693725586,13.1501693725586,13.1501693725586,13.1501693725586,13.1501693725586,13.1501693725586,13.1501693725586,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,15.4621505737305,18.4950790405273,18.4950790405273,18.4950790405273,18.4950790405273,18.4950790405273,18.4950790405273,18.4950790405273,12.0688705444336,12.0688705444336,12.0688705444336,12.0688705444336,12.0688705444336,12.0688705444336,12.0688705444336,12.0688705444336,12.0688705444336,12.0688705444336,12.0688705444336,12.0688705444336,14.2892150878906,14.2892150878906,14.2892150878906,14.2892150878906,14.2892150878906,14.2892150878906,14.2892150878906,14.2892150878906,14.2892150878906,14.2892150878906,14.2892150878906,14.2892150878906,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,16.4028625488281,18.4428863525391,18.4428863525391,18.4428863525391,12.0068664550781,12.0068664550781,12.0068664550781,12.0068664550781,12.0068664550781,12.0068664550781,12.0068664550781,12.0068664550781,12.0068664550781,12.0068664550781,14.0270690917969,14.0270690917969,14.0270690917969,14.0270690917969,14.0270690917969,14.0270690917969,14.0270690917969,14.0270690917969,14.0270690917969,14.0270690917969,17.3934783935547,17.3934783935547,17.3934783935547,17.3934783935547,17.3934783935547,17.3934783935547,17.3934783935547,17.3934783935547,18.3302917480469,18.3302917480469,18.3302917480469,18.3302917480469,18.3302917480469,18.3302917480469,18.3302917480469,18.3302917480469,12.8767013549805,12.8767013549805,12.8767013549805,12.8767013549805,12.8767013549805,12.8767013549805,12.8767013549805,12.8767013549805,12.8767013549805,12.8767013549805,12.8767013549805,12.8767013549805,15.0243530273438,15.0243530273438,15.0243530273438,15.0243530273438,16.3813781738281,16.3813781738281,16.3813781738281,16.3813781738281,16.3813781738281,16.3813781738281,16.3813781738281,16.3813781738281,16.3813781738281,17.5636138916016,17.5636138916016,17.5636138916016,17.5636138916016,17.5636138916016,17.5636138916016,17.5636138916016,17.5636138916016,17.5636138916016,19.3230209350586,19.3230209350586,19.3230209350586,19.3230209350586,19.3230209350586,19.3230209350586,19.3230209350586,19.3230209350586,19.3230209350586,19.3230209350586,12.8919448852539,12.8919448852539,12.8919448852539,12.8919448852539,12.8919448852539,12.8919448852539,12.8919448852539,13.4178314208984,13.4178314208984,13.4178314208984,13.4178314208984,13.4178314208984,13.4178314208984,13.4178314208984,13.4178314208984,13.4178314208984,14.6625213623047,14.6625213623047,14.6625213623047,14.6625213623047,14.6625213623047,14.6625213623047,14.6625213623047,14.6625213623047,14.6625213623047,14.6625213623047,16.2847137451172,16.2847137451172,16.2847137451172,16.2847137451172,16.2847137451172,16.2847137451172,16.2847137451172,16.2847137451172,16.2847137451172,16.2847137451172,17.9728012084961,17.9728012084961,17.9728012084961,17.9728012084961,17.9728012084961,17.9728012084961,17.9728012084961,19.4699325561523,19.4699325561523,19.4699325561523,19.4699325561523,19.4699325561523,19.4699325561523,19.4699325561523,19.4699325561523,19.4699325561523,13.2043609619141,13.2043609619141,13.2043609619141,13.2043609619141,13.2043609619141,13.2043609619141,13.2043609619141,13.2043609619141,13.2043609619141,13.2043609619141,13.2043609619141,13.2043609619141,14.6782150268555,14.6782150268555,14.6782150268555,14.6782150268555,14.6782150268555,14.6782150268555,14.6782150268555,14.6782150268555,14.6782150268555,14.6782150268555,16.1704330444336,16.1704330444336,16.1704330444336,16.1704330444336,16.1704330444336,16.1704330444336,16.1704330444336,16.1704330444336,16.1704330444336,16.1704330444336,16.1704330444336,16.1704330444336,16.1704330444336,16.1704330444336,16.1704330444336,18.0311584472656,18.0311584472656,18.0311584472656,18.0311584472656,18.0311584472656,18.0311584472656,18.0311584472656,18.0311584472656,18.8208770751953,18.8208770751953,18.8208770751953,18.8208770751953,18.8208770751953,18.8208770751953,18.8208770751953,19.4715423583984,19.4715423583984,19.4715423583984,19.4715423583984,19.4715423583984,19.4715423583984,19.4715423583984,19.4715423583984,19.4715423583984,19.4715423583984,13.9003372192383,13.9003372192383,13.9003372192383,13.9003372192383,13.9003372192383,13.9003372192383,13.9003372192383,13.9003372192383,13.9003372192383,13.9003372192383,15.9364700317383,15.9364700317383,15.9364700317383,15.9364700317383,15.9364700317383,15.9364700317383,15.9364700317383,15.9364700317383,15.9364700317383,15.9364700317383,18.1597595214844,18.1597595214844,18.1597595214844,18.1597595214844,18.1597595214844,19.4705123901367,19.4705123901367,19.4705123901367,19.4705123901367,19.4705123901367,19.4705123901367,19.4705123901367,19.4705123901367,19.4705123901367,14.2523040771484,14.2523040771484,14.2523040771484,14.2523040771484,14.2523040771484,14.2523040771484,14.2523040771484,14.2523040771484,14.2523040771484,16.7392272949219,16.7392272949219,16.7392272949219,16.7392272949219,16.7392272949219,16.7392272949219,16.7392272949219,16.7392272949219,17.7104339599609,17.7104339599609,17.7104339599609,17.7104339599609,17.7104339599609,17.7104339599609,17.7104339599609,18.7667083740234,18.7667083740234,18.7667083740234,18.7667083740234,18.7667083740234,18.7667083740234,18.7667083740234,18.7667083740234,18.7667083740234,18.7667083740234,18.7667083740234,18.7667083740234,18.7667083740234,18.7667083740234,18.7667083740234,19.4709854125977,19.4709854125977,19.4709854125977,19.4709854125977,19.4709854125977,14.2807769775391,14.2807769775391,14.2807769775391,14.2807769775391,14.2807769775391,14.2807769775391,14.2807769775391,14.2807769775391,14.2807769775391,14.2807769775391,14.2807769775391,14.2807769775391,14.2807769775391,14.2807769775391,15.7012329101562,15.7012329101562,15.7012329101562,15.7012329101562,15.7012329101562,15.7012329101562,15.7012329101562,15.7012329101562,15.7012329101562,15.7012329101562,15.7012329101562,17.472900390625,17.472900390625,17.472900390625,17.472900390625,17.472900390625,17.472900390625,17.472900390625,17.472900390625,19.4455261230469,19.4455261230469,19.4455261230469,19.4455261230469,19.4455261230469,19.4455261230469,19.4455261230469,19.4455261230469,19.4455261230469,19.4455261230469,19.4455261230469,19.4455261230469,15.0270233154297,15.0270233154297,15.0270233154297,15.0270233154297,15.0270233154297,15.0270233154297,15.0270233154297,15.0270233154297,17.9521026611328,17.9521026611328,17.9521026611328,17.9521026611328,17.9521026611328,17.9521026611328,17.9521026611328,17.9521026611328,14.0243453979492,14.0243453979492,14.0243453979492,14.0243453979492,14.0243453979492,14.0243453979492,14.0243453979492,14.0243453979492,14.0243453979492,14.0243453979492,14.0243453979492,14.0243453979492,14.0243453979492,16.4496765136719,16.4496765136719,16.4496765136719,16.4496765136719,16.4496765136719,16.4496765136719,16.4496765136719,16.4496765136719,16.4496765136719,16.4496765136719,16.4496765136719,18.4079055786133,18.4079055786133,18.4079055786133,18.4079055786133,18.4079055786133,18.4079055786133,18.4079055786133,18.4079055786133,18.4079055786133,18.4079055786133,18.4079055786133,18.4079055786133,18.4079055786133,14.8623352050781,14.8623352050781,14.8623352050781,14.8623352050781,14.8623352050781,14.8623352050781,14.8623352050781,14.8623352050781,18.0454483032227,18.0454483032227,18.0454483032227,18.0454483032227,18.0454483032227,18.0454483032227,18.0454483032227,18.0454483032227,18.0454483032227,18.0454483032227,18.0454483032227,18.0454483032227,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,15.0535125732422,17.1482620239258,17.1482620239258,17.1482620239258,17.1482620239258,17.1482620239258,17.1482620239258,17.1482620239258,18.2749099731445,18.2749099731445,18.2749099731445,18.2749099731445,18.2749099731445,18.2749099731445,18.2749099731445,18.2749099731445,18.2749099731445,18.2749099731445,18.2749099731445,18.2749099731445,18.2749099731445,19.4641494750977,19.4641494750977,19.4641494750977,19.4641494750977,19.4641494750977,19.4641494750977,19.4641494750977,19.4641494750977,19.4641494750977,19.4641494750977,19.4641494750977,19.4641494750977,15.6169586181641,15.6169586181641,15.6169586181641,15.6169586181641,15.6169586181641,15.6169586181641,15.6169586181641,15.6169586181641,15.6169586181641,15.6169586181641,15.6169586181641,15.6169586181641,15.6169586181641,17.263069152832,17.263069152832,17.263069152832,17.263069152832,17.263069152832,17.263069152832,17.263069152832,17.263069152832,17.263069152832,19.2010803222656,19.2010803222656,19.2010803222656,19.2010803222656,19.2010803222656,19.2010803222656,19.2010803222656,19.2010803222656,19.2010803222656,19.2010803222656,19.2010803222656,15.6317291259766,15.6317291259766,15.6317291259766,15.6317291259766,15.6317291259766,15.6317291259766,15.6317291259766,15.6317291259766,17.8001708984375,17.8001708984375,17.8001708984375,17.8001708984375,17.8001708984375,17.8001708984375,15.7318572998047,15.7318572998047,15.7318572998047,15.7318572998047,15.7318572998047,15.7318572998047,15.7318572998047,15.7318572998047,15.7318572998047,15.7318572998047,15.7318572998047,17.3708114624023,17.3708114624023,17.3708114624023,17.3708114624023,17.3708114624023,17.3708114624023,17.3708114624023,17.3708114624023,18.8849639892578,18.8849639892578,18.8849639892578,18.8849639892578,18.8849639892578,18.8849639892578,17.0189056396484,17.0189056396484,17.0189056396484,17.0189056396484,17.0189056396484,17.0189056396484,17.0189056396484,17.0189056396484,18.9646453857422,18.9646453857422,18.9646453857422,18.9646453857422,18.9646453857422,18.9646453857422,18.9646453857422,18.9646453857422,18.9646453857422,18.9646453857422,18.9646453857422,18.9646453857422,12.0951232910156,12.0951232910156,12.0951232910156,12.0951232910156,12.0951232910156,12.0951232910156,12.0951232910156,12.0951232910156,12.0951232910156,12.0951232910156,12.0951232910156,14.6537094116211,14.6537094116211,14.6537094116211,14.6537094116211,14.6537094116211,14.6537094116211,14.6537094116211,16.9664306640625,16.9664306640625,16.9664306640625,16.9664306640625,16.9664306640625,16.9664306640625,16.9664306640625,16.9664306640625,16.9664306640625,16.9664306640625,16.9664306640625,16.9664306640625,19.0249786376953,19.0249786376953,19.0249786376953,19.0249786376953,19.0249786376953,19.0249786376953,19.0249786376953,19.0249786376953,19.0249786376953,11.4948043823242,11.4948043823242,11.4948043823242,11.4948043823242,11.4948043823242,11.4948043823242,16.3590240478516,16.3590240478516,16.3590240478516,16.3590240478516,16.3590240478516,16.3590240478516,16.3590240478516,16.3590240478516,16.3590240478516,16.3590240478516,16.3590240478516,16.3590240478516,18.2031784057617,18.2031784057617,18.2031784057617,18.2031784057617,18.2031784057617,18.2031784057617,18.2031784057617,18.2031784057617,18.2031784057617,18.2031784057617,18.2031784057617,18.2031784057617,11.5611877441406,11.5611877441406,11.5611877441406,11.5611877441406,11.5611877441406,11.5611877441406,11.5611877441406,11.5611877441406,11.5611877441406,11.5611877441406,11.5611877441406,11.5611877441406,11.5611877441406,13.8758392333984,13.8758392333984,13.8758392333984,13.8758392333984,13.8758392333984,13.8758392333984,13.8758392333984,13.8758392333984,16.0622329711914,16.0622329711914,16.0622329711914,16.0622329711914,16.0622329711914,16.0622329711914,16.0622329711914,16.0622329711914,16.0622329711914,16.0622329711914,18.4002075195312,18.4002075195312,18.4002075195312,18.4002075195312,18.4002075195312,18.4002075195312,18.4002075195312,18.4002075195312,18.4002075195312,18.4002075195312,12.8332672119141,12.8332672119141,12.8332672119141,12.8332672119141,12.8332672119141,12.8332672119141,12.8332672119141,12.8332672119141,12.8332672119141,12.8332672119141,12.8332672119141,15.3961944580078,15.3961944580078,15.3961944580078,15.3961944580078,15.3961944580078,15.3961944580078,15.3961944580078,15.3961944580078,15.3961944580078,15.3961944580078,15.3961944580078,15.3961944580078,15.3961944580078,15.3961944580078,15.3961944580078,17.855598449707,17.855598449707,17.855598449707,17.855598449707,17.855598449707,17.855598449707,12.3621292114258,12.3621292114258,12.3621292114258,12.3621292114258,12.3621292114258,12.3621292114258,12.3621292114258,12.3621292114258,15.0403442382812,15.0403442382812,15.0403442382812,15.0403442382812,15.0403442382812,15.0403442382812,15.0403442382812,15.0403442382812,15.0403442382812,15.0403442382812,15.0403442382812,17.509765625,17.509765625,17.509765625,17.509765625,17.509765625,17.509765625,17.509765625,17.509765625,17.509765625,17.509765625,12.3066558837891,12.3066558837891,12.3066558837891,12.3066558837891,12.3066558837891,12.3066558837891,12.3066558837891,12.3066558837891,12.3066558837891,12.3066558837891,12.3066558837891,14.6355438232422,14.6355438232422,14.6355438232422,14.6355438232422,14.6355438232422,14.6355438232422,14.6355438232422,14.6355438232422,14.6355438232422,14.6355438232422,14.6355438232422,14.6355438232422,14.6355438232422,14.6355438232422,17.3612594604492,17.3612594604492,17.3612594604492,17.3612594604492,17.3612594604492,17.3612594604492,17.3612594604492,17.3612594604492,12.5836791992188,12.5836791992188,12.5836791992188,12.5836791992188,12.5836791992188,12.5836791992188,12.5836791992188,12.5836791992188,12.5836791992188,15.1595001220703,15.1595001220703,15.1595001220703,15.1595001220703,15.1595001220703,15.1595001220703,15.1595001220703,15.1595001220703,15.1595001220703,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,18.4460906982422,13.7711944580078,13.7711944580078,13.7711944580078,13.7711944580078,13.7711944580078,13.7711944580078,16.7483749389648,16.7483749389648,16.7483749389648,16.7483749389648,16.7483749389648,16.7483749389648,16.7483749389648,18.4934234619141,18.4934234619141,18.4934234619141,18.4934234619141,18.4934234619141,18.4934234619141,18.4934234619141,18.4934234619141,18.4934234619141,13.8219833374023,13.8219833374023,13.8219833374023,13.8219833374023,13.8219833374023,13.8219833374023,16.7437896728516,16.7437896728516,16.7437896728516,16.7437896728516,16.7437896728516,16.7437896728516,16.7437896728516,16.7437896728516,16.7437896728516,16.7437896728516,16.7437896728516,16.7437896728516,16.7437896728516,16.7437896728516,13.5017242431641,13.5017242431641,13.5017242431641,13.5017242431641,16.3664245605469,16.3664245605469,16.3664245605469,16.3664245605469,16.3664245605469,16.3664245605469,16.3664245605469,16.3664245605469,16.3664245605469,16.3664245605469,16.3664245605469,16.3664245605469,19.2858657836914,19.2858657836914,19.2858657836914,19.2858657836914,19.2858657836914,19.2858657836914,19.2858657836914,19.2858657836914,19.2858657836914,14.7045288085938,14.7045288085938,14.7045288085938,14.7045288085938,14.7045288085938,14.7045288085938,14.7045288085938,14.7045288085938,14.7045288085938,14.7045288085938,14.7045288085938,18.957405090332,18.957405090332,18.957405090332,18.957405090332,18.957405090332,18.957405090332,18.957405090332,18.957405090332,18.957405090332,18.957405090332,15.3734130859375,15.3734130859375,15.3734130859375,15.3734130859375,15.3734130859375,15.3734130859375,15.3734130859375,15.3734130859375,15.3734130859375,19.4662246704102,19.4662246704102,19.4662246704102,19.4662246704102,19.4662246704102,19.4662246704102,19.4662246704102,19.4662246704102,19.4662246704102,19.4662246704102,19.4662246704102,19.4662246704102,19.4662246704102,19.4662246704102,16.6581573486328,16.6581573486328,16.6581573486328,16.6581573486328,16.6581573486328,16.6581573486328,16.6581573486328,16.6581573486328,16.6581573486328,14.5236282348633,14.5236282348633,14.5236282348633,14.5236282348633,14.5236282348633,14.5236282348633,14.5236282348633,14.5236282348633,14.5236282348633,14.5236282348633,14.5236282348633,17.9290313720703,17.9290313720703,17.9290313720703,17.9290313720703,17.9290313720703,17.9290313720703,17.9290313720703,17.9290313720703,17.9290313720703,17.9290313720703,17.9290313720703,17.9290313720703,15.2509536743164,15.2509536743164,15.2509536743164,15.2509536743164,15.2509536743164,15.2509536743164,15.2509536743164,15.2509536743164,15.2509536743164,15.2509536743164,15.2509536743164,15.2509536743164,15.2509536743164,15.2509536743164,18.1669692993164,18.1669692993164,18.1669692993164,18.1669692993164,18.1669692993164,18.1669692993164,18.1669692993164,18.1669692993164,16.3675994873047,16.3675994873047,16.3675994873047,16.3675994873047,16.3675994873047,16.3675994873047,16.3675994873047,16.3675994873047,16.3675994873047,18.5214004516602,18.5214004516602,18.5214004516602,18.5214004516602,18.5214004516602,18.5214004516602,16.4042587280273,16.4042587280273,16.4042587280273,16.4042587280273,16.4042587280273,16.4042587280273,16.4042587280273,16.4042587280273,16.4042587280273,16.4042587280273,16.0946044921875,16.0946044921875,16.0946044921875,16.0946044921875,16.0946044921875,16.0946044921875,16.0946044921875,16.0946044921875,16.0946044921875,16.0946044921875,16.0946044921875,19.0040817260742,19.0040817260742,19.0040817260742,19.0040817260742,19.0040817260742,19.0040817260742,19.0040817260742,19.0040817260742,19.0040817260742,19.0040817260742,19.0040817260742,19.0040817260742,19.0040817260742,16.6222534179688,16.6222534179688,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,19.4663772583008,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,10.1604080200195,12.9778823852539,12.9778823852539,12.9778823852539,12.9778823852539,12.9778823852539,12.9778823852539,12.9778823852539,12.9778823852539,12.9778823852539,16.8749084472656,16.8749084472656,16.8749084472656,16.8749084472656,16.8749084472656,16.8749084472656,19.4712142944336,19.4712142944336,19.4712142944336,19.4712142944336,19.4712142944336,19.4712142944336,19.4712142944336,19.4712142944336,19.4712142944336,13.6543121337891,13.6543121337891,13.6543121337891,13.6543121337891,13.6543121337891,13.6543121337891,13.6543121337891,13.6543121337891,13.6543121337891,13.6543121337891,16.6842422485352,16.6842422485352,16.6842422485352,16.6842422485352,16.6842422485352,16.6842422485352,16.6842422485352,16.6842422485352,16.6842422485352,16.6842422485352,16.8918380737305,16.8918380737305,16.8918380737305,16.8918380737305,16.8918380737305,16.8918380737305,16.8918380737305,16.8918380737305,16.8918380737305,13.6641387939453,13.6641387939453,13.6641387939453,13.6641387939453,13.6641387939453,13.6641387939453,13.6641387939453,13.6641387939453,13.6641387939453,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,16.5800857543945,19.1392669677734,19.1392669677734,19.1392669677734,19.1392669677734,19.1392669677734,19.1392669677734,19.1392669677734,14.5386047363281,14.5386047363281,14.5386047363281,14.5386047363281,14.5386047363281,14.5386047363281,14.5386047363281,14.5386047363281,14.5386047363281,14.5386047363281,14.5386047363281,14.5386047363281,14.5386047363281,14.5386047363281,16.8621597290039,16.8621597290039,16.8621597290039,16.8621597290039,16.8621597290039,16.8621597290039,16.8621597290039,16.8621597290039,16.8621597290039,16.8621597290039,16.8621597290039,16.8621597290039,16.8621597290039,19.4716033935547,19.4716033935547,19.4716033935547,19.4716033935547,19.4716033935547,19.4716033935547,19.4716033935547,19.4716033935547,19.4716033935547,15.8757553100586,15.8757553100586,15.8757553100586,15.8757553100586,15.8757553100586,15.8757553100586,15.8757553100586,15.8757553100586,15.8757553100586,15.8757553100586,15.8757553100586,15.8757553100586,15.8757553100586,10.8059616088867,10.8059616088867,10.8059616088867,10.8059616088867,10.8059616088867,10.8059616088867,10.8059616088867,10.8059616088867,10.8059616088867,10.8059616088867,13.779182434082,13.779182434082,13.779182434082,13.779182434082,13.779182434082,13.779182434082,13.779182434082,13.779182434082,13.779182434082,13.779182434082,13.779182434082,13.779182434082,13.779182434082,13.779182434082,13.779182434082,16.9501953125,16.9501953125,16.9501953125,16.9501953125,16.9501953125,16.9501953125,19.4701232910156,19.4701232910156,19.4701232910156,19.4701232910156,19.4701232910156,16.5651016235352,16.5651016235352,16.5651016235352,16.5651016235352,19.4706420898438,19.4706420898438,19.4706420898438,19.4706420898438,19.4706420898438,13.6554794311523,13.6554794311523,13.6554794311523,13.6554794311523,13.6554794311523,13.6554794311523,13.6554794311523,13.6554794311523,13.6554794311523,13.6554794311523,13.6554794311523,16.6189956665039,16.6189956665039,16.6189956665039,16.6189956665039,16.6189956665039,16.6189956665039,16.6189956665039,16.6189956665039,19.4340896606445,19.4340896606445,19.4340896606445,14.97216796875,14.97216796875,14.97216796875,14.97216796875,14.97216796875,14.97216796875,17.7730255126953,17.7730255126953,17.7730255126953,17.7730255126953,17.7730255126953,17.7730255126953,17.7730255126953,17.7730255126953,17.7730255126953,17.7730255126953,17.7730255126953,17.7730255126953,12.6942825317383,12.6942825317383,12.6942825317383,15.5167617797852,15.5167617797852,15.5167617797852,15.5167617797852,15.5167617797852,15.5167617797852,15.5167617797852,15.5167617797852,15.5167617797852,15.5167617797852,15.5167617797852,18.6211700439453,18.6211700439453,18.6211700439453,18.6211700439453,18.6211700439453,18.6211700439453,18.6211700439453,18.6211700439453,18.6211700439453,18.6211700439453,18.6211700439453,18.6211700439453,12.2623596191406,12.2623596191406,12.2623596191406,12.2623596191406,12.2623596191406,12.2623596191406,12.2623596191406,12.2623596191406,15.1802520751953,15.1802520751953,15.1802520751953,15.1802520751953,15.1802520751953,15.1802520751953,15.1802520751953,18.0992050170898,18.0992050170898,18.0992050170898,18.0992050170898,18.0992050170898,18.0992050170898,18.0992050170898,12.4316864013672,12.4316864013672,12.4316864013672,12.4316864013672,12.4316864013672,12.4316864013672,12.4316864013672,12.4316864013672,12.4316864013672,12.4316864013672,12.4316864013672,15.2360458374023,15.2360458374023,15.2360458374023,15.2360458374023,15.2360458374023,15.2360458374023,18.4942855834961,18.4942855834961,18.4942855834961,18.4942855834961,18.4942855834961,18.4942855834961,18.4942855834961,18.4942855834961,18.4942855834961,18.4942855834961,18.4942855834961,18.4942855834961,18.4942855834961,13.0190582275391,13.0190582275391,13.0190582275391,13.0190582275391,13.0190582275391,13.0190582275391,13.0190582275391,13.0190582275391,13.0190582275391,13.0190582275391,13.0190582275391,16.252555847168,16.252555847168,16.252555847168,16.252555847168,16.252555847168,16.252555847168,16.252555847168,16.252555847168,16.252555847168,16.252555847168,19.4660263061523,19.4660263061523,19.4660263061523,19.4660263061523,19.4660263061523,19.4660263061523,19.4660263061523,19.4660263061523,19.4660263061523,15.0944976806641,15.0944976806641,15.0944976806641,15.0944976806641,15.0944976806641,15.0944976806641,15.0944976806641,15.0944976806641,18.4876098632812,18.4876098632812,18.4876098632812,18.4876098632812,18.4876098632812,18.4876098632812,18.4876098632812,18.4876098632812,18.4876098632812,18.4876098632812,18.4876098632812,14.3377304077148,14.3377304077148,14.3377304077148,14.3377304077148,14.3377304077148,14.3377304077148,14.3377304077148,14.3377304077148,16.6301574707031,16.6301574707031,16.6301574707031,16.6301574707031,16.6301574707031,16.6301574707031,16.6301574707031,16.6301574707031,16.6301574707031,19.4704360961914,19.4704360961914,15.6666870117188,15.6666870117188,15.6666870117188,15.6666870117188,15.6666870117188,15.6666870117188,15.6666870117188,15.6666870117188,18.6695556640625,18.6695556640625,18.6695556640625,18.6695556640625,18.6695556640625,18.6695556640625,14.3936080932617,14.3936080932617,14.3936080932617,14.3936080932617,14.3936080932617,14.3936080932617,17.4778900146484,17.4778900146484,17.4778900146484,17.4778900146484,17.4778900146484,17.4778900146484,17.4778900146484,17.4778900146484,17.4778900146484,15.8812103271484,15.8812103271484,15.8812103271484,15.8812103271484,15.8812103271484,15.8812103271484,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,19.1672210693359,18.3490142822266,18.3490142822266,18.3490142822266,18.3490142822266,18.3490142822266,18.3490142822266,18.3490142822266,18.3490142822266,18.3490142822266,18.3490142822266,18.3490142822266,18.3490142822266,18.3490142822266,15.231689453125,15.231689453125,15.231689453125,15.231689453125,15.231689453125,15.231689453125,15.231689453125,15.231689453125,15.231689453125,17.5998001098633,17.5998001098633,17.5998001098633,17.5998001098633,17.5998001098633,17.5998001098633,17.5998001098633,17.5998001098633,17.5998001098633,17.5998001098633,17.5998001098633,17.5998001098633,14.484733581543,14.484733581543,14.484733581543,14.484733581543,14.484733581543,14.484733581543,14.484733581543,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,19.0366744995117,16.3618545532227,16.3618545532227,16.3618545532227,16.3618545532227,16.3618545532227,16.3618545532227,16.3618545532227,16.3618545532227,19.4717636108398,19.4717636108398,19.4717636108398,19.4717636108398,19.4717636108398,19.4717636108398,19.4717636108398,19.4717636108398,19.4717636108398,19.4717636108398,18.3773956298828,18.3773956298828,18.3773956298828,18.3773956298828,18.3773956298828,18.3773956298828,18.3773956298828,17.1479797363281,17.1479797363281,17.1479797363281,17.1479797363281,17.1479797363281,17.1479797363281,17.1479797363281,17.1479797363281,19.4365768432617,19.4365768432617,19.4365768432617,19.4365768432617,19.4365768432617,19.4365768432617,19.4365768432617,19.4365768432617,19.4365768432617,19.4365768432617,17.9524154663086,17.9524154663086,17.9524154663086,17.9524154663086,17.9524154663086,17.9524154663086,17.9524154663086,17.9524154663086,17.9524154663086,17.9524154663086,12.9764633178711,12.9764633178711,12.9764633178711,12.9764633178711,12.9764633178711,12.9764633178711,12.9764633178711,12.9764633178711,12.9764633178711,16.1631317138672,16.1631317138672,16.1631317138672,16.1631317138672,16.1631317138672,16.1631317138672,16.1631317138672,16.1631317138672,16.1631317138672,16.1631317138672,16.1631317138672,16.1631317138672,19.2348480224609,19.2348480224609,19.2348480224609,19.2348480224609,19.2348480224609,19.2348480224609,19.2348480224609,19.2348480224609,12.7906188964844,12.7906188964844,12.7906188964844,12.7906188964844,12.7906188964844,12.7906188964844,12.7906188964844,12.7906188964844,12.7906188964844,12.7906188964844,15.7198028564453,15.7198028564453,15.7198028564453,15.7198028564453,15.7198028564453,15.7198028564453,15.7198028564453,15.7198028564453,19.4715957641602,19.4715957641602,19.4715957641602,19.4715957641602,19.4715957641602,19.4715957641602,19.4715957641602,19.4715957641602,19.4715957641602,19.4715957641602,19.4715957641602,13.4121704101562,13.4121704101562,13.4121704101562,13.4121704101562,13.4121704101562,13.4121704101562,13.4121704101562,13.4121704101562,13.4121704101562,13.4121704101562,13.4121704101562,17.0131607055664,17.0131607055664,17.0131607055664,17.0131607055664,17.0131607055664,17.0131607055664,17.0131607055664,17.0131607055664,17.0131607055664,17.0131607055664,17.0131607055664,17.0131607055664,19.4664306640625,19.4664306640625,19.4664306640625,19.4664306640625,19.4664306640625,19.4664306640625,19.4664306640625,19.4664306640625,19.4664306640625,19.4664306640625,19.4664306640625,19.4664306640625,19.4664306640625,19.4664306640625,15.2065277099609,15.2065277099609,15.2065277099609,15.2065277099609,15.2065277099609,15.2065277099609,15.2065277099609,15.2065277099609,15.2065277099609,15.2065277099609,15.2065277099609,15.2065277099609,15.2065277099609,11.9494857788086,11.9494857788086,11.9494857788086,11.9494857788086,11.9494857788086,11.9494857788086,11.9494857788086,11.9494857788086,11.9494857788086,11.9494857788086,11.9494857788086,14.991096496582,14.991096496582,14.991096496582,14.991096496582,14.991096496582,14.991096496582,14.991096496582,14.991096496582,18.1486968994141,18.1486968994141,18.1486968994141,18.1486968994141,18.1486968994141,18.1486968994141,18.1486968994141,18.1486968994141,18.1486968994141,18.1486968994141,18.1486968994141,18.1486968994141,18.1486968994141,15.853874206543,15.853874206543,15.853874206543,15.853874206543,15.853874206543,15.853874206543,15.853874206543,15.853874206543,19.1755523681641,19.1755523681641,19.1755523681641,19.1755523681641,19.1755523681641,19.1755523681641,19.1755523681641,19.1755523681641,19.1755523681641,19.1755523681641,19.1755523681641,19.1755523681641,14.4462203979492,14.4462203979492,14.4462203979492,14.4462203979492,14.4462203979492,14.4462203979492,14.4462203979492,14.4462203979492,14.4462203979492,14.4462203979492,14.4462203979492,17.7743759155273,17.7743759155273,17.7743759155273,17.7743759155273,17.7743759155273,17.7743759155273,17.7743759155273,13.5321197509766,13.5321197509766,13.5321197509766,13.5321197509766,13.5321197509766,13.5321197509766,13.5321197509766,13.5321197509766,13.5321197509766,18.6346969604492,18.6346969604492,18.6346969604492,18.6346969604492,18.6346969604492,18.6346969604492,18.6346969604492,18.6346969604492,18.6346969604492,18.6346969604492,15.2971954345703,15.2971954345703,15.2971954345703,15.2971954345703,15.2971954345703,15.2971954345703,15.2971954345703,15.2971954345703,15.2971954345703,15.2971954345703,15.2971954345703,15.2289810180664,15.2289810180664,15.2289810180664,15.2289810180664,15.2289810180664,15.2289810180664,15.2289810180664,15.2289810180664,15.2289810180664,15.2289810180664,15.2289810180664,15.2289810180664,15.2289810180664,14.0704803466797,14.0704803466797,14.0704803466797,14.0704803466797,14.0704803466797,14.0704803466797,14.0704803466797,14.0704803466797,17.5130004882812,17.5130004882812,17.5130004882812,17.5130004882812,17.5130004882812,17.5130004882812,17.5130004882812,17.5130004882812,15.4508972167969,15.4508972167969,15.4508972167969,15.4508972167969,15.4508972167969,15.4508972167969,15.4508972167969,15.4508972167969,15.4508972167969,15.9467468261719,15.9467468261719,15.9467468261719,15.9467468261719,15.9467468261719,15.9467468261719,15.9467468261719,15.9467468261719,15.9467468261719,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,19.2567977905273,17.2215728759766,17.2215728759766,17.2215728759766,17.2215728759766,17.2215728759766,17.2215728759766,17.2215728759766,17.2215728759766,17.2215728759766,17.2215728759766,17.2215728759766,17.2215728759766,17.7923126220703,17.7923126220703,17.7923126220703,17.7923126220703,17.7923126220703,17.7923126220703,17.7923126220703,16.0854110717773,16.0854110717773,16.0854110717773,16.0854110717773,16.0854110717773,16.0854110717773,16.0854110717773,16.0854110717773,16.0854110717773,16.0854110717773,16.0854110717773,16.0854110717773,16.0854110717773,16.0854110717773,19.4700698852539,19.4700698852539,19.4700698852539,19.4700698852539,19.4700698852539,19.4700698852539,19.4700698852539,19.4700698852539,19.4700698852539,19.4700698852539,19.4700698852539,19.4700698852539,19.4700698852539,19.4700698852539,18.3070831298828,18.3070831298828,18.3070831298828,18.3070831298828,18.3070831298828,18.3070831298828,18.3070831298828,18.3070831298828,18.3070831298828,18.3070831298828,18.3070831298828,18.3070831298828,18.3070831298828,18.3070831298828,11.1222991943359,11.1222991943359,11.1222991943359,11.1222991943359,11.1222991943359,11.1222991943359,11.1222991943359,11.1222991943359,11.1222991943359,11.1222991943359,11.1222991943359,11.1222991943359,11.1222991943359,11.1222991943359,11.1222991943359,14.0449676513672,14.0449676513672,14.0449676513672,14.0449676513672,14.0449676513672,14.0449676513672,14.0449676513672,14.0449676513672,14.0449676513672,14.0449676513672,14.0449676513672,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,17.3524932861328,11.1265487670898,11.1265487670898,11.1265487670898,11.1265487670898,11.1265487670898,11.1265487670898,11.1265487670898,13.7215957641602,13.7215957641602,13.7215957641602,13.7215957641602,13.7215957641602,13.7215957641602,13.7215957641602,13.7215957641602,19.4694061279297,19.4694061279297,19.4694061279297,19.4694061279297,19.4694061279297,13.9728317260742,13.9728317260742,13.9728317260742,13.9728317260742,13.9728317260742,13.9728317260742,13.9728317260742,13.9728317260742,13.9728317260742,13.9728317260742,13.9728317260742,13.9728317260742,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,17.2817993164062,11.2166137695312,11.2166137695312,11.2166137695312,11.2166137695312,11.2166137695312,11.2166137695312,11.2166137695312,11.2166137695312,11.2166137695312,11.2166137695312,11.2166137695312,11.2166137695312,14.5170440673828,14.5170440673828,14.5170440673828,14.5170440673828,14.5170440673828,14.5170440673828,14.5170440673828,14.5170440673828,14.5170440673828,14.5170440673828,14.5170440673828,17.8289642333984,17.8289642333984,17.8289642333984,17.8289642333984,17.8289642333984,12.1861190795898,12.1861190795898,12.1861190795898,12.1861190795898,12.1861190795898,12.1861190795898,12.1861190795898,15.4927215576172,15.4927215576172,15.4927215576172,15.4927215576172,15.4927215576172,15.4927215576172,15.4927215576172,15.4927215576172,15.4927215576172,10.7969360351562,10.7969360351562,10.7969360351562,10.7969360351562,10.7969360351562,10.7969360351562,10.7969360351562,10.7969360351562,15.349235534668,15.349235534668,15.349235534668,15.349235534668,15.349235534668,15.349235534668,15.349235534668,15.349235534668,18.4332809448242,18.4332809448242,18.4332809448242,18.4332809448242,18.4332809448242,18.4332809448242,18.4332809448242,18.4332809448242,12.0349807739258,12.0349807739258,12.0349807739258,12.0349807739258,12.0349807739258,12.0349807739258,12.0349807739258,12.0349807739258,12.0349807739258,15.3510665893555,15.3510665893555,15.3510665893555,15.3510665893555,15.3510665893555,15.3510665893555,15.3510665893555,15.3510665893555,15.3510665893555,15.3510665893555,15.3510665893555,11.8898315429688,11.8898315429688,11.8898315429688,11.8898315429688,11.8898315429688,11.8898315429688,11.8898315429688,11.8898315429688,11.8898315429688,14.8721008300781,14.8721008300781,14.8721008300781,14.8721008300781,14.8721008300781,14.8721008300781,14.8721008300781,14.8721008300781,14.8721008300781,14.8721008300781,14.8721008300781,14.8721008300781,14.8721008300781,14.8721008300781,14.8721008300781,18.1944580078125,18.1944580078125,18.1944580078125,18.1944580078125,18.1944580078125,18.1944580078125,18.1944580078125,18.1944580078125,18.1944580078125,18.1944580078125,18.1944580078125,18.1944580078125,18.1944580078125,18.1944580078125,18.1944580078125,12.6527404785156,12.6527404785156,12.6527404785156,12.6527404785156,12.6527404785156,12.6527404785156,12.6527404785156,12.6527404785156,12.6527404785156,12.6527404785156,12.6527404785156,12.6527404785156,16.323112487793,16.323112487793,16.323112487793,16.323112487793,16.323112487793,16.323112487793,16.323112487793,19.4717102050781,19.4717102050781,19.4717102050781,19.4717102050781,19.4717102050781,19.4717102050781,19.4717102050781,19.4717102050781,19.4717102050781,14.0304794311523,14.0304794311523,14.0304794311523,14.0304794311523,14.0304794311523,14.0304794311523,14.0304794311523,14.0304794311523,14.0304794311523,18.5765533447266,18.5765533447266,18.5765533447266,18.5765533447266,18.5765533447266,18.5765533447266,18.5765533447266,18.5765533447266,18.5765533447266,16.0904769897461,16.0904769897461,16.0904769897461,16.0904769897461,16.0904769897461,16.0904769897461,14.200080871582,14.200080871582,14.200080871582,14.200080871582,14.200080871582,14.200080871582,14.200080871582,14.200080871582,14.200080871582,14.200080871582,14.200080871582,14.200080871582,17.0414352416992,17.0414352416992,17.0414352416992,17.0414352416992,17.0414352416992,17.0414352416992,17.0414352416992,17.0414352416992,17.0414352416992,13.5923767089844,13.5923767089844,13.5923767089844,13.5923767089844,13.5923767089844,13.5923767089844,13.5923767089844,13.5923767089844,13.5923767089844,13.5923767089844,16.0762939453125,16.0762939453125,16.0762939453125,16.0762939453125,16.0762939453125,16.0762939453125,16.0762939453125,16.0762939453125,19.1141891479492,19.1141891479492,19.1141891479492,19.1141891479492,19.1141891479492,19.1141891479492,19.1141891479492,19.1141891479492,19.1141891479492,19.1141891479492,19.1141891479492,19.1141891479492,19.1141891479492,19.1141891479492,15.7137298583984,15.7137298583984,15.7137298583984,15.7137298583984,15.7137298583984,15.7137298583984,15.7137298583984,15.7137298583984,15.7137298583984,15.7137298583984,15.7137298583984,15.7137298583984,18.805908203125,18.805908203125,18.805908203125,18.805908203125,18.805908203125,18.805908203125,18.805908203125,18.805908203125,18.805908203125,18.805908203125,18.805908203125,18.805908203125,18.805908203125,18.805908203125,18.805908203125,17.0356216430664,17.0356216430664,17.0356216430664,17.0356216430664,17.0356216430664,17.0356216430664,17.0356216430664,19.4712371826172,19.4712371826172,19.4712371826172,19.4712371826172,19.4712371826172,15.8006439208984,15.8006439208984,15.8006439208984,15.8006439208984,15.8006439208984,15.8006439208984,15.8006439208984,15.8006439208984,18.4204254150391,18.4204254150391,18.4204254150391,18.4204254150391,18.4204254150391,18.4204254150391,18.4204254150391,18.4204254150391,18.4204254150391,18.4204254150391,18.4204254150391,18.4204254150391,18.4204254150391,18.4204254150391,18.4204254150391,14.9723434448242,14.9723434448242,14.9723434448242,14.9723434448242,14.9723434448242,14.9723434448242,14.9723434448242,14.9723434448242,14.9723434448242,18.0745544433594,18.0745544433594,18.0745544433594,18.0745544433594,18.0745544433594,18.0745544433594,18.0745544433594,18.0745544433594,17.5987854003906,17.5987854003906,17.5987854003906,17.5987854003906,17.5987854003906,17.5987854003906,17.5987854003906,17.5987854003906,17.5987854003906,14.8467102050781,14.8467102050781,14.8467102050781,14.8467102050781,14.8467102050781,14.8467102050781,17.7579879760742,17.7579879760742,17.7579879760742,17.7579879760742,17.7579879760742,17.7579879760742,17.7579879760742,17.7579879760742,17.7579879760742,17.7579879760742,17.7579879760742,15.3506851196289,15.3506851196289,15.3506851196289,15.3506851196289,15.3506851196289,18.0620269775391,18.0620269775391,18.0620269775391,18.0620269775391,18.0620269775391,18.0620269775391,18.0620269775391,18.0620269775391,18.0620269775391,15.7798919677734,15.7798919677734,15.7798919677734,15.7798919677734,15.7798919677734,15.7798919677734,15.7798919677734,15.7798919677734,15.7798919677734,15.7798919677734,15.7798919677734,15.7798919677734,16.2198181152344,16.2198181152344,16.2198181152344,16.2198181152344,16.2198181152344,16.2198181152344,16.2198181152344,16.2198181152344,16.2198181152344,16.2198181152344,16.2198181152344,16.2198181152344,19.4711608886719,19.4711608886719,19.4711608886719,19.4711608886719,19.4711608886719,19.4711608886719,19.4711608886719,19.4711608886719,19.4711608886719,17.064826965332,17.064826965332,17.064826965332,17.064826965332,17.064826965332,17.064826965332,17.064826965332,17.064826965332,17.064826965332,19.471061706543,19.471061706543,19.471061706543,19.471061706543,19.471061706543,12.5632095336914,12.5632095336914,12.5632095336914,12.5632095336914,12.5632095336914,12.5632095336914,12.5632095336914,12.5632095336914,12.5632095336914,12.5632095336914,12.5632095336914,12.5632095336914,12.5632095336914,12.5632095336914,12.5632095336914,15.8707427978516,15.8707427978516,15.8707427978516,15.8707427978516,15.8707427978516,15.8707427978516,15.8707427978516,15.8707427978516,15.8707427978516,15.8707427978516,15.8707427978516,15.8707427978516,15.8707427978516,19.4698944091797,19.4698944091797,19.4698944091797,19.4698944091797,19.4698944091797,19.4698944091797,19.4698944091797,14.8907089233398,14.8907089233398,14.8907089233398,14.8907089233398,14.8907089233398,14.8907089233398,14.8907089233398,14.8907089233398,14.8907089233398,14.8907089233398,14.8907089233398,14.8907089233398,18.1952972412109,18.1952972412109,18.1952972412109,18.1952972412109,18.1952972412109,18.1952972412109,18.1952972412109,18.1952972412109,18.1952972412109,11.9520263671875,11.9520263671875,11.9520263671875,11.9520263671875,11.9520263671875,11.9520263671875,11.9520263671875,11.9520263671875,11.9520263671875,11.9520263671875,11.9520263671875,11.9520263671875,11.9520263671875,15.2732086181641,15.2732086181641,15.2732086181641,15.2732086181641,15.2732086181641,15.2732086181641,15.2732086181641,17.8033447265625,17.8033447265625,17.8033447265625,17.8033447265625,17.8033447265625,17.8033447265625,17.8033447265625,17.8033447265625,17.8033447265625,17.8033447265625,17.8033447265625,17.8033447265625,17.8033447265625,17.8033447265625,13.2971725463867,13.2971725463867,13.2971725463867,13.2971725463867,13.2971725463867,13.2971725463867,13.2971725463867,13.2971725463867,13.2971725463867,13.2971725463867,13.2971725463867,13.2971725463867,13.2971725463867,13.2971725463867,13.2971725463867,18.2990264892578,18.2990264892578,18.2990264892578,18.2990264892578,18.2990264892578,18.2990264892578,18.2990264892578,18.2990264892578,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,19.4693603515625,10.2008666992188,10.2008666992188,10.2008666992188,10.2008666992188,10.2008666992188,10.2008666992188,10.2008666992188,10.2008666992188,10.2008666992188,10.2008666992188,10.2008666992188,10.2008666992188,10.2008666992188,10.2008666992188,13.2510147094727,13.2510147094727,13.2510147094727,13.2510147094727,13.2510147094727,13.2510147094727,13.2510147094727,13.2510147094727,13.2510147094727,13.2510147094727,13.2510147094727,13.2510147094727,17.6555709838867,17.6555709838867,17.6555709838867,17.6555709838867,17.6555709838867,11.0259780883789,11.0259780883789,11.0259780883789,11.0259780883789,11.0259780883789,11.0259780883789,11.0259780883789,11.0259780883789,11.0259780883789,11.0259780883789,11.0259780883789,11.0259780883789,11.0259780883789,14.2904205322266,14.2904205322266,14.2904205322266,14.2904205322266,14.2904205322266,14.2904205322266,14.2904205322266,14.2904205322266,14.2904205322266,14.2904205322266,14.2904205322266,17.5034790039062,17.5034790039062,17.5034790039062,17.5034790039062,17.5034790039062,17.5034790039062,17.5034790039062,17.5034790039062,17.5034790039062,17.5034790039062,14.8747482299805,14.8747482299805,14.8747482299805,14.8747482299805,14.8747482299805,14.8747482299805,14.8747482299805,14.8747482299805,14.8747482299805,14.8747482299805,14.8747482299805,14.8747482299805,14.8747482299805,14.8747482299805,19.4712905883789,19.4712905883789,19.4712905883789,19.4712905883789,19.4712905883789,19.4712905883789,19.4712905883789,19.4712905883789,19.4712905883789,19.4712905883789,19.4712905883789,19.4712905883789,19.4712905883789,19.4712905883789,19.4712905883789,14.0605545043945,14.0605545043945,14.0605545043945,14.0605545043945,14.0605545043945,14.0605545043945,18.080207824707,18.080207824707,18.080207824707,18.080207824707,18.080207824707,18.080207824707,18.080207824707,18.080207824707,18.080207824707,12.3252258300781,12.3252258300781,12.3252258300781,12.3252258300781,12.3252258300781,15.6328887939453,15.6328887939453,15.6328887939453,15.6328887939453,15.6328887939453,15.6328887939453,15.6328887939453,15.6328887939453,15.6328887939453,15.6328887939453,13.5350036621094,13.5350036621094,13.5350036621094,13.5350036621094,13.5350036621094,13.5350036621094,13.5350036621094,16.9814834594727,16.9814834594727,16.9814834594727,16.9814834594727,16.9814834594727,16.9814834594727,16.9814834594727,16.9814834594727,12.2295532226562,12.2295532226562,12.2295532226562,12.2295532226562,12.2295532226562,12.2295532226562,12.2295532226562,12.2295532226562,12.2295532226562,15.5563201904297,15.5563201904297,15.5563201904297,15.5563201904297,15.5563201904297,15.5563201904297,15.5563201904297,15.5563201904297,18.9077758789062,18.9077758789062,18.9077758789062,18.9077758789062,18.9077758789062,18.9077758789062,18.9077758789062,18.9077758789062,14.2898406982422,14.2898406982422,14.2898406982422,14.2898406982422,14.2898406982422,14.2898406982422,14.2898406982422,14.2898406982422,14.2898406982422,14.2898406982422,17.5994110107422,17.5994110107422,17.5994110107422,15.6591339111328,15.6591339111328,15.6591339111328,15.6591339111328,15.6591339111328,15.6591339111328,15.6591339111328,15.6591339111328,15.6591339111328,15.6591339111328,15.6591339111328,15.6591339111328,15.1411666870117,15.1411666870117,15.1411666870117,15.1411666870117,15.1411666870117,15.1411666870117,15.1411666870117,15.1411666870117,15.1411666870117,15.1411666870117,18.4563522338867,18.4563522338867,18.4563522338867,18.4563522338867,18.4563522338867,18.4563522338867,18.4563522338867,15.6048126220703,15.6048126220703,15.6048126220703,15.6048126220703,15.6048126220703,15.6048126220703,15.6048126220703,15.6048126220703,15.6048126220703,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.468635559082,19.4694900512695,19.4694900512695,19.4694900512695,19.4694900512695,19.4694900512695,19.4694900512695,19.4694900512695,19.4694900512695,19.4694900512695,19.4694900512695,19.4694900512695,19.4694900512695,16.748649597168,16.748649597168,16.748649597168,16.748649597168,16.748649597168,16.748649597168,16.748649597168,16.748649597168,19.4702606201172,19.4702606201172,19.4702606201172,19.4702606201172,19.4702606201172,19.4702606201172,19.4702606201172,19.4702606201172,19.4702606201172,17.1397399902344,17.1397399902344,17.1397399902344,17.1397399902344,17.1397399902344,17.1397399902344,17.1397399902344,17.1397399902344,17.1397399902344,14.6662216186523,14.6662216186523,14.6662216186523,14.6662216186523,14.6662216186523,14.6662216186523,18.0690383911133,18.0690383911133,18.0690383911133,18.0690383911133,18.0690383911133,18.0690383911133,18.0690383911133,18.0690383911133,17.9448165893555,17.9448165893555,17.9448165893555,17.9448165893555,17.9448165893555,17.9448165893555,17.9448165893555,15.9350204467773,15.9350204467773,15.9350204467773,15.9350204467773,15.9350204467773,15.9350204467773,15.9350204467773,15.9350204467773,15.9350204467773,15.9350204467773,15.9350204467773,15.9350204467773,19.463737487793,19.463737487793,19.463737487793,19.463737487793,19.463737487793,19.463737487793,19.463737487793,19.463737487793,19.463737487793,15.8027267456055,15.8027267456055,15.8027267456055,15.8027267456055,15.8027267456055,15.8027267456055,15.8027267456055,15.8027267456055,15.8027267456055,15.8027267456055,15.8027267456055,15.8027267456055,15.8027267456055,15.8027267456055,15.8027267456055,19.2438583374023,19.2438583374023,19.2438583374023,19.2438583374023,19.2438583374023,19.2438583374023,19.2438583374023,19.2438583374023,19.4704132080078,19.4704132080078,19.4704132080078,19.4704132080078,19.4704132080078,19.4704132080078,19.4704132080078,19.4704132080078,19.4704132080078,14.7952728271484,14.7952728271484,14.7952728271484,14.7952728271484,14.7952728271484,14.7952728271484,14.7952728271484,14.7952728271484,14.7952728271484,14.7952728271484,14.7952728271484,14.7952728271484,18.2395477294922,18.2395477294922,18.2395477294922,18.2395477294922,18.2395477294922,18.2395477294922,18.2395477294922,18.2395477294922,18.2395477294922,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,11.9870376586914,15.3763885498047,15.3763885498047,15.3763885498047,15.3763885498047,15.3763885498047,15.3763885498047,15.3763885498047,15.3763885498047,15.3763885498047,15.3763885498047,15.3763885498047,18.7870025634766,18.7870025634766,18.7870025634766,18.7870025634766,18.7870025634766,18.7870025634766,18.7870025634766,18.7870025634766,12.3506927490234,12.3506927490234,12.3506927490234,12.3506927490234,12.3506927490234,12.3506927490234,12.3506927490234,12.3506927490234,12.3506927490234,12.3506927490234,12.3506927490234,12.3506927490234,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,15.8194122314453,19.4715805053711,19.4715805053711,19.4715805053711,19.4715805053711,19.4715805053711,19.4715805053711,19.4715805053711,19.4715805053711,19.4715805053711,19.4715805053711,15.2448883056641,15.2448883056641,15.2448883056641,15.2448883056641,15.2448883056641,15.2448883056641,15.2448883056641,18.5540084838867,18.5540084838867,18.5540084838867,18.5540084838867,18.5540084838867,18.5540084838867,18.5540084838867,12.5084381103516,12.5084381103516,12.5084381103516,12.5084381103516,12.5084381103516,12.5084381103516,12.5084381103516,16.0043258666992,16.0043258666992,16.0043258666992,16.0043258666992,16.0043258666992,16.0043258666992,16.0043258666992,16.0043258666992,16.0043258666992,16.0043258666992,16.0043258666992,12.7739334106445,12.7739334106445,12.7739334106445,12.7739334106445,12.7739334106445,12.7739334106445,12.7739334106445,17.5632934570312,17.5632934570312,17.5632934570312,17.5632934570312,17.5632934570312,17.5632934570312,17.5632934570312,17.5632934570312,17.5632934570312,17.5632934570312,17.5632934570312,12.8292083740234,12.8292083740234,12.8292083740234,12.8292083740234,12.8292083740234,12.8292083740234,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,16.5847320556641,19.4596939086914,19.4596939086914,19.4596939086914,19.4596939086914,19.4596939086914,19.4596939086914,19.4596939086914,19.4596939086914,19.4596939086914,19.4596939086914,19.4596939086914,19.4596939086914,19.4596939086914,19.4596939086914,18.0697479248047,18.0697479248047,18.0697479248047,18.0697479248047,18.0697479248047,18.0697479248047,18.0697479248047,18.0697479248047,18.0697479248047,18.0697479248047,18.0697479248047,18.0697479248047,18.0697479248047,18.0697479248047,18.0697479248047,13.7283172607422,13.7283172607422,13.7283172607422,13.7283172607422,13.7283172607422,13.7283172607422,13.7283172607422,13.7283172607422,13.7283172607422,13.7283172607422,13.7283172607422,17.4163665771484,17.4163665771484,17.4163665771484,17.4163665771484,17.4163665771484,17.4163665771484,17.4163665771484,17.4163665771484,13.3157730102539,13.3157730102539,13.3157730102539,13.3157730102539,13.3157730102539,13.3157730102539,13.3157730102539,13.3157730102539,17.1063995361328,17.1063995361328,17.1063995361328,17.1063995361328,17.1063995361328,17.1063995361328,17.1063995361328,17.1063995361328,17.1063995361328,17.1063995361328,17.1063995361328,16.3967056274414,16.3967056274414,16.3967056274414,16.3967056274414,16.3967056274414,16.3967056274414,16.3967056274414,16.3967056274414,19.4704437255859,19.4704437255859,17.939208984375,17.939208984375,17.939208984375,17.939208984375,17.939208984375,17.939208984375,17.939208984375,17.939208984375,17.939208984375,17.939208984375,15.5856399536133,15.5856399536133,15.5856399536133,15.5856399536133,15.5856399536133,15.5856399536133,15.5856399536133,15.5856399536133,15.5856399536133,15.5856399536133,15.5856399536133,15.5856399536133,15.5856399536133,15.5856399536133,15.5856399536133,19.2755126953125,19.2755126953125,19.2755126953125,19.2755126953125,19.2755126953125,19.2755126953125,19.2755126953125,19.2755126953125,15.9532470703125,15.9532470703125,15.9532470703125,15.9532470703125,15.9532470703125,15.9532470703125,15.9532470703125,15.9532470703125,15.9532470703125,15.9532470703125,15.9532470703125,15.9532470703125,16.6115036010742,16.6115036010742,16.6115036010742,16.6115036010742,16.6115036010742,16.6115036010742,16.6115036010742,16.6115036010742,19.4637069702148,19.4637069702148,19.4637069702148,19.4637069702148,19.4637069702148,19.4637069702148,19.4637069702148,19.4637069702148,19.4637069702148,19.4637069702148,19.4637069702148,19.4637069702148,19.4637069702148,19.4637069702148,17.5794982910156,17.5794982910156,17.5794982910156,17.5794982910156,17.5794982910156,17.5794982910156,17.5794982910156,17.5794982910156,17.5794982910156,17.5794982910156,17.5794982910156,19.0466079711914,19.0466079711914,19.0466079711914,19.0466079711914,19.0466079711914,19.0466079711914,19.0466079711914,19.0466079711914,19.0466079711914,19.0466079711914,19.0466079711914,19.0466079711914,19.0466079711914,19.0466079711914,19.0466079711914,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.2248611450195,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,17.0645217895508,10.7141418457031,10.7141418457031,10.7141418457031,10.7141418457031,10.7141418457031,10.7141418457031,10.7141418457031,14.3977813720703,14.3977813720703,14.3977813720703,14.3977813720703,14.3977813720703,14.3977813720703,18.0864791870117,18.0864791870117,18.0864791870117,18.0864791870117,18.0864791870117,18.0864791870117,18.0864791870117,11.9039611816406,11.9039611816406,11.9039611816406,11.9039611816406,11.9039611816406,11.9039611816406,11.9039611816406,11.9039611816406,11.9039611816406,11.9039611816406,11.9039611816406,15.82568359375,15.82568359375,15.82568359375,15.82568359375,15.82568359375,15.82568359375,19.4657440185547,19.4657440185547,19.4657440185547,19.4657440185547,19.4657440185547,19.4657440185547,19.4657440185547,19.4657440185547,19.4657440185547,19.4657440185547,14.5268173217773,14.5268173217773,14.5268173217773,14.5268173217773,14.5268173217773,14.5268173217773,14.5268173217773,14.5268173217773,14.5268173217773,14.5268173217773,18.2153701782227,18.2153701782227,18.2153701782227,18.2153701782227,18.2153701782227,18.2153701782227,18.2153701782227,12.3798599243164,12.3798599243164,12.3798599243164,12.3798599243164,12.3798599243164,12.3798599243164,12.3798599243164,12.3798599243164,12.3798599243164,12.3798599243164,12.3798599243164,16.0644226074219,16.0644226074219,16.0644226074219,16.0644226074219,16.0644226074219,16.0644226074219,16.0644226074219,16.0644226074219,16.0644226074219,16.0644226074219,16.0644226074219,16.0644226074219,16.0644226074219,13.0002899169922,13.0002899169922,13.0002899169922,13.0002899169922,13.0002899169922,13.0002899169922,13.0002899169922,13.0002899169922,13.0002899169922,13.0002899169922,13.0002899169922,13.0002899169922,13.0002899169922,17.9228973388672,17.9228973388672,17.9228973388672,17.9228973388672,17.9228973388672,17.9228973388672,17.9228973388672,17.9228973388672,17.9228973388672,17.9228973388672,17.9228973388672,11.6220321655273,11.6220321655273,11.6220321655273,11.6220321655273,11.6220321655273,11.6220321655273,11.6220321655273,11.6220321655273,11.6220321655273,11.6220321655273,11.6220321655273,18.6003875732422,18.6003875732422,18.6003875732422,18.6003875732422,18.6003875732422,18.6003875732422,18.6003875732422,18.6003875732422,18.6003875732422,12.6876068115234,12.6876068115234,12.6876068115234,12.6876068115234,12.6876068115234,12.6876068115234,12.6876068115234,12.6876068115234,12.6876068115234,12.6876068115234,12.6876068115234,12.6876068115234,12.6876068115234,12.6876068115234,12.6876068115234,17.9326858520508,17.9326858520508,17.9326858520508,17.9326858520508,17.9326858520508,17.9326858520508,17.9326858520508,17.9326858520508,14.4179534912109,14.4179534912109,14.4179534912109,14.4179534912109,14.4179534912109,14.4179534912109,14.4179534912109,14.4179534912109,14.4179534912109,14.4179534912109,14.4179534912109,14.4179534912109,14.4179534912109,14.4179534912109,14.4179534912109,19.4691619873047,19.4691619873047,19.4691619873047,19.4691619873047,19.4691619873047,19.4691619873047,19.4691619873047,19.4691619873047,19.4691619873047,16.1799011230469,16.1799011230469,16.1799011230469,16.1799011230469,16.1799011230469,16.1799011230469,16.1799011230469,16.1799011230469,16.1799011230469,16.1799011230469,16.1799011230469,16.1799011230469,16.1799011230469,16.1799011230469,19.46875,19.46875,19.46875,19.46875,19.46875,19.46875,19.46875,19.46875,19.46875,19.46875,19.46875,19.46875,19.46875,14.3999328613281,14.3999328613281,14.3999328613281,14.3999328613281,14.3999328613281,14.3999328613281,14.3999328613281,13.0392379760742,13.0392379760742,16.3509140014648,16.3509140014648,16.3509140014648,16.3509140014648,16.3509140014648,16.3509140014648,16.3509140014648,16.3509140014648,19.470832824707,19.470832824707,19.470832824707,19.470832824707,19.470832824707,15.3969955444336,15.3969955444336,15.3969955444336,15.3969955444336,15.3969955444336,15.3969955444336,15.3969955444336,15.3969955444336,15.3969955444336,15.3969955444336,13.5245361328125,13.5245361328125,13.5245361328125,13.5245361328125,13.5245361328125,13.5245361328125,13.5245361328125,13.5245361328125,13.5245361328125,13.5245361328125,18.3009948730469,18.3009948730469,18.3009948730469,18.3009948730469,18.3009948730469,18.3009948730469,18.3009948730469,18.3009948730469,18.3009948730469,18.3009948730469,18.3009948730469,18.3009948730469,14.0098342895508,14.0098342895508,14.0098342895508,14.0098342895508,14.0098342895508,14.0098342895508,14.0098342895508,14.0098342895508,14.0098342895508,14.0098342895508,14.0098342895508,14.0098342895508,14.0098342895508,14.0098342895508,19.4714660644531,19.4714660644531,19.4714660644531,19.4714660644531,19.4714660644531,19.4714660644531,19.4714660644531,19.4714660644531,19.4714660644531,17.8122177124023,17.8122177124023,17.8122177124023,17.8122177124023,17.8122177124023,17.8122177124023,17.8122177124023,17.8122177124023,17.8122177124023,17.8122177124023,17.8122177124023,17.8122177124023,17.8122177124023,14.9004974365234,14.9004974365234,14.9004974365234,14.9004974365234,14.9004974365234,14.9004974365234,14.9004974365234,14.9004974365234,19.4640960693359,19.4640960693359,19.4640960693359,19.4640960693359,19.4640960693359,19.4640960693359,19.4640960693359,19.4640960693359,19.4640960693359,19.4640960693359,19.4640960693359,19.4640960693359,19.4640960693359,16.8449935913086,16.8449935913086,16.8449935913086,16.8449935913086,16.8449935913086,16.8449935913086,16.8449935913086,14.6578979492188,14.6578979492188,14.6578979492188,14.6578979492188,14.6578979492188,14.6578979492188,17.684326171875,17.684326171875,17.684326171875,17.684326171875,17.684326171875,17.684326171875,17.684326171875,17.684326171875,17.684326171875,17.684326171875,17.684326171875,17.684326171875,17.684326171875,17.684326171875,15.6042633056641,15.6042633056641,15.6042633056641,15.6042633056641,15.6042633056641,15.6042633056641,15.6042633056641,15.6042633056641,15.6042633056641,15.6042633056641,15.6042633056641,15.6042633056641,15.0820007324219,15.0820007324219,15.0820007324219,15.0820007324219,15.0820007324219,15.0820007324219,18.0066070556641,18.0066070556641,18.0066070556641,18.0066070556641,18.0066070556641,18.0066070556641,18.0066070556641,18.0066070556641,18.0066070556641,18.0066070556641,18.0066070556641,16.5043869018555,16.5043869018555,16.5043869018555,16.5043869018555,16.5043869018555,16.5043869018555,16.5043869018555,18.9893798828125,18.9893798828125,18.9893798828125,18.9893798828125,18.9893798828125,18.9893798828125,18.9893798828125,18.9893798828125,18.9893798828125,18.9893798828125,18.9893798828125,18.9893798828125,18.9893798828125,18.9893798828125,18.9893798828125,11.1316528320312,11.1316528320312,11.1316528320312,11.1316528320312,11.1316528320312,11.1316528320312,11.1316528320312,11.1316528320312,11.1316528320312,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,14.8191909790039,18.1428985595703,18.1428985595703,18.1428985595703,18.1428985595703,18.1428985595703,18.1428985595703,18.1428985595703,18.1428985595703,18.1428985595703,12.1883392333984,12.1883392333984,12.1883392333984,12.1883392333984,12.1883392333984,12.1883392333984,12.1883392333984,12.1883392333984,12.1883392333984,12.1883392333984,12.1883392333984,15.6346588134766,15.6346588134766,15.6346588134766,15.6346588134766,15.6346588134766,15.6346588134766,15.6346588134766,15.6346588134766,15.6346588134766,15.6346588134766,15.6346588134766,15.6346588134766,15.6346588134766,15.6346588134766,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,18.755859375,13.3427505493164,13.3427505493164,13.3427505493164,13.3427505493164,13.3427505493164,13.3427505493164,13.3427505493164,13.3427505493164,13.3427505493164,13.3427505493164,13.3427505493164,19.4637756347656,19.4637756347656,19.4637756347656,19.4637756347656,19.4637756347656,19.4637756347656,19.4637756347656,19.4637756347656,19.4637756347656,19.4637756347656,19.4637756347656,19.4637756347656,19.4637756347656,14.7103042602539,14.7103042602539,14.7103042602539,14.7103042602539,14.7103042602539,14.7103042602539,14.7103042602539,14.7103042602539,14.7103042602539,14.7103042602539,14.7103042602539,14.7103042602539,14.7103042602539,14.7103042602539,18.3137664794922,18.3137664794922,18.3137664794922,18.3137664794922,18.3137664794922,18.3137664794922,18.3137664794922,18.3137664794922,18.3137664794922,18.3137664794922,18.3137664794922,18.3137664794922,13.1112976074219,13.1112976074219,13.1112976074219,13.1112976074219,13.1112976074219,13.1112976074219,13.1112976074219,13.1112976074219,13.1112976074219,13.1112976074219,13.1112976074219,13.1112976074219,13.1112976074219,13.1112976074219,16.7036361694336,16.7036361694336,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,19.4605484008789,10.3601760864258,10.3601760864258,10.3601760864258,10.3601760864258,10.3601760864258,10.3601760864258,10.3601760864258,10.3601760864258,10.3601760864258,10.3601760864258,10.3601760864258,10.3601760864258,10.3601760864258,10.3601760864258,10.3601760864258,11.0055999755859,11.0055999755859,11.0055999755859,11.0055999755859,11.0055999755859,11.0055999755859,11.0055999755859,11.0055999755859,11.0055999755859,11.0055999755859,14.236213684082,14.236213684082,14.236213684082,14.236213684082,14.236213684082,14.236213684082,14.236213684082,14.236213684082,14.236213684082,14.236213684082,14.236213684082,14.236213684082,19.4710006713867,19.4710006713867,19.4710006713867,19.4710006713867,19.4710006713867,19.4710006713867,19.4710006713867,19.4710006713867,19.4710006713867,17.2596817016602,17.2596817016602,17.2596817016602,17.2596817016602,17.2596817016602,17.2596817016602,17.2596817016602,17.2596817016602,11.5886077880859,11.5886077880859,11.5886077880859,11.5886077880859,11.5886077880859,11.5886077880859,11.5886077880859,11.5886077880859,11.5886077880859,11.5886077880859,11.5886077880859,15.3699111938477,15.3699111938477,15.3699111938477,15.3699111938477,15.3699111938477,15.3699111938477,15.3699111938477,15.3699111938477,15.3699111938477,15.3699111938477,15.3699111938477,15.3699111938477,15.3699111938477,15.3699111938477,15.3699111938477,19.2319564819336,19.2319564819336,19.2319564819336,19.2319564819336,19.2319564819336,19.2319564819336,19.2319564819336,19.2319564819336,19.2319564819336,19.2319564819336,19.2319564819336,19.2319564819336,13.848274230957,13.848274230957,13.848274230957,13.848274230957,13.848274230957,13.848274230957,13.848274230957,13.848274230957,17.7638931274414,17.7638931274414,17.7638931274414,17.7638931274414,17.7638931274414,17.7638931274414,17.7638931274414,17.7638931274414,12.5732192993164,12.5732192993164,12.5732192993164,12.5732192993164,12.5732192993164,12.5732192993164,18.0127258300781,18.0127258300781,18.0127258300781,18.0127258300781,18.0127258300781,18.0127258300781,18.0127258300781,18.0127258300781,18.0127258300781,18.0127258300781,18.0127258300781,18.0127258300781,18.0127258300781,18.0127258300781,15.3527755737305,15.3527755737305,15.3527755737305,15.3527755737305,15.3527755737305,15.3527755737305,15.3527755737305,15.3527755737305,15.3527755737305,15.3527755737305,15.3527755737305,15.3527755737305,15.3527755737305,15.3527755737305,19.2153625488281,19.2153625488281,19.2153625488281,19.2153625488281,19.2153625488281,19.2153625488281,19.2153625488281,19.2153625488281,19.2153625488281,19.2153625488281,19.2153625488281,19.2153625488281,19.2153625488281,19.2153625488281,15.1926574707031,15.1926574707031,15.1926574707031,15.1926574707031,15.1926574707031,15.1926574707031,15.1926574707031,15.1926574707031,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,19.3831405639648,15.4633941650391,15.4633941650391,15.4633941650391,15.4633941650391,15.4633941650391,15.4633941650391,15.4633941650391,15.4633941650391,15.4633941650391,15.4633941650391,15.4633941650391,15.4633941650391,14.3193359375,14.3193359375,14.3193359375,14.3193359375,14.3193359375,14.3193359375,14.3193359375,14.3193359375,14.3193359375,19.026496887207,19.026496887207,19.026496887207,19.026496887207,19.026496887207,19.026496887207,19.026496887207,19.026496887207,19.026496887207,19.026496887207,19.026496887207,19.026496887207,19.4604415893555,19.4604415893555,19.4604415893555,19.4604415893555,19.4604415893555,19.4604415893555,19.4604415893555,19.4604415893555,19.4604415893555,19.4604415893555,19.4604415893555,19.4604415893555,19.4604415893555,19.4604415893555,16.4506301879883,16.4506301879883,16.4506301879883,16.4506301879883,16.4506301879883,16.4506301879883,16.4506301879883,16.4506301879883,17.47314453125,17.47314453125,17.47314453125,17.47314453125,17.47314453125,17.47314453125,17.47314453125,17.47314453125,17.47314453125,17.47314453125,17.47314453125,15.1531143188477,15.1531143188477,15.1531143188477,15.1531143188477,15.1531143188477,15.1531143188477,18.7961349487305,18.7961349487305,18.7961349487305,18.7961349487305,18.7961349487305,18.7961349487305,18.7961349487305,18.7961349487305,18.7961349487305,18.7961349487305,18.7961349487305,18.7961349487305,18.7961349487305,18.7961349487305,18.7961349487305,16.703483581543,16.703483581543,16.703483581543,16.703483581543,16.703483581543,16.703483581543,16.703483581543,16.703483581543,16.703483581543,15.7824630737305,15.7824630737305,15.7824630737305,15.7824630737305,15.7824630737305,15.7824630737305,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.4709625244141,19.1039276123047,19.1039276123047,19.1039276123047,19.1039276123047,19.1039276123047,19.1039276123047,19.1039276123047,19.1039276123047,19.1039276123047,19.1039276123047,19.1039276123047,17.7635879516602,17.7635879516602,17.7635879516602,17.7635879516602,17.7635879516602,17.7635879516602,17.7635879516602,17.7635879516602,17.7635879516602,17.7635879516602,17.7635879516602,11.9391098022461,11.9391098022461,11.9391098022461,11.9391098022461,11.9391098022461,11.9391098022461,11.9391098022461,11.9391098022461,11.9391098022461,11.9391098022461,11.9391098022461,11.9391098022461,11.9391098022461,11.9391098022461,15.4114379882812,15.4114379882812,15.4114379882812,15.4114379882812,15.4114379882812,15.4114379882812,15.4114379882812,15.4114379882812,15.4114379882812,15.4114379882812,15.4114379882812,15.4114379882812,15.4114379882812,15.4114379882812,18.8185424804688,18.8185424804688,18.8185424804688,18.8185424804688,18.8185424804688,18.8185424804688,18.8185424804688,12.7398910522461,12.7398910522461,12.7398910522461,12.7398910522461,12.7398910522461,12.7398910522461,16.4556121826172,16.4556121826172,16.4556121826172,16.4556121826172,16.4556121826172,16.4556121826172,16.4556121826172,16.4556121826172,11.7114715576172,11.7114715576172,11.7114715576172,11.7114715576172,11.7114715576172,11.7114715576172,11.7114715576172,11.7114715576172,11.7114715576172,11.7114715576172,11.7114715576172,11.7114715576172,15.5104827880859,15.5104827880859,15.5104827880859,15.5104827880859,15.5104827880859,12.3381576538086,12.3381576538086,12.3381576538086,12.3381576538086,12.3381576538086,12.3381576538086,12.3381576538086,12.3381576538086,12.3381576538086,12.3381576538086,12.3381576538086,16.0649642944336,16.0649642944336,16.0649642944336,16.0649642944336,16.0649642944336,16.0649642944336,16.0649642944336,16.0649642944336,16.0649642944336,16.0649642944336,16.0649642944336,19.4714584350586,19.4714584350586,19.4714584350586,19.4714584350586,19.4714584350586,19.4714584350586,19.4714584350586,19.4714584350586,19.4714584350586,19.4714584350586,19.4714584350586,14.9086151123047,14.9086151123047,18.6569595336914,18.6569595336914,18.6569595336914,18.6569595336914,18.6569595336914,15.4833145141602,15.4833145141602,15.4833145141602,15.4833145141602,15.4833145141602,15.4833145141602,15.4833145141602,15.4833145141602,15.4833145141602,15.4833145141602,15.4833145141602,15.4833145141602,15.4833145141602,15.4833145141602,19.4692306518555,19.4692306518555,19.4692306518555,19.4692306518555,19.4692306518555,19.4692306518555,19.4692306518555,13.6660308837891,13.6660308837891,13.6660308837891,13.6660308837891,13.6660308837891,13.6660308837891,13.6660308837891,11.6961364746094,11.6961364746094,11.6961364746094,11.6961364746094,11.6961364746094,11.6961364746094,11.6961364746094,11.6961364746094,15.4341735839844,15.4341735839844,15.4341735839844,15.4341735839844,15.4341735839844,15.4341735839844,15.4341735839844,15.4341735839844,13.3750381469727,13.3750381469727,13.3750381469727,13.3750381469727,13.3750381469727,13.3750381469727,13.3750381469727,13.3750381469727,17.2282104492188,17.2282104492188,17.2282104492188,17.2282104492188,17.2282104492188,17.2282104492188,11.8700790405273,11.8700790405273,11.8700790405273,11.8700790405273,11.8700790405273,11.8700790405273,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,19.2476043701172,15.9989013671875,15.9989013671875,15.9989013671875,15.9989013671875,15.9989013671875,15.9989013671875,15.9989013671875,15.9989013671875,15.9989013671875,19.4680328369141,19.4680328369141,19.4680328369141,19.4680328369141,19.4680328369141,19.4680328369141,19.4680328369141,19.4680328369141,19.4680328369141,19.4680328369141,19.4680328369141,19.4680328369141,19.4680328369141,15.3397369384766,15.3397369384766,15.3397369384766,15.3397369384766,15.3397369384766,15.3397369384766,15.3397369384766,19.0298614501953,19.0298614501953,19.0298614501953,19.0298614501953,19.0298614501953,19.0298614501953,19.0298614501953,19.0298614501953,14.5590896606445,14.5590896606445,14.5590896606445,14.5590896606445,14.5590896606445,14.5590896606445,14.5590896606445,14.5590896606445,14.5590896606445,13.5465393066406,13.5465393066406,13.5465393066406,13.5465393066406,13.5465393066406,13.5465393066406,13.5465393066406,13.5465393066406,13.5465393066406,13.5465393066406,13.5465393066406,13.5465393066406,13.5465393066406,13.5465393066406,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219,17.2841491699219],"meminc":[0,0,0,0,0,0,0,0,0,1.17354583740234,0,0,0,0,0,0,0,1.27454376220703,0,0,0,0,0,0,0,0,0,1.64861297607422,0,0,0,0,0,0,0,0,0,1.98233032226562,0,0,0,0,0,0,0,0,0,0,-7.41567230224609,0,0,0,0,0,0,0,0,0,1.82005310058594,0,0,0,0,0,0,1.43410491943359,0,0,0,0,0,0,0,1.42919921875,0,0,0,0,0,0,0,0,0,1.12300109863281,0,0,0,0,0,0,0,1.03441619873047,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.951820373535156,0,0,0,0,0,0,0,0,0,0,0,0,1.18308258056641,0,0,0,0,0,0,0,0,0,0,-8.43390655517578,0,0,0,0,0,0,0,0,1.68227386474609,0,0,0,0,0,0,2.13517761230469,0,0,0,0,0,0,0,0,0,0,2.82889556884766,0,0,0,0,0,0,0,1.92478942871094,0,0,0,0,0,0,0,0,-6.62148284912109,0,0,0,0,0,0,0,0,2.15776824951172,0,0,0,0,0,0,0,0,0,0,2.14991760253906,0,0,0,0,0,0,-5.39696502685547,0,0,0,0,0,0,0,2.34149932861328,0,0,0,0,0,0,0,1.81725311279297,0,0,0,0,0,0,1.85551452636719,0,0,0,-6.43133544921875,0,0,0,0,0,0,0,0,0,0,0,0,2.50709533691406,0,0,0,0,0,0,0,0,1.8245849609375,0,0,0,0,0,0,0,0,1.76297760009766,0,0,0,0,0,0,0,0,0,1.44222259521484,0,0,0,0,0,0,0,0,0,-6.62473297119141,0,0,0,0,0,0,0,0,1.86470794677734,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.57094573974609,0,0,0,0,0,0,0,0,0,0,0,1.77885437011719,0,0,0,0,0,0,0,0,-6.07588195800781,0,0,0,0,0,0,0,1.59579467773438,0,0,2.07740783691406,0,0,0,0,0,0,0,0,0,0,1.60359954833984,0,0,0,0,-5.8558349609375,0,0,0,0,0,0,0,0,0,0,0,1.96504974365234,0,0,0,0,0,0,0,0,0,0,2.11962127685547,0,0,0,0,0,0,0,0,1.64945983886719,0,0,0,0,0,0,0,0,0,0,0,0,0,-5.70692443847656,0,0,0,0,0,1.68328094482422,0,0,0,0,0,1.80442047119141,0,0,0,0,0,0,0,0,0,0,0,0,2.10818481445312,0,0,-5.53929901123047,0,0,0,0,0,0,0,1.84047698974609,0,0,0,0,0,0,0,0,0,0,0,1.98057556152344,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.14896392822266,0,0,0,0,0,0,-8.2432861328125,0,0,0,0,0,0,0,0,0,0,0,0,1.91358184814453,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.47679901123047,0,0,0,0,0,0,0,0,2.52535247802734,0,0,0,0,0,0,0,0,0,0,0,0,1.92716217041016,0,0,0,0,0,0,0,0,-7.27462005615234,0,0,0,0,0,0,0,0,0,0,2.15026092529297,0,0,0,0,0,0,0,0,0,2.16124725341797,0,0,0,0,0,0,2.156005859375,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-7.34072113037109,0,0,0,0,0,0,1.68449401855469,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.65737915039062,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.10631561279297,0,0,0,0,0,0,0,0,0,0,0,0,0,1.70125579833984,0,0,0,0,0,0,0,0,0,0,0,0,0,-6.59819793701172,0,0,0,0,0,0,0,0,0,2.82173919677734,0,0,0,0,0,0,0,0,2.14516448974609,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.43463897705078,0,0,0,0,0,0,0,0,0,0,-6.43843841552734,0,0,0,0,0,0,0,2.11697387695312,0,0,0,0,0,0,0,0,0,0,0,0,0,2.50383758544922,0,0,0,0,0,0,0,0,0,0,2.01494598388672,0,0,0,0,0,0,0,0,-6.01226043701172,0,0,0,0,0,3.19126892089844,0,0,0,0,0,0,0,0,0,0,0,0,2.15042877197266,0,0,0,0,0,0,0,0,0,0,-6.2098388671875,0,0,0,0,0,0,0,0,0,0,0,2.11067962646484,0,0,0,0,0,0,0,0,0,0,0,0,0,2.86438751220703,0,0,0,0,0,0,0,0,0,-5.33051300048828,0,0,0,0,0,0,0,0,2.48665618896484,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.61720275878906,0,0,-5.14015960693359,0,0,0,0,2.77576446533203,0,0,0,0,0,0,0,0,2.55683135986328,0,0,0,0,0,0,0,-5.02825164794922,0,0,0,0,0,0,0,0,0,2.54100799560547,0,0,0,0,0,0,0,0,0,0,2.14649200439453,0,0,2.2744140625,0,0,-4.87509155273438,0,0,0,0,0,0,2.85068511962891,0,0,0,0,-1.97376251220703,0,0,0,0,0,0,0,0,-0.309127807617188,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.27804565429688,0,0,0,0,0,0,0,-4.03427124023438,0,0,0,0,0,0,0,0,0,0,2.69145965576172,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.65020751953125,0,0,0,0,0,0,0,0,0,0,-4.45724487304688,0,0,0,0,0,0,0,0,2.04615020751953,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.69807434082031,0,0,0,0,0,0,0,-4.39479827880859,0,0,0,0,0,0,0,0,3.16695404052734,0,0,0,0,0,0,0,0,0,0,0,0,0,-4.05841827392578,0,0,0,0,0,0,0,0,0,0,0,2.52590179443359,0,0,0,0,0,0,0,0,2.13644409179688,0,0,0,0,0,0,0,-3.65775299072266,0,0,0,0,0,0,2.69062042236328,0,0,0,0,0,0,0,0,0,0,0,2.03018188476562,0,0,0,0,-3.28574371337891,0,0,0,0,0,2.27079010009766,0,0,0,0,0,0,-2.59890747070312,0,0,0,0,0,0,0,0,2.35649108886719,0,0,0,0,0,0,0,0,0,0,-2.90280914306641,0,0,0,0,0,0,0,0,2.33798217773438,0,0,0,0,0,0,0,0,0,0,0,1.8243408203125,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-2.88902282714844,0,0,0,0,0,0,2.86168670654297,0,0,0,0,0,0.0243377685546875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-9.33606719970703,0,0,0,0,0,0,0,0,0,0,0,0,1.08369445800781,0,0,0,0,0,0,2.4739990234375,0,0,0,0,0,0,0,0,0,0,3.25997924804688,0,0,0,0,0,0,0,0,0,0,0,0,2.52140808105469,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-7.43709564208984,0,0,0,0,0,0,0,0,0,0,3.41524505615234,0,0,0,0,0,0,0,0,0,0,0,2.47906494140625,0,0,0,0,0,0,0,-7.357421875,0,0,0,0,2.36133575439453,0,0,0,0,0,0,0,0,0,0,0,2.38138580322266,0,0,0,0,0,0,0,2.423583984375,0,0,0,0,0,0,0,-6.88071441650391,0,0,0,0,0,0,2.63310241699219,0,0,0,0,0,4.23141479492188,0,0,0,0,-6.50827026367188,0,0,0,0,0,0,0,2.25335693359375,0,0,0,0,0,0,0,2.14691162109375,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.15808868408203,0,0,0,0,0,0,0,-5.42093658447266,0,0,0,0,0,0,0,0,0,0,0,0,0,0.8914794921875,0,0,0,0,0,0,3.49631500244141,0,0,0,0,0,0,0,2.47787475585938,0,0,0,0,0,0,0,0,0,0,0,0,-4.62409210205078,0,0,0,0,0,3.21881866455078,1.65822601318359,0,0,0,0,0,0,0,0,0,-4.85926818847656,0,0,0,0,0,0,0,2.43460845947266,0,0,0,0,0,0,0,0,2.42649841308594,0,0,0,0,0,0,0,0,0,-4.64400482177734,0,0,0,0,0,0,0,1.98670196533203,0,0,0,0,0,0,2.55931854248047,-4.88260650634766,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.12206268310547,0,0,0,0,0,0,0,2.48069000244141,0,0,0,0,0,0,0,0,0,0,-3.26308441162109,0,0,0,0,0,0,0,0,0,0,0,0,2.54277801513672,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-4.13169097900391,0,0,0,0,0,0,2.78462982177734,0,0,0,0,0,0,0,0,2.44467163085938,0,0,0,0,0,0,0,0,-4.02075958251953,2.91879272460938,0,0,0,0,0,0,-4.28694152832031,0,0,0,0,3.40946197509766,0,0,0,0,0,0,0,0,0,0,0,-3.19209289550781,0,0,0,0,0,0,0,0,0,2.55137634277344,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.61849212646484,0,0,0,0,0,0,-2.85196685791016,0,0,0,0,0,0,0,0,2.83404541015625,0,0,0,0,0,0,0,0,0,0,0,0,0,-3.14127349853516,0,0,0,0,0,0,0,0,0,0,0,2.1168212890625,0,0,0,0,0,0,0,0,-2.92160797119141,0,0,0,0,0,0,0,2.33515167236328,0,0,0,0,0,0,0,0,0,0,0,0,-0.990188598632812,0,0,0,0,0,0,0,0,1.00137329101562,0,0,0,0,0,0,-1.86666870117188,0,0,0,0,0,0,0,0,0,0,2.53729248046875,0,0,0,0,0,0,0,0,0,0,-2.74958038330078,0,0,0,0,0,2.47206878662109,0,0,0,0,0,0,0,0,0,0,0,0,0,-7.172607421875,0,0,0,0,0,0,0,0,0,2.48506164550781,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.50984191894531,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.40408325195312,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-6.62776184082031,0,0,0,0,0,0,2.52969360351562,0,0,0,0,0,0,0,0,0,0,2.4586181640625,0,0,0,0,0,0,0,0,-6.76027679443359,0,0,0,0,0,0,0,0,1.82107543945312,0,0,0,0,0,0,1.69424438476562,0,0,0,0,0,0,0,0,1.68486022949219,0,0,0,0,0,0,0,0,0,0,0,0,0,2.64676666259766,0,0,0,0,0,-6.35446929931641,0,0,0,0,0,1.73574829101562,0,0,0,0,1.51023101806641,0,0,0,0,0,0,0,0,0,0,0,1.64910125732422,0,0,0,0,0,0,0,0,1.76704406738281,0,0,0,0,0,0,0,0,0,-6.79996490478516,0,0,0,0,0,0,0,0,0,0,1.51687622070312,0,0,0,0,0,0,0,1.25617980957031,0,0,0,0,0,0,0,0,0,0,1.27687072753906,0,0,0,0,0,0,0,0,2.45513916015625,0,0,0,0,0,0,-5.87712860107422,0,0,0,0,0,0,0,2.64646911621094,0,0,0,0,0,0,0,0,0,0,0,3.30880737304688,0,0,0,-5.54948425292969,0,0,0,0,0,0,0,0,0,0,0,0,0,2.51995086669922,0,0,0,0,0,0,0,0,0,0,1.70082092285156,0,0,0,0,0,0,0,0,0,0,0,0,0,1.68625640869141,0,0,0,0,0,0,0,-6.19802093505859,0,0,0,0,0,0,0,0,0,0,0,0,2.19052124023438,0,0,0,0,0,0,0,0,2.55365753173828,0,0,0,0,0,0,0,0,1.54910278320312,0,0,0,0,0,0,0,0,0,0,0,0,-5.6971435546875,0,0,0,0,0,0,0,3.68025970458984,0,0,0,0,0,0,0,0,0,0,0,-4.35994720458984,0,0,0,0,0,0,0,0,2.03997802734375,0,0,0,0,0,0,0,0,1.95610046386719,0,0,0,0,0,0,0,0,0,0,0,0,0,2.38779449462891,0,0,0,0,-3.867431640625,0,0,0,0,0,0,0,0,0,0,2.15925598144531,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.73117065429688,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.530715942382812,0,0,0,0,0,0,2.53371429443359,0,0,0,0,0,0,0,0,0,0,0,-3.82413482666016,0,0,0,0,0,0,0,0,2.27391052246094,0,0,0,0,0,0,0,0,0,0,0,0,0,2.541259765625,0,0,0,0,0,0,0,-4.30838775634766,0,0,0,0,0,0,0,0,0,0,0,2.22294616699219,0,0,0,0,0,0,0,2.53363037109375,0,0,0,0,0,0,0,-2.76473999023438,0,0,0,0,0,0,0,0,2.76384735107422,0,0,0,0,0,0,0,0,-3.04270172119141,0,0,0,0,0,0,0,2.84996795654297,0,0,0,0,0,0,0,0,0,0,0,-2.34958648681641,0,0,0,2.54064178466797,0,0,0,0,0,0,0,0,0,0,-2.88826751708984,0,0,0,0,0,2.31109619140625,0,0,0,0,0,0,0,0,-1.93422698974609,0,0,0,0,0,0,0,0,0,0,0,0,0,2.32418823242188,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-2.54837799072266,0,0,0,0,0,0,0,0,0,0,0,0,0,2.73673248291016,0,0,0,0,0,0,-1.96710205078125,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-2.03513336181641,0,0,0,0,0,0,0,0,-2.63238525390625,0,0,0,0,0,0,0,0,0,0,0,0,0,1.66083526611328,0,0,0,0,0,2.30453491210938,0,0,0,0,2.53440856933594,0,0,0,0,0,0,0,0,0,0,-7.60157012939453,0,0,0,0,0,0,0,0,0,0,2.29165649414062,0,0,0,0,0,0,0,0,0,0,0,0,0,1.43447113037109,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.04319000244141,0,0,0,0,0,0,1.49750518798828,0,0,0,0,0,0,0,-4.40962982177734,0,0,0,0,0,0,0,0,0,-1.2664794921875,0,0,0,0,0,0,0,0,0,0,0,1.69074249267578,0,0,0,0,0,0,0,0,0,0,1.19917297363281,0,0,0,0,0,0,0,0,0,0,0,1.76859283447266,0,0,0,0,0,0,0,0,2.458984375,0,0,0,0,0,0,-7.99752044677734,0,0,0,0,0,0,0,1.25144958496094,0,0,0,0,0,1.54637908935547,0,0,0,0,0,0,0,1.83529663085938,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.10379791259766,0,0,0,0,0,0,0,0,0,0,0,0,-7.06910705566406,0,0,0,0,0,0,2.01435852050781,0,0,0,0,0,0,0,0,0,0,0,0,2.12666320800781,0,0,0,0,0,2.52305603027344,0,0,0,0,0,0,0,0,0,0,1.69521331787109,0,0,0,0,0,0,0,0,0,0,0,0,-6.32167053222656,0,0,0,0,0,0,2.31198120117188,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.03292846679688,0,0,0,0,0,0,-6.42620849609375,0,0,0,0,0,0,0,0,0,0,0,2.22034454345703,0,0,0,0,0,0,0,0,0,0,0,2.1136474609375,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.04002380371094,0,0,-6.43601989746094,0,0,0,0,0,0,0,0,0,2.02020263671875,0,0,0,0,0,0,0,0,0,3.36640930175781,0,0,0,0,0,0,0,0.936813354492188,0,0,0,0,0,0,0,-5.45359039306641,0,0,0,0,0,0,0,0,0,0,0,2.14765167236328,0,0,0,1.35702514648438,0,0,0,0,0,0,0,0,1.18223571777344,0,0,0,0,0,0,0,0,1.75940704345703,0,0,0,0,0,0,0,0,0,-6.43107604980469,0,0,0,0,0,0,0.525886535644531,0,0,0,0,0,0,0,0,1.24468994140625,0,0,0,0,0,0,0,0,0,1.6221923828125,0,0,0,0,0,0,0,0,0,1.68808746337891,0,0,0,0,0,0,1.49713134765625,0,0,0,0,0,0,0,0,-6.26557159423828,0,0,0,0,0,0,0,0,0,0,0,1.47385406494141,0,0,0,0,0,0,0,0,0,1.49221801757812,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1.86072540283203,0,0,0,0,0,0,0,0.789718627929688,0,0,0,0,0,0,0.650665283203125,0,0,0,0,0,0,0,0,0,-5.57120513916016,0,0,0,0,0,0,0,0,0,2.0361328125,0,0,0,0,0,0,0,0,0,2.22328948974609,0,0,0,0,1.31075286865234,0,0,0,0,0,0,0,0,-5.21820831298828,0,0,0,0,0,0,0,0,2.48692321777344,0,0,0,0,0,0,0,0.971206665039062,0,0,0,0,0,0,1.0562744140625,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.704277038574219,0,0,0,0,-5.19020843505859,0,0,0,0,0,0,0,0,0,0,0,0,0,1.42045593261719,0,0,0,0,0,0,0,0,0,0,1.77166748046875,0,0,0,0,0,0,0,1.97262573242188,0,0,0,0,0,0,0,0,0,0,0,-4.41850280761719,0,0,0,0,0,0,0,2.92507934570312,0,0,0,0,0,0,0,-3.92775726318359,0,0,0,0,0,0,0,0,0,0,0,0,2.42533111572266,0,0,0,0,0,0,0,0,0,0,1.95822906494141,0,0,0,0,0,0,0,0,0,0,0,0,-3.54557037353516,0,0,0,0,0,0,0,3.18311309814453,0,0,0,0,0,0,0,0,0,0,0,-2.99193572998047,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.09474945068359,0,0,0,0,0,0,1.12664794921875,0,0,0,0,0,0,0,0,0,0,0,0,1.18923950195312,0,0,0,0,0,0,0,0,0,0,0,-3.84719085693359,0,0,0,0,0,0,0,0,0,0,0,0,1.64611053466797,0,0,0,0,0,0,0,0,1.93801116943359,0,0,0,0,0,0,0,0,0,0,-3.56935119628906,0,0,0,0,0,0,0,2.16844177246094,0,0,0,0,0,-2.06831359863281,0,0,0,0,0,0,0,0,0,0,1.63895416259766,0,0,0,0,0,0,0,1.51415252685547,0,0,0,0,0,-1.86605834960938,0,0,0,0,0,0,0,1.94573974609375,0,0,0,0,0,0,0,0,0,0,0,-6.86952209472656,0,0,0,0,0,0,0,0,0,0,2.55858612060547,0,0,0,0,0,0,2.31272125244141,0,0,0,0,0,0,0,0,0,0,0,2.05854797363281,0,0,0,0,0,0,0,0,-7.53017425537109,0,0,0,0,0,4.86421966552734,0,0,0,0,0,0,0,0,0,0,0,1.84415435791016,0,0,0,0,0,0,0,0,0,0,0,-6.64199066162109,0,0,0,0,0,0,0,0,0,0,0,0,2.31465148925781,0,0,0,0,0,0,0,2.18639373779297,0,0,0,0,0,0,0,0,0,2.33797454833984,0,0,0,0,0,0,0,0,0,-5.56694030761719,0,0,0,0,0,0,0,0,0,0,2.56292724609375,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.45940399169922,0,0,0,0,0,-5.49346923828125,0,0,0,0,0,0,0,2.67821502685547,0,0,0,0,0,0,0,0,0,0,2.46942138671875,0,0,0,0,0,0,0,0,0,-5.20310974121094,0,0,0,0,0,0,0,0,0,0,2.32888793945312,0,0,0,0,0,0,0,0,0,0,0,0,0,2.72571563720703,0,0,0,0,0,0,0,-4.77758026123047,0,0,0,0,0,0,0,0,2.57582092285156,0,0,0,0,0,0,0,0,3.28659057617188,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-4.67489624023438,0,0,0,0,0,2.97718048095703,0,0,0,0,0,0,1.74504852294922,0,0,0,0,0,0,0,0,-4.67144012451172,0,0,0,0,0,2.92180633544922,0,0,0,0,0,0,0,0,0,0,0,0,0,-3.2420654296875,0,0,0,2.86470031738281,0,0,0,0,0,0,0,0,0,0,0,2.91944122314453,0,0,0,0,0,0,0,0,-4.58133697509766,0,0,0,0,0,0,0,0,0,0,4.25287628173828,0,0,0,0,0,0,0,0,0,-3.58399200439453,0,0,0,0,0,0,0,0,4.09281158447266,0,0,0,0,0,0,0,0,0,0,0,0,0,-2.80806732177734,0,0,0,0,0,0,0,0,-2.13452911376953,0,0,0,0,0,0,0,0,0,0,3.40540313720703,0,0,0,0,0,0,0,0,0,0,0,-2.67807769775391,0,0,0,0,0,0,0,0,0,0,0,0,0,2.916015625,0,0,0,0,0,0,0,-1.79936981201172,0,0,0,0,0,0,0,0,2.15380096435547,0,0,0,0,0,-2.11714172363281,0,0,0,0,0,0,0,0,0,-0.309654235839844,0,0,0,0,0,0,0,0,0,0,2.90947723388672,0,0,0,0,0,0,0,0,0,0,0,0,-2.38182830810547,0,2.84412384033203,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-9.30596923828125,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.81747436523438,0,0,0,0,0,0,0,0,3.89702606201172,0,0,0,0,0,2.59630584716797,0,0,0,0,0,0,0,0,-5.81690216064453,0,0,0,0,0,0,0,0,0,3.02993011474609,0,0,0,0,0,0,0,0,0,0.207595825195312,0,0,0,0,0,0,0,0,-3.22769927978516,0,0,0,0,0,0,0,0,2.91594696044922,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.55918121337891,0,0,0,0,0,0,-4.60066223144531,0,0,0,0,0,0,0,0,0,0,0,0,0,2.32355499267578,0,0,0,0,0,0,0,0,0,0,0,0,2.60944366455078,0,0,0,0,0,0,0,0,-3.59584808349609,0,0,0,0,0,0,0,0,0,0,0,0,-5.06979370117188,0,0,0,0,0,0,0,0,0,2.97322082519531,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.17101287841797,0,0,0,0,0,2.51992797851562,0,0,0,0,-2.90502166748047,0,0,0,2.90554046630859,0,0,0,0,-5.81516265869141,0,0,0,0,0,0,0,0,0,0,2.96351623535156,0,0,0,0,0,0,0,2.81509399414062,0,0,-4.46192169189453,0,0,0,0,0,2.80085754394531,0,0,0,0,0,0,0,0,0,0,0,-5.07874298095703,0,0,2.82247924804688,0,0,0,0,0,0,0,0,0,0,3.10440826416016,0,0,0,0,0,0,0,0,0,0,0,-6.35881042480469,0,0,0,0,0,0,0,2.91789245605469,0,0,0,0,0,0,2.91895294189453,0,0,0,0,0,0,-5.66751861572266,0,0,0,0,0,0,0,0,0,0,2.80435943603516,0,0,0,0,0,3.25823974609375,0,0,0,0,0,0,0,0,0,0,0,0,-5.47522735595703,0,0,0,0,0,0,0,0,0,0,3.23349761962891,0,0,0,0,0,0,0,0,0,3.21347045898438,0,0,0,0,0,0,0,0,-4.37152862548828,0,0,0,0,0,0,0,3.39311218261719,0,0,0,0,0,0,0,0,0,0,-4.14987945556641,0,0,0,0,0,0,0,2.29242706298828,0,0,0,0,0,0,0,0,2.84027862548828,0,-3.80374908447266,0,0,0,0,0,0,0,3.00286865234375,0,0,0,0,0,-4.27594757080078,0,0,0,0,0,3.08428192138672,0,0,0,0,0,0,0,0,-1.5966796875,0,0,0,0,0,3.2860107421875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.818206787109375,0,0,0,0,0,0,0,0,0,0,0,0,-3.11732482910156,0,0,0,0,0,0,0,0,2.36811065673828,0,0,0,0,0,0,0,0,0,0,0,-3.11506652832031,0,0,0,0,0,0,4.55194091796875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-2.67481994628906,0,0,0,0,0,0,0,3.10990905761719,0,0,0,0,0,0,0,0,0,-1.09436798095703,0,0,0,0,0,0,-1.22941589355469,0,0,0,0,0,0,0,2.28859710693359,0,0,0,0,0,0,0,0,0,-1.48416137695312,0,0,0,0,0,0,0,0,0,-4.9759521484375,0,0,0,0,0,0,0,0,3.18666839599609,0,0,0,0,0,0,0,0,0,0,0,3.07171630859375,0,0,0,0,0,0,0,-6.44422912597656,0,0,0,0,0,0,0,0,0,2.92918395996094,0,0,0,0,0,0,0,3.75179290771484,0,0,0,0,0,0,0,0,0,0,-6.05942535400391,0,0,0,0,0,0,0,0,0,0,3.60099029541016,0,0,0,0,0,0,0,0,0,0,0,2.45326995849609,0,0,0,0,0,0,0,0,0,0,0,0,0,-4.25990295410156,0,0,0,0,0,0,0,0,0,0,0,0,-3.25704193115234,0,0,0,0,0,0,0,0,0,0,3.04161071777344,0,0,0,0,0,0,0,3.15760040283203,0,0,0,0,0,0,0,0,0,0,0,0,-2.29482269287109,0,0,0,0,0,0,0,3.32167816162109,0,0,0,0,0,0,0,0,0,0,0,-4.72933197021484,0,0,0,0,0,0,0,0,0,0,3.32815551757812,0,0,0,0,0,0,-4.24225616455078,0,0,0,0,0,0,0,0,5.10257720947266,0,0,0,0,0,0,0,0,0,-3.33750152587891,0,0,0,0,0,0,0,0,0,0,-0.0682144165039062,0,0,0,0,0,0,0,0,0,0,0,0,-1.15850067138672,0,0,0,0,0,0,0,3.44252014160156,0,0,0,0,0,0,0,-2.06210327148438,0,0,0,0,0,0,0,0,0.495849609375,0,0,0,0,0,0,0,0,3.31005096435547,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-2.03522491455078,0,0,0,0,0,0,0,0,0,0,0,0.57073974609375,0,0,0,0,0,0,-1.70690155029297,0,0,0,0,0,0,0,0,0,0,0,0,0,3.38465881347656,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.16298675537109,0,0,0,0,0,0,0,0,0,0,0,0,0,-7.18478393554688,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.92266845703125,0,0,0,0,0,0,0,0,0,0,3.30752563476562,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-6.22594451904297,0,0,0,0,0,0,2.59504699707031,0,0,0,0,0,0,0,5.74781036376953,0,0,0,0,-5.49657440185547,0,0,0,0,0,0,0,0,0,0,0,3.30896759033203,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-6.065185546875,0,0,0,0,0,0,0,0,0,0,0,3.30043029785156,0,0,0,0,0,0,0,0,0,0,3.31192016601562,0,0,0,0,-5.64284515380859,0,0,0,0,0,0,3.30660247802734,0,0,0,0,0,0,0,0,-4.69578552246094,0,0,0,0,0,0,0,4.55229949951172,0,0,0,0,0,0,0,3.08404541015625,0,0,0,0,0,0,0,-6.39830017089844,0,0,0,0,0,0,0,0,3.31608581542969,0,0,0,0,0,0,0,0,0,0,-3.46123504638672,0,0,0,0,0,0,0,0,2.98226928710938,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.32235717773438,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-5.54171752929688,0,0,0,0,0,0,0,0,0,0,0,3.67037200927734,0,0,0,0,0,0,3.14859771728516,0,0,0,0,0,0,0,0,-5.44123077392578,0,0,0,0,0,0,0,0,4.54607391357422,0,0,0,0,0,0,0,0,-2.48607635498047,0,0,0,0,0,-1.89039611816406,0,0,0,0,0,0,0,0,0,0,0,2.84135437011719,0,0,0,0,0,0,0,0,-3.44905853271484,0,0,0,0,0,0,0,0,0,2.48391723632812,0,0,0,0,0,0,0,3.03789520263672,0,0,0,0,0,0,0,0,0,0,0,0,0,-3.40045928955078,0,0,0,0,0,0,0,0,0,0,0,3.09217834472656,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.77028656005859,0,0,0,0,0,0,2.43561553955078,0,0,0,0,-3.67059326171875,0,0,0,0,0,0,0,2.61978149414062,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-3.44808197021484,0,0,0,0,0,0,0,0,3.10221099853516,0,0,0,0,0,0,0,-0.47576904296875,0,0,0,0,0,0,0,0,-2.7520751953125,0,0,0,0,0,2.91127777099609,0,0,0,0,0,0,0,0,0,0,-2.40730285644531,0,0,0,0,2.71134185791016,0,0,0,0,0,0,0,0,-2.28213500976562,0,0,0,0,0,0,0,0,0,0,0,0.439926147460938,0,0,0,0,0,0,0,0,0,0,0,3.2513427734375,0,0,0,0,0,0,0,0,-2.40633392333984,0,0,0,0,0,0,0,0,2.40623474121094,0,0,0,0,-6.90785217285156,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.30753326416016,0,0,0,0,0,0,0,0,0,0,0,0,3.59915161132812,0,0,0,0,0,0,-4.57918548583984,0,0,0,0,0,0,0,0,0,0,0,3.30458831787109,0,0,0,0,0,0,0,0,-6.24327087402344,0,0,0,0,0,0,0,0,0,0,0,0,3.32118225097656,0,0,0,0,0,0,2.53013610839844,0,0,0,0,0,0,0,0,0,0,0,0,0,-4.50617218017578,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5.00185394287109,0,0,0,0,0,0,0,1.17033386230469,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-9.26849365234375,0,0,0,0,0,0,0,0,0,0,0,0,0,3.05014801025391,0,0,0,0,0,0,0,0,0,0,0,4.40455627441406,0,0,0,0,-6.62959289550781,0,0,0,0,0,0,0,0,0,0,0,0,3.26444244384766,0,0,0,0,0,0,0,0,0,0,3.21305847167969,0,0,0,0,0,0,0,0,0,-2.62873077392578,0,0,0,0,0,0,0,0,0,0,0,0,0,4.59654235839844,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-5.41073608398438,0,0,0,0,0,4.0196533203125,0,0,0,0,0,0,0,0,-5.75498199462891,0,0,0,0,3.30766296386719,0,0,0,0,0,0,0,0,0,-2.09788513183594,0,0,0,0,0,0,3.44647979736328,0,0,0,0,0,0,0,-4.75193023681641,0,0,0,0,0,0,0,0,3.32676696777344,0,0,0,0,0,0,0,3.35145568847656,0,0,0,0,0,0,0,-4.61793518066406,0,0,0,0,0,0,0,0,0,3.3095703125,0,0,-1.94027709960938,0,0,0,0,0,0,0,0,0,0,0,-0.517967224121094,0,0,0,0,0,0,0,0,0,3.315185546875,0,0,0,0,0,0,-2.85153961181641,0,0,0,0,0,0,0,0,3.86382293701172,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.0008544921875,0,0,0,0,0,0,0,0,0,0,0,-2.72084045410156,0,0,0,0,0,0,0,2.72161102294922,0,0,0,0,0,0,0,0,-2.33052062988281,0,0,0,0,0,0,0,0,-2.47351837158203,0,0,0,0,0,3.40281677246094,0,0,0,0,0,0,0,-0.124221801757812,0,0,0,0,0,0,-2.00979614257812,0,0,0,0,0,0,0,0,0,0,0,3.52871704101562,0,0,0,0,0,0,0,0,-3.6610107421875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.44113159179688,0,0,0,0,0,0,0,0.226554870605469,0,0,0,0,0,0,0,0,-4.67514038085938,0,0,0,0,0,0,0,0,0,0,0,3.44427490234375,0,0,0,0,0,0,0,0,-6.25251007080078,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.38935089111328,0,0,0,0,0,0,0,0,0,0,3.41061401367188,0,0,0,0,0,0,0,-6.43630981445312,0,0,0,0,0,0,0,0,0,0,0,3.46871948242188,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.65216827392578,0,0,0,0,0,0,0,0,0,-4.22669219970703,0,0,0,0,0,0,3.30912017822266,0,0,0,0,0,0,-6.04557037353516,0,0,0,0,0,0,3.49588775634766,0,0,0,0,0,0,0,0,0,0,-3.23039245605469,0,0,0,0,0,0,4.78936004638672,0,0,0,0,0,0,0,0,0,0,-4.73408508300781,0,0,0,0,0,3.75552368164062,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2.87496185302734,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.38994598388672,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-4.3414306640625,0,0,0,0,0,0,0,0,0,0,3.68804931640625,0,0,0,0,0,0,0,-4.10059356689453,0,0,0,0,0,0,0,3.79062652587891,0,0,0,0,0,0,0,0,0,0,-0.709693908691406,0,0,0,0,0,0,0,3.07373809814453,0,-1.53123474121094,0,0,0,0,0,0,0,0,0,-2.35356903076172,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.68987274169922,0,0,0,0,0,0,0,-3.322265625,0,0,0,0,0,0,0,0,0,0,0,0.658256530761719,0,0,0,0,0,0,0,2.85220336914062,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.88420867919922,0,0,0,0,0,0,0,0,0,0,1.46710968017578,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1.82174682617188,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.16033935546875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-6.35037994384766,0,0,0,0,0,0,3.68363952636719,0,0,0,0,0,3.68869781494141,0,0,0,0,0,0,-6.18251800537109,0,0,0,0,0,0,0,0,0,0,3.92172241210938,0,0,0,0,0,3.64006042480469,0,0,0,0,0,0,0,0,0,-4.93892669677734,0,0,0,0,0,0,0,0,0,3.68855285644531,0,0,0,0,0,0,-5.83551025390625,0,0,0,0,0,0,0,0,0,0,3.68456268310547,0,0,0,0,0,0,0,0,0,0,0,0,-3.06413269042969,0,0,0,0,0,0,0,0,0,0,0,0,4.922607421875,0,0,0,0,0,0,0,0,0,0,-6.30086517333984,0,0,0,0,0,0,0,0,0,0,6.97835540771484,0,0,0,0,0,0,0,0,-5.91278076171875,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5.24507904052734,0,0,0,0,0,0,0,-3.51473236083984,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5.05120849609375,0,0,0,0,0,0,0,0,-3.28926086425781,0,0,0,0,0,0,0,0,0,0,0,0,0,3.28884887695312,0,0,0,0,0,0,0,0,0,0,0,0,-5.06881713867188,0,0,0,0,0,0,-1.36069488525391,0,3.31167602539062,0,0,0,0,0,0,0,3.11991882324219,0,0,0,0,-4.07383728027344,0,0,0,0,0,0,0,0,0,-1.87245941162109,0,0,0,0,0,0,0,0,0,4.77645874023438,0,0,0,0,0,0,0,0,0,0,0,-4.29116058349609,0,0,0,0,0,0,0,0,0,0,0,0,0,5.46163177490234,0,0,0,0,0,0,0,0,-1.65924835205078,0,0,0,0,0,0,0,0,0,0,0,0,-2.91172027587891,0,0,0,0,0,0,0,4.5635986328125,0,0,0,0,0,0,0,0,0,0,0,0,-2.61910247802734,0,0,0,0,0,0,-2.18709564208984,0,0,0,0,0,3.02642822265625,0,0,0,0,0,0,0,0,0,0,0,0,0,-2.08006286621094,0,0,0,0,0,0,0,0,0,0,0,-0.522262573242188,0,0,0,0,0,2.92460632324219,0,0,0,0,0,0,0,0,0,0,-1.50222015380859,0,0,0,0,0,0,2.48499298095703,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-7.85772705078125,0,0,0,0,0,0,0,0,3.68753814697266,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.32370758056641,0,0,0,0,0,0,0,0,-5.95455932617188,0,0,0,0,0,0,0,0,0,0,3.44631958007812,0,0,0,0,0,0,0,0,0,0,0,0,0,3.12120056152344,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-5.41310882568359,0,0,0,0,0,0,0,0,0,0,6.12102508544922,0,0,0,0,0,0,0,0,0,0,0,0,-4.75347137451172,0,0,0,0,0,0,0,0,0,0,0,0,0,3.60346221923828,0,0,0,0,0,0,0,0,0,0,0,-5.20246887207031,0,0,0,0,0,0,0,0,0,0,0,0,0,3.59233856201172,0,2.75691223144531,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-9.10037231445312,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0.645423889160156,0,0,0,0,0,0,0,0,0,3.23061370849609,0,0,0,0,0,0,0,0,0,0,0,5.23478698730469,0,0,0,0,0,0,0,0,-2.21131896972656,0,0,0,0,0,0,0,-5.67107391357422,0,0,0,0,0,0,0,0,0,0,3.78130340576172,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.86204528808594,0,0,0,0,0,0,0,0,0,0,0,-5.38368225097656,0,0,0,0,0,0,0,3.91561889648438,0,0,0,0,0,0,0,-5.190673828125,0,0,0,0,0,5.43950653076172,0,0,0,0,0,0,0,0,0,0,0,0,0,-2.65995025634766,0,0,0,0,0,0,0,0,0,0,0,0,0,3.86258697509766,0,0,0,0,0,0,0,0,0,0,0,0,0,-4.022705078125,0,0,0,0,0,0,0,4.19048309326172,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-3.91974639892578,0,0,0,0,0,0,0,0,0,0,0,-1.14405822753906,0,0,0,0,0,0,0,0,4.70716094970703,0,0,0,0,0,0,0,0,0,0,0,0.433944702148438,0,0,0,0,0,0,0,0,0,0,0,0,0,-3.00981140136719,0,0,0,0,0,0,0,1.02251434326172,0,0,0,0,0,0,0,0,0,0,-2.32003021240234,0,0,0,0,0,3.64302062988281,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-2.0926513671875,0,0,0,0,0,0,0,0,-0.9210205078125,0,0,0,0,0,3.68849945068359,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-0.367034912109375,0,0,0,0,0,0,0,0,0,0,-1.34033966064453,0,0,0,0,0,0,0,0,0,0,-5.82447814941406,0,0,0,0,0,0,0,0,0,0,0,0,0,3.47232818603516,0,0,0,0,0,0,0,0,0,0,0,0,0,3.4071044921875,0,0,0,0,0,0,-6.07865142822266,0,0,0,0,0,3.71572113037109,0,0,0,0,0,0,0,-4.744140625,0,0,0,0,0,0,0,0,0,0,0,3.79901123046875,0,0,0,0,-3.17232513427734,0,0,0,0,0,0,0,0,0,0,3.726806640625,0,0,0,0,0,0,0,0,0,0,3.406494140625,0,0,0,0,0,0,0,0,0,0,-4.56284332275391,0,3.74834442138672,0,0,0,0,-3.17364501953125,0,0,0,0,0,0,0,0,0,0,0,0,0,3.98591613769531,0,0,0,0,0,0,-5.80319976806641,0,0,0,0,0,0,-1.96989440917969,0,0,0,0,0,0,0,3.738037109375,0,0,0,0,0,0,0,-2.05913543701172,0,0,0,0,0,0,0,3.85317230224609,0,0,0,0,0,-5.35813140869141,0,0,0,0,0,7.37752532958984,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-3.24870300292969,0,0,0,0,0,0,0,0,3.46913146972656,0,0,0,0,0,0,0,0,0,0,0,0,-4.1282958984375,0,0,0,0,0,0,3.69012451171875,0,0,0,0,0,0,0,-4.47077178955078,0,0,0,0,0,0,0,0,-1.01255035400391,0,0,0,0,0,0,0,0,0,0,0,0,0,3.73760986328125,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],"filename":[null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>","<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>","<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>","<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>"]},"interval":10,"files":[{"filename":"<expr>","content":"library(profvis)\n\nprofvis({\n  cv_error <- vector(\"numeric\", 5)\n  terms <- 1:5\n  \n  for (i in terms) {\n    glm_fit <- glm(mpg ~ poly(horsepower, i), data = Auto)\n    cv_error[[i]] <- cv.glm(Auto, glm_fit)$delta[[1]]\n  }\n})","normpath":"<expr>"}],"prof_output":"/var/folders/vw/l7k7vwhn3qqd990ww0dd101c0000gn/T//RtmpMC8zYb/file7ef873ac5893.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="fold-cv" class="section level4">
<h4>10-fold CV</h4>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(profvis)

<span class="kw">profvis</span>({
  cv_error_fold10 &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="st">&quot;numeric&quot;</span>, <span class="dv">5</span>)
  terms &lt;-<span class="st"> </span><span class="dv">1</span>:<span class="dv">5</span>
  
  for (i in terms) {
    glm_fit &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg ~<span class="st"> </span><span class="kw">poly</span>(horsepower, i), <span class="dt">data =</span> Auto)
    cv_error_fold10[[i]] &lt;-<span class="st"> </span><span class="kw">cv.glm</span>(Auto, glm_fit, <span class="dt">K =</span> <span class="dv">10</span>)$delta[[<span class="dv">1</span>]]
  }
})</code></pre></div>
<div id="htmlwidget-cfa19873ee74caef45a0" style="width:100%;height:600px;" class="profvis html-widget"></div>
<script type="application/json" data-for="htmlwidget-cfa19873ee74caef45a0">{"x":{"message":{"prof":{"time":[1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,10,10,10,10,10,10,10,10,10,11,11,11,11,11,11,11,11,12,13,13,13,13,13,13,13,13,13,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,15,15,15,15,15,15,15,15],"depth":[9,8,7,6,5,4,3,2,1,6,5,4,3,2,1,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,9,8,7,6,5,4,3,2,1,16,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,12,11,10,9,8,7,6,5,4,3,2,1,11,10,9,8,7,6,5,4,3,2,1,5,4,3,2,1,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1,1,9,8,7,6,5,4,3,2,1,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,8,7,6,5,4,3,2,1],"label":["sys.function","formals","match.arg","predict.lm","predict.glm","predict","mean","cost","cv.glm","predict.lm","predict.glm","predict","mean","cost","cv.glm","delete.response","predict.lm","predict.glm","predict","mean","cost","cv.glm","list","structure","family","glm","eval","eval","eval.parent","cv.glm","eval","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","anyDuplicated.default","anyDuplicated","[.data.frame","[","na.omit.data.frame","na.omit",".External2","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","is.ordered","FUN","vapply","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","[[.data.frame","[[","$.data.frame","$","model.weights","as.vector","glm","eval","eval","eval.parent","cv.glm","glm","eval","eval","eval.parent","cv.glm","model.frame.default","stats::model.frame","eval","eval","glm","eval","eval","eval.parent","cv.glm","glm.fit","eval","eval","glm","eval","eval","eval.parent","cv.glm","cv.glm","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm","match","%in%","deparse","paste","FUN","lapply","sapply","match","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm",".External2","model.matrix.default","model.matrix","glm","eval","eval","eval.parent","cv.glm"],"filenum":[null,null,null,null,null,null,null,null,1,null,null,null,null,null,1,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1,1,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,null,null,null,null,null,null,null,1,null,null,null,null,null,null,null,1],"linenum":[null,null,null,null,null,null,null,null,9,null,null,null,null,null,9,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9,9,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,null,null,null,null,null,null,null,9,null,null,null,null,null,null,null,9],"memalloc":[12.9335784912109,12.9335784912109,12.9335784912109,12.9335784912109,12.9335784912109,12.9335784912109,12.9335784912109,12.9335784912109,12.9335784912109,15.0008926391602,15.0008926391602,15.0008926391602,15.0008926391602,15.0008926391602,15.0008926391602,17.6344375610352,17.6344375610352,17.6344375610352,17.6344375610352,17.6344375610352,17.6344375610352,17.6344375610352,12.0588760375977,12.0588760375977,12.0588760375977,12.0588760375977,12.0588760375977,12.0588760375977,12.0588760375977,12.0588760375977,14.7419967651367,14.7419967651367,14.7419967651367,14.7419967651367,14.7419967651367,14.7419967651367,14.7419967651367,14.7419967651367,14.7419967651367,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,11.3077850341797,14.9752731323242,14.9752731323242,14.9752731323242,14.9752731323242,14.9752731323242,14.9752731323242,14.9752731323242,14.9752731323242,14.9752731323242,14.9752731323242,14.9752731323242,14.9752731323242,17.7517623901367,17.7517623901367,17.7517623901367,17.7517623901367,17.7517623901367,17.7517623901367,17.7517623901367,17.7517623901367,17.7517623901367,17.7517623901367,17.7517623901367,15.1074447631836,15.1074447631836,15.1074447631836,15.1074447631836,15.1074447631836,18.5343246459961,18.5343246459961,18.5343246459961,18.5343246459961,18.5343246459961,18.5343246459961,18.5343246459961,18.5343246459961,18.5343246459961,13.3498764038086,13.3498764038086,13.3498764038086,13.3498764038086,13.3498764038086,13.3498764038086,13.3498764038086,13.3498764038086,16.7066421508789,12.4074249267578,12.4074249267578,12.4074249267578,12.4074249267578,12.4074249267578,12.4074249267578,12.4074249267578,12.4074249267578,12.4074249267578,15.8992004394531,15.8992004394531,15.8992004394531,15.8992004394531,15.8992004394531,15.8992004394531,15.8992004394531,15.8992004394531,15.8992004394531,15.8992004394531,15.8992004394531,15.8992004394531,15.8992004394531,15.8992004394531,15.8992004394531,19.4107894897461,19.4107894897461,19.4107894897461,19.4107894897461,19.4107894897461,19.4107894897461,19.4107894897461,19.4107894897461],"meminc":[0,0,0,0,0,0,0,0,0,2.06731414794922,0,0,0,0,0,2.633544921875,0,0,0,0,0,0,-5.5755615234375,0,0,0,0,0,0,0,2.68312072753906,0,0,0,0,0,0,0,0,-3.43421173095703,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.66748809814453,0,0,0,0,0,0,0,0,0,0,0,2.7764892578125,0,0,0,0,0,0,0,0,0,0,-2.64431762695312,0,0,0,0,3.4268798828125,0,0,0,0,0,0,0,0,-5.1844482421875,0,0,0,0,0,0,0,3.35676574707031,-4.29921722412109,0,0,0,0,0,0,0,0,3.49177551269531,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.51158905029297,0,0,0,0,0,0,0],"filename":[null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,"<expr>",null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>","<expr>",null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,null,null,null,null,null,null,null,"<expr>",null,null,null,null,null,null,null,"<expr>"]},"interval":10,"files":[{"filename":"<expr>","content":"library(profvis)\n\nprofvis({\n  cv_error_fold10 <- vector(\"numeric\", 5)\n  terms <- 1:5\n  \n  for (i in terms) {\n    glm_fit <- glm(mpg ~ poly(horsepower, i), data = Auto)\n    cv_error_fold10[[i]] <- cv.glm(Auto, glm_fit, K = 10)$delta[[1]]\n  }\n})","normpath":"<expr>"}],"prof_output":"/var/folders/vw/l7k7vwhn3qqd990ww0dd101c0000gn/T//RtmpMC8zYb/file7ef82e998c84.prof","highlight":{"output":["^output\\$"],"gc":["^<GC>$"],"stacktrace":["^\\.\\.stacktraceo(n|ff)\\.\\.$"]},"split":"h"}},"evals":[],"jsHooks":[]}</script>
<p>On my machine, 10-fold CV was about 40 times faster than LOOCV. Again, estimating <span class="math inline">\(k=10\)</span> models is going to be much easier than estimating <span class="math inline">\(k=392\)</span> models.</p>
</div>
</div>
<div id="k-fold-cv-in-logistic-regression" class="section level3">
<h3>k-fold CV in logistic regression</h3>
<p>You’ve gotten the idea by now, but let’s do it one more time on our interactive Titanic model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_kfold &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">is.na</span>(Survived), !<span class="kw">is.na</span>(Age), !<span class="kw">is.na</span>(Sex)) %&gt;%
<span class="st">  </span><span class="kw">cv.glm</span>(titanic_model, <span class="dt">K =</span> <span class="dv">10</span>)
titanic_kfold$delta[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>## [1] 0.1708052</code></pre>
<p>Not a large difference from the LOOCV approach, but it take much less time to compute.</p>
</div>
</div>
</div>
<div id="decision-trees" class="section level1">
<h1>Decision trees</h1>
<div class="figure">
<img src="https://eight2late.files.wordpress.com/2016/02/7214525854_733237dd83_z1.jpg?w=700" />

</div>
<div class="figure">
<img src="https://s-media-cache-ak0.pinimg.com/564x/0b/87/df/0b87df1a54474716384f8ec94b52eab9.jpg" />

</div>
<div class="figure">
<img src="http://data.iwastesomuchtime.com/November-26-2012-17-34-05-cookie.gif" alt="Should I Have a Cookie?" />
<p class="caption"><a href="http://iwastesomuchtime.com/58217">Should I Have a Cookie?</a></p>
</div>
<p><em>Decision trees</em> are intuitive concepts for making decisions. They are also useful methods for regression and classification. They work by splitting the observations into a number of regions, and predictions are made based on the mean or mode of the training observations in that region.</p>
<div id="interpreting-a-decision-tree" class="section level2">
<h2>Interpreting a decision tree</h2>
<p>Let’s start with the Titanic data. I want to predict who lives and who dies during this event. Instead of using logistic regression, I’m going to calculate a decision tree based on a passenger’s age and gender. Here’s what that decision tree looks like:</p>
<p><img src="stat_learning02_files/figure-html/titanic_tree-1.png" width="672" /></p>
<p>Some key terminology:</p>
<ul>
<li>Each outcome (survived or died) is a <em>terminal node</em> or a <em>leaf</em></li>
<li>Splits occur at <em>internal nodes</em></li>
<li>The segments connecting each node are called <em>branches</em></li>
</ul>
<p>To make a prediction for a specific passenger, we start the decision tree from the top node and follow the appropriate branches down until we reach a terminal node. At each internal node, if our observation matches the condition, then travel down the left branch. If our observation does not match the condition, then travel down the right branch.</p>
<p>So for a 50 year old female passenger:</p>
<ul>
<li>Start at the first internal node. The passenger in question is a female, so take the branch to the left.</li>
<li>We reach a terminal node (“Survived”). We would predict the passenger in question survived the sinking of the Titanic.</li>
</ul>
<p>For a 20 year old male passenger:</p>
<ul>
<li>Start at the first internal node - the passenger in question is a male, so take the branch to the right.</li>
<li>The passenger in question is not less than 13 years old (R would say the condition is <code>FALSE</code>), so take the branch to the right.</li>
<li>We reach a terminal node (“Died”). We would predict the passenger in question died in the sinking of the Titanic.</li>
</ul>
</div>
<div id="estimating-a-decision-tree" class="section level2">
<h2>Estimating a decision tree</h2>
<p>First we need to load the <code>tree</code> library and prepare the data. <code>tree</code> is somewhat finicky about how data must be formatted in order to estimate the tree. For the Titanic data, we need to convert all qualitiative variables to <a href="http://r4ds.had.co.nz/factors.html">factors</a> using the <code>as.factor</code> function. To make interpretation easier, I also recode <code>Survived</code> from its <code>0/1</code> coding to explicitly identify which passengers survived and which died.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tree)

titanic_tree_data &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Survived =</span> <span class="kw">ifelse</span>(Survived ==<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;Survived&quot;</span>,
                           <span class="kw">ifelse</span>(Survived ==<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;Died&quot;</span>, <span class="ot">NA</span>)),
         <span class="dt">Survived =</span> <span class="kw">as.factor</span>(Survived),
         <span class="dt">Sex =</span> <span class="kw">as.factor</span>(Sex))
titanic_tree_data</code></pre></div>
<pre><code>## # A tibble: 891 × 12
##    PassengerId Survived Pclass
##          &lt;int&gt;   &lt;fctr&gt;  &lt;int&gt;
## 1            1     Died      3
## 2            2 Survived      1
## 3            3 Survived      3
## 4            4 Survived      1
## 5            5     Died      3
## 6            6     Died      3
## 7            7     Died      1
## 8            8     Died      3
## 9            9 Survived      3
## 10          10 Survived      2
## # ... with 881 more rows, and 9 more variables: Name &lt;chr&gt;, Sex &lt;fctr&gt;,
## #   Age &lt;dbl&gt;, SibSp &lt;int&gt;, Parch &lt;int&gt;, Ticket &lt;chr&gt;, Fare &lt;dbl&gt;,
## #   Cabin &lt;chr&gt;, Embarked &lt;chr&gt;</code></pre>
<p>Now we can use the <code>tree</code> function to estimate the model. The format looks exactly like <code>lm</code> or <code>glm</code> - first we specify the formula that defines the model, then we specify where the data is stored:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_tree &lt;-<span class="st"> </span><span class="kw">tree</span>(Survived ~<span class="st"> </span>Age +<span class="st"> </span>Sex, <span class="dt">data =</span> titanic_tree_data)
<span class="kw">summary</span>(titanic_tree)</code></pre></div>
<pre><code>## 
## Classification tree:
## tree(formula = Survived ~ Age + Sex, data = titanic_tree_data)
## Number of terminal nodes:  3 
## Residual mean deviance:  1.019 = 724.7 / 711 
## Misclassification error rate: 0.2129 = 152 / 714</code></pre>
<p>The <code>summary</code> function provides several important statistics:</p>
<ul>
<li>There are three terminal nodes in the tree</li>
<li><em>Residual mean deviance</em> is an estimate of model fit. It is usually helpful in comparing the effectiveness of different models.</li>
<li>This decision tree misclassifies <span class="math inline">\(21.3\%\)</span> of the training set observations (note that we did not create a validation set - this model is based on all the original data)</li>
</ul>
<p>That’s all well in good, but decision trees are meant to be viewed. Let’s plot it!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(titanic_tree)
<span class="kw">text</span>(titanic_tree, <span class="dt">pretty =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/titanic_tree_plot-1.png" width="672" /></p>
<p><code>tree</code> does not use <code>ggplot2</code> to graph the results; instead it relies on the base <code>graphics</code> package. <code>plot(titanic_tree)</code> draws the branches and <code>text(titanic_tree, pretty = 0)</code> adds the text labeling each node.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
<div id="build-a-more-complex-tree" class="section level3">
<h3>Build a more complex tree</h3>
<p>Since we have a lot of other variables in our Titanic data set, let’s estimate a more complex model that accounts for all the information we have.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> We’ll have to format all our columns this time before we can estimate the model. Because there are multiple qualitative variables as predictors, I will use <code>mutate_each</code> to apply <code>as.factor</code> to all of these variables in one line of code (another type of iterative operation):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_tree_full_data &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Survived =</span> <span class="kw">ifelse</span>(Survived ==<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;Survived&quot;</span>,
                           <span class="kw">ifelse</span>(Survived ==<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;Died&quot;</span>, <span class="ot">NA</span>))) %&gt;%
<span class="st">  </span><span class="kw">mutate_each</span>(<span class="kw">funs</span>(as.factor), Survived, Pclass, Sex, Embarked)

titanic_tree_full &lt;-<span class="st"> </span><span class="kw">tree</span>(Survived ~<span class="st"> </span>Pclass +<span class="st"> </span>Sex +<span class="st"> </span>Age +<span class="st"> </span>SibSp +
<span class="st">                       </span>Parch +<span class="st"> </span>Fare +<span class="st"> </span>Embarked, <span class="dt">data =</span> titanic_tree_full_data)
<span class="kw">summary</span>(titanic_tree_full)</code></pre></div>
<pre><code>## 
## Classification tree:
## tree(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + 
##     Fare + Embarked, data = titanic_tree_full_data)
## Variables actually used in tree construction:
## [1] &quot;Sex&quot;    &quot;Pclass&quot; &quot;Fare&quot;   &quot;Age&quot;    &quot;SibSp&quot; 
## Number of terminal nodes:  7 
## Residual mean deviance:  0.7995 = 563.6 / 705 
## Misclassification error rate: 0.1742 = 124 / 712</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(titanic_tree_full)
<span class="kw">text</span>(titanic_tree_full, <span class="dt">pretty =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/titanic_tree_full-1.png" width="672" /></p>
<p>Now we’ve built a more complicated decision tree. Fortunately it is still pretty interpretable. Notice that some of the variables we included in the model (<code>Parch</code> and <code>Embarked</code>) ended up being dropped from the final model. This is because to build the tree and ensure it is not overly complicated, the algorithm goes through a process of iteration and <em>pruning</em> to remove twigs or branches that result in a complicated model that does not provide significant improvement in overall model accuracy. You can tweak these parameters to ensure the model keeps all the variables, but could result in a nasty looking picture:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_tree_messy &lt;-<span class="st"> </span><span class="kw">tree</span>(Survived ~<span class="st"> </span>Pclass +<span class="st"> </span>Sex +<span class="st"> </span>Age +<span class="st"> </span>SibSp +
<span class="st">                       </span>Parch +<span class="st"> </span>Fare +<span class="st"> </span>Embarked,
                       <span class="dt">data =</span> titanic_tree_full_data,
                       <span class="dt">control =</span> <span class="kw">tree.control</span>(<span class="dt">nobs =</span> <span class="kw">nrow</span>(titanic_tree_full_data),
                                              <span class="dt">mindev =</span> <span class="dv">0</span>, <span class="dt">minsize =</span> <span class="dv">10</span>))
<span class="kw">summary</span>(titanic_tree_messy)</code></pre></div>
<pre><code>## 
## Classification tree:
## tree(formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + 
##     Fare + Embarked, data = titanic_tree_full_data, control = tree.control(nobs = nrow(titanic_tree_full_data), 
##     mindev = 0, minsize = 10))
## Number of terminal nodes:  76 
## Residual mean deviance:  0.5164 = 328.4 / 636 
## Misclassification error rate: 0.1124 = 80 / 712</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(titanic_tree_messy)
<span class="kw">text</span>(titanic_tree_messy, <span class="dt">pretty =</span> <span class="dv">0</span>)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/titanic_tree_complicated-1.png" width="672" /></p>
<p>The misclassification error rate for this model is much lower than the previous versions, but it is also much less interpretable. Depending on your audience and how you want to present the results of your statistical model, you need to determine the optimal trade-off between accuracy and interpretability.</p>
</div>
</div>
<div id="benefitsdrawbacks-to-decision-trees" class="section level2">
<h2>Benefits/drawbacks to decision trees</h2>
<p>Decision trees are an entirely different method of estimating functional forms as compared to linear regression. There are some benefits to trees:</p>
<ul>
<li>They are easy to explain. Most people, even if they lack statistical training, can understand decision trees.</li>
<li>They are easily presented as visualizations, and pretty interpretable.</li>
<li>Qualitative predictors are easily handled without the need to create a long series of dummy variables.</li>
</ul>
<p>However there are also drawbacks to trees:</p>
<ul>
<li>Their accuracy rates are generally lower than other regression and classification approaches.</li>
<li>Trees can be non-robust. That is, a small change in the data or inclusion/exclusion of a handful of observations can dramatically alter the final estimated tree.</li>
</ul>
<p>Fortuntately, there is an easy way to improve on these poor predictions: by aggregating many decision trees and averaging across them, we can substantially improve performance.</p>
</div>
<div id="random-forests" class="section level2">
<h2>Random forests</h2>
<p>One method of aggregating trees is the <em>random forest</em> approach. This uses the concept of <em>bootstrapping</em> build a forest of trees using the same underlying data set. Bootstrapping is standard resampling process whereby you repeatedly <em>sample with replacement</em> from a data set. So if you have a dataset of 500 observations, you might draw a sample of 500 observations from the data. But by sampling with replacement, some observations may be sampled multiple times and some observations may never be sampled. This essentially treats your data as a population of interest. You repeat this process many times (say 1000), then estimate your quantity or model of interest on each sample. Then finally you average across all the bootstrapped samples to calculate the final model or statistical estimator.</p>
<p>As with other resampling methods, each individual sample will have some degree of bias to it. However by averaging across all the bootstrapped samples you cancel out much of this bias. Most importantly, averaging a set of observations reduces <em>variance</em> - this is what LOOCV and <span class="math inline">\(k\)</span>-fold cross-validation do. You achieve stable estimates of the prediction accuracy or overall model error.</p>
<p>In the context of decision trees, this means we draw repeated samples from the original dataset and estimate a decision tree model on each sample. To make predictions, we estimate the outcome using each tree and average across all of them to obtain the final prediction. Rather than being a binary outcome (<span class="math inline">\([0,1]\)</span>, survived/died), the average prediction will be a probability of the given outcome (i.e. the probability of survival). This process is called <em>bagging</em>. Random forests go a step further: when building individual decision trees, each time a split in the tree is considered a random sample of predictors is selected as the candidates for the split. <em>Random forests specifically exclude a portion of the predictor variables when building individual trees</em>. Why throw away good data? This ensures each decision tree is not correlated with one another. If one specific variable was a strong predictor in the data set (say gender in the Titanic data set), it could potentially dominate every decision tree and the result would be nearly-identical trees regardless of the sampling procedure. By forcibly excluding a random subset of variables, individual trees in random forests will not have strong correlations with one another. Therefore the average predictions will be more <em>reliable</em>.</p>
</div>
</div>
<div id="estimating-statistical-models-using-caret" class="section level1">
<h1>Estimating statistical models using <code>caret</code></h1>
<p>To estimate a random forest, we move outside the world of <code>tree</code> and into a new package in R: <a href="https://cran.r-project.org/web/packages/caret/index.html"><code>caret</code></a>. <code>caret</code> is a package in R for training and plotting a wide variety of statistical learning models. It is outside of the <code>tidyverse</code> so can be a bit more difficult to master. <code>caret</code> does not contain the estimation algorithms itself; instead it creates a unified interface to approximately <a href="https://topepo.github.io/caret/available-models.html">233 different models</a> from various packages in R. To install <code>caret</code> and make sure you install all the related packages it relies on, run the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="st">&quot;caret&quot;</span>, <span class="dt">dependencies =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p>The basic function to train models is <code>train</code>. We can train regression and classification models using one of <a href="https://topepo.github.io/caret/available-models.html">these models</a>. For instance, rather than using <code>glm</code> to estimate a logistic regression model, we could use <code>caret</code> and the <code>&quot;glm&quot;</code> method instead. Note that <code>caret</code> is extremely picky about preparing data for analysis. For instance, we have to remove all missing values before training a model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)

titanic_clean &lt;-<span class="st"> </span>titanic %&gt;%
<span class="st">  </span><span class="kw">filter</span>(!<span class="kw">is.na</span>(Survived), !<span class="kw">is.na</span>(Age))

caret_glm &lt;-<span class="st"> </span><span class="kw">train</span>(Survived ~<span class="st"> </span>Age, <span class="dt">data =</span> titanic_clean,
                   <span class="dt">method =</span> <span class="st">&quot;glm&quot;</span>,
                   <span class="dt">family =</span> binomial,
                   <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;none&quot;</span>))
<span class="kw">summary</span>(caret_glm)</code></pre></div>
<pre><code>## 
## Call:
## NULL
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1488  -1.0361  -0.9544   1.3159   1.5908  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -0.05672    0.17358  -0.327   0.7438  
## Age         -0.01096    0.00533  -2.057   0.0397 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 960.23  on 712  degrees of freedom
## AIC: 964.23
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<ul>
<li><code>trControl = trainControl(method = &quot;none&quot;)</code> - by default <code>caret</code> implements a bootstrap resampling procedure to validate the results of the model. For our purposes here I want to turn that off by setting the resampling method to <code>&quot;none&quot;</code>.</li>
</ul>
<p>The results are identical to those obtained by the <code>glm</code> function:<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">glm_glm &lt;-<span class="st"> </span><span class="kw">glm</span>(Survived ~<span class="st"> </span>Age, <span class="dt">data =</span> titanic_clean, <span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)
<span class="kw">summary</span>(glm_glm)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ Age, family = &quot;binomial&quot;, data = titanic_clean)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1488  -1.0361  -0.9544   1.3159   1.5908  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -0.05672    0.17358  -0.327   0.7438  
## Age         -0.01096    0.00533  -2.057   0.0397 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 960.23  on 712  degrees of freedom
## AIC: 964.23
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div id="estimating-a-random-forest" class="section level2">
<h2>Estimating a random forest</h2>
<p>We will reuse <code>titanic_tree_full_data</code> with the adjustment that we need to remove observations with missing values. In the process, let’s pare the data frame down to only columns that will be used the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_rf_data &lt;-<span class="st"> </span>titanic_tree_full_data %&gt;%
<span class="st">  </span><span class="kw">select</span>(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %&gt;%
<span class="st">  </span><span class="kw">na.omit</span>()
titanic_rf_data</code></pre></div>
<pre><code>## # A tibble: 712 × 8
##    Survived Pclass    Sex   Age SibSp Parch    Fare Embarked
##      &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;fctr&gt;
## 1      Died      3   male    22     1     0  7.2500        S
## 2  Survived      1 female    38     1     0 71.2833        C
## 3  Survived      3 female    26     0     0  7.9250        S
## 4  Survived      1 female    35     1     0 53.1000        S
## 5      Died      3   male    35     0     0  8.0500        S
## 6      Died      1   male    54     0     0 51.8625        S
## 7      Died      3   male     2     3     1 21.0750        S
## 8  Survived      3 female    27     0     2 11.1333        S
## 9  Survived      2 female    14     1     0 30.0708        C
## 10 Survived      3 female     4     1     1 16.7000        S
## # ... with 702 more rows</code></pre>
<p>Now that the data is prepped, let’s estimate the model. To start, we’ll estimate a simple model that only uses age and gender. Again we use the <code>train</code> function but this time we will use the <code>rf</code> method.<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> To start with, I will estimate a forest with 200 trees (the default is 500 trees) and set the <code>trainControl</code> method to <code>&quot;oob&quot;</code> (I will explain this shortly):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">age_sex_rf &lt;-<span class="st"> </span><span class="kw">train</span>(Survived ~<span class="st"> </span>Age +<span class="st"> </span>Sex, <span class="dt">data =</span> titanic_rf_data,
                   <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,
                   <span class="dt">ntree =</span> <span class="dv">200</span>,
                   <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;oob&quot;</span>))</code></pre></div>
<pre><code>## note: only 1 unique complexity parameters in default grid. Truncating the grid to 1 .</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">age_sex_rf</code></pre></div>
<pre><code>## Random Forest 
## 
## 712 samples
##   2 predictor
##   2 classes: &#39;Died&#39;, &#39;Survived&#39; 
## 
## No pre-processing
## Resampling results:
## 
##   Accuracy   Kappa    
##   0.7598315  0.4938641
## 
## Tuning parameter &#39;mtry&#39; was held constant at a value of 2
## </code></pre>
<p>Hmm. What have we generated here? How can we analyze the results?</p>
<div id="structure-of-train-object" class="section level3">
<h3>Structure of <code>train</code> object</h3>
<p>The object generated by <code>train</code> is a named list:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(age_sex_rf, <span class="dt">max.level =</span> <span class="dv">1</span>)</code></pre></div>
<pre><code>## List of 24
##  $ method      : chr &quot;rf&quot;
##  $ modelInfo   :List of 15
##  $ modelType   : chr &quot;Classification&quot;
##  $ results     :&#39;data.frame&#39;:    1 obs. of  3 variables:
##  $ pred        : NULL
##  $ bestTune    :&#39;data.frame&#39;:    1 obs. of  1 variable:
##  $ call        : language train.formula(form = Survived ~ Age + Sex, data = titanic_rf_data,      method = &quot;rf&quot;, ntree = 200, trControl = trainControl(method = &quot;oob&quot;))
##  $ dots        :List of 1
##  $ metric      : chr &quot;Accuracy&quot;
##  $ control     :List of 26
##  $ finalModel  :List of 22
##   ..- attr(*, &quot;class&quot;)= chr &quot;randomForest&quot;
##  $ preProcess  : NULL
##  $ trainingData:Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;:    712 obs. of  3 variables:
##  $ resample    : NULL
##  $ resampledCM : NULL
##  $ perfNames   : chr [1:2] &quot;Accuracy&quot; &quot;Kappa&quot;
##  $ maximize    : logi TRUE
##  $ yLimits     : NULL
##  $ times       :List of 3
##  $ levels      : atomic [1:2] Died Survived
##   ..- attr(*, &quot;ordered&quot;)= logi FALSE
##  $ terms       :Classes &#39;terms&#39;, &#39;formula&#39;  language Survived ~ Age + Sex
##   .. ..- attr(*, &quot;variables&quot;)= language list(Survived, Age, Sex)
##   .. ..- attr(*, &quot;factors&quot;)= int [1:3, 1:2] 0 1 0 0 0 1
##   .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..- attr(*, &quot;term.labels&quot;)= chr [1:2] &quot;Age&quot; &quot;Sex&quot;
##   .. ..- attr(*, &quot;order&quot;)= int [1:2] 1 1
##   .. ..- attr(*, &quot;intercept&quot;)= int 1
##   .. ..- attr(*, &quot;response&quot;)= int 1
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; 
##   .. ..- attr(*, &quot;predvars&quot;)= language list(Survived, Age, Sex)
##   .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:3] &quot;factor&quot; &quot;numeric&quot; &quot;factor&quot;
##   .. .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Survived&quot; &quot;Age&quot; &quot;Sex&quot;
##  $ coefnames   : chr [1:2] &quot;Age&quot; &quot;Sexmale&quot;
##  $ contrasts   :List of 1
##  $ xlevels     :List of 1
##  - attr(*, &quot;class&quot;)= chr [1:2] &quot;train&quot; &quot;train.formula&quot;</code></pre>
<p>The model itself is always stored in the <code>finalModel</code> element. So to use the model in other functions, we would refer to it as <code>age_sex_rf$finalModel</code>.</p>
</div>
<div id="model-statistics" class="section level3">
<h3>Model statistics</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">age_sex_rf$finalModel</code></pre></div>
<pre><code>## 
## Call:
##  randomForest(x = x, y = y, ntree = 200, mtry = param$mtry) 
##                Type of random forest: classification
##                      Number of trees: 200
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 24.3%
## Confusion matrix:
##          Died Survived class.error
## Died      352       72   0.1698113
## Survived  101      187   0.3506944</code></pre>
<p>This tells us some important things:</p>
<ul>
<li>We used 200 trees</li>
<li>At every potential branch, the model randomly used one of 2 variables to define the split</li>
<li><p>The <em>out-of-bag</em> (OOB) error rate</p>
<p>This requires further explanation. Because each tree is built from a bootstrapped sample, for any given tree approximately one-third of the observations are not used to build the tree. In essence, we have a natural validation set for each tree. For each observation, we predict the outcome of interest using all trees where the observation was not used to build the tree, then average across these predictions. For any observation, we should have <span class="math inline">\(K/3\)</span> validation predictions where <span class="math inline">\(K\)</span> is the total number of trees in the forest. Averaging across these predictions gives us an out-of-bag error rate for every observation (even if they are derived from different combinations of trees). Because the OOB estimate is built only using trees that were not fit to the observation, this is a valid estimate of the test error for the random forest.</p>
Here we get an OOB estimate of the error rate of 24%. This means for test observations, the model misclassifies the individual’s survival 24% of the time.</li>
<li><p>The <em>confusion matrix</em> - this compares the predictions to the actual known outcomes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knitr::<span class="kw">kable</span>(age_sex_rf$finalModel$confusion)</code></pre></div>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">Died</th>
<th align="right">Survived</th>
<th align="right">class.error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Died</td>
<td align="right">352</td>
<td align="right">72</td>
<td align="right">0.1698113</td>
</tr>
<tr class="even">
<td>Survived</td>
<td align="right">101</td>
<td align="right">187</td>
<td align="right">0.3506944</td>
</tr>
</tbody>
</table>
<p>The rows indicate the actual known outcomes, and the columns indicate the predictions. A perfect model would have 0s on the off-diagonal cells because every prediction is perfect. Clearly that is not the case. Not only is there substantial error, most it comes from misclassifying survivors. The error rate for those who actually died is much smaller than for those who actually survived.</p></li>
</ul>
</div>
<div id="look-at-an-individual-tree" class="section level3">
<h3>Look at an individual tree</h3>
<p>We could look at one tree generated by the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">randomForest::<span class="kw">getTree</span>(age_sex_rf$finalModel, <span class="dt">labelVar =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>##     left daughter right daughter split var split point status prediction
## 1               2              3   Sexmale        0.50      1       &lt;NA&gt;
## 2               4              5       Age       21.50      1       &lt;NA&gt;
## 3               6              7       Age        6.50      1       &lt;NA&gt;
## 4               8              9       Age       19.50      1       &lt;NA&gt;
## 5              10             11       Age       37.00      1       &lt;NA&gt;
## 6              12             13       Age        0.96      1       &lt;NA&gt;
## 7              14             15       Age       23.75      1       &lt;NA&gt;
## 8              16             17       Age        1.50      1       &lt;NA&gt;
## 9              18             19       Age       20.50      1       &lt;NA&gt;
## 10             20             21       Age       32.25      1       &lt;NA&gt;
## 11             22             23       Age       39.50      1       &lt;NA&gt;
## 12              0              0      &lt;NA&gt;        0.00     -1   Survived
## 13             24             25       Age        5.00      1       &lt;NA&gt;
## 14             26             27       Age       13.00      1       &lt;NA&gt;
## 15             28             29       Age       51.50      1       &lt;NA&gt;
## 16              0              0      &lt;NA&gt;        0.00     -1   Survived
## 17             30             31       Age        2.50      1       &lt;NA&gt;
## 18              0              0      &lt;NA&gt;        0.00     -1       Died
## 19              0              0      &lt;NA&gt;        0.00     -1       Died
## 20             32             33       Age       30.25      1       &lt;NA&gt;
## 21              0              0      &lt;NA&gt;        0.00     -1   Survived
## 22              0              0      &lt;NA&gt;        0.00     -1       Died
## 23             34             35       Age       56.50      1       &lt;NA&gt;
## 24             36             37       Age        3.50      1       &lt;NA&gt;
## 25              0              0      &lt;NA&gt;        0.00     -1   Survived
## 26             38             39       Age       10.00      1       &lt;NA&gt;
## 27             40             41       Age       22.50      1       &lt;NA&gt;
## 28             42             43       Age       44.50      1       &lt;NA&gt;
## 29             44             45       Age       77.00      1       &lt;NA&gt;
## 30              0              0      &lt;NA&gt;        0.00     -1       Died
## 31             46             47       Age        5.50      1       &lt;NA&gt;
## 32             48             49       Age       24.50      1       &lt;NA&gt;
## 33             50             51       Age       30.75      1       &lt;NA&gt;
## 34             52             53       Age       50.50      1       &lt;NA&gt;
## 35             54             55       Age       57.50      1       &lt;NA&gt;
## 36             56             57       Age        2.50      1       &lt;NA&gt;
## 37              0              0      &lt;NA&gt;        0.00     -1       Died
## 38             58             59       Age        8.50      1       &lt;NA&gt;
## 39             60             61       Age       11.50      1       &lt;NA&gt;
## 40             62             63       Age       16.50      1       &lt;NA&gt;
## 41              0              0      &lt;NA&gt;        0.00     -1       Died
## 42             64             65       Age       39.50      1       &lt;NA&gt;
## 43             66             67       Age       45.25      1       &lt;NA&gt;
## 44             68             69       Age       63.00      1       &lt;NA&gt;
## 45              0              0      &lt;NA&gt;        0.00     -1   Survived
## 46             70             71       Age        3.50      1       &lt;NA&gt;
## 47             72             73       Age       12.50      1       &lt;NA&gt;
## 48             74             75       Age       22.50      1       &lt;NA&gt;
## 49             76             77       Age       25.50      1       &lt;NA&gt;
## 50              0              0      &lt;NA&gt;        0.00     -1       Died
## 51             78             79       Age       31.50      1       &lt;NA&gt;
## 52             80             81       Age       43.50      1       &lt;NA&gt;
## 53              0              0      &lt;NA&gt;        0.00     -1   Survived
## 54              0              0      &lt;NA&gt;        0.00     -1       Died
## 55              0              0      &lt;NA&gt;        0.00     -1   Survived
## 56             82             83       Age        1.50      1       &lt;NA&gt;
## 57              0              0      &lt;NA&gt;        0.00     -1   Survived
## 58             84             85       Age        7.50      1       &lt;NA&gt;
## 59              0              0      &lt;NA&gt;        0.00     -1       Died
## 60              0              0      &lt;NA&gt;        0.00     -1   Survived
## 61              0              0      &lt;NA&gt;        0.00     -1   Survived
## 62              0              0      &lt;NA&gt;        0.00     -1       Died
## 63             86             87       Age       17.50      1       &lt;NA&gt;
## 64             88             89       Age       33.50      1       &lt;NA&gt;
## 65             90             91       Age       43.50      1       &lt;NA&gt;
## 66              0              0      &lt;NA&gt;        0.00     -1   Survived
## 67             92             93       Age       47.50      1       &lt;NA&gt;
## 68             94             95       Age       59.50      1       &lt;NA&gt;
## 69              0              0      &lt;NA&gt;        0.00     -1       Died
## 70              0              0      &lt;NA&gt;        0.00     -1   Survived
## 71              0              0      &lt;NA&gt;        0.00     -1   Survived
## 72             96             97       Age        7.50      1       &lt;NA&gt;
## 73             98             99       Age       14.25      1       &lt;NA&gt;
## 74              0              0      &lt;NA&gt;        0.00     -1   Survived
## 75            100            101       Age       23.50      1       &lt;NA&gt;
## 76              0              0      &lt;NA&gt;        0.00     -1       Died
## 77            102            103       Age       26.50      1       &lt;NA&gt;
## 78              0              0      &lt;NA&gt;        0.00     -1   Survived
## 79              0              0      &lt;NA&gt;        0.00     -1   Survived
## 80            104            105       Age       41.00      1       &lt;NA&gt;
## 81            106            107       Age       46.50      1       &lt;NA&gt;
## 82              0              0      &lt;NA&gt;        0.00     -1   Survived
## 83              0              0      &lt;NA&gt;        0.00     -1       Died
## 84              0              0      &lt;NA&gt;        0.00     -1       Died
## 85              0              0      &lt;NA&gt;        0.00     -1   Survived
## 86              0              0      &lt;NA&gt;        0.00     -1       Died
## 87            108            109       Age       18.50      1       &lt;NA&gt;
## 88            110            111       Age       32.50      1       &lt;NA&gt;
## 89            112            113       Age       35.50      1       &lt;NA&gt;
## 90            114            115       Age       40.25      1       &lt;NA&gt;
## 91              0              0      &lt;NA&gt;        0.00     -1       Died
## 92              0              0      &lt;NA&gt;        0.00     -1       Died
## 93            116            117       Age       49.50      1       &lt;NA&gt;
## 94            118            119       Age       53.00      1       &lt;NA&gt;
## 95            120            121       Age       60.50      1       &lt;NA&gt;
## 96            122            123       Age        6.50      1       &lt;NA&gt;
## 97              0              0      &lt;NA&gt;        0.00     -1       Died
## 98              0              0      &lt;NA&gt;        0.00     -1   Survived
## 99            124            125       Age       14.75      1       &lt;NA&gt;
## 100             0              0      &lt;NA&gt;        0.00     -1   Survived
## 101             0              0      &lt;NA&gt;        0.00     -1   Survived
## 102             0              0      &lt;NA&gt;        0.00     -1   Survived
## 103           126            127       Age       27.50      1       &lt;NA&gt;
## 104             0              0      &lt;NA&gt;        0.00     -1   Survived
## 105             0              0      &lt;NA&gt;        0.00     -1   Survived
## 106           128            129       Age       44.50      1       &lt;NA&gt;
## 107           130            131       Age       49.50      1       &lt;NA&gt;
## 108             0              0      &lt;NA&gt;        0.00     -1       Died
## 109           132            133       Age       19.50      1       &lt;NA&gt;
## 110           134            135       Age       31.50      1       &lt;NA&gt;
## 111             0              0      &lt;NA&gt;        0.00     -1       Died
## 112           136            137       Age       34.50      1       &lt;NA&gt;
## 113           138            139       Age       36.50      1       &lt;NA&gt;
## 114             0              0      &lt;NA&gt;        0.00     -1       Died
## 115           140            141       Age       41.50      1       &lt;NA&gt;
## 116           142            143       Age       48.50      1       &lt;NA&gt;
## 117           144            145       Age       50.50      1       &lt;NA&gt;
## 118             0              0      &lt;NA&gt;        0.00     -1       Died
## 119             0              0      &lt;NA&gt;        0.00     -1       Died
## 120             0              0      &lt;NA&gt;        0.00     -1       Died
## 121           146            147       Age       61.50      1       &lt;NA&gt;
## 122             0              0      &lt;NA&gt;        0.00     -1       Died
## 123             0              0      &lt;NA&gt;        0.00     -1   Survived
## 124             0              0      &lt;NA&gt;        0.00     -1       Died
## 125           148            149       Age       15.50      1       &lt;NA&gt;
## 126             0              0      &lt;NA&gt;        0.00     -1   Survived
## 127           150            151       Age       29.50      1       &lt;NA&gt;
## 128             0              0      &lt;NA&gt;        0.00     -1   Survived
## 129             0              0      &lt;NA&gt;        0.00     -1   Survived
## 130           152            153       Age       48.50      1       &lt;NA&gt;
## 131             0              0      &lt;NA&gt;        0.00     -1   Survived
## 132             0              0      &lt;NA&gt;        0.00     -1       Died
## 133           154            155       Age       20.75      1       &lt;NA&gt;
## 134           156            157       Age       29.50      1       &lt;NA&gt;
## 135             0              0      &lt;NA&gt;        0.00     -1       Died
## 136             0              0      &lt;NA&gt;        0.00     -1       Died
## 137             0              0      &lt;NA&gt;        0.00     -1       Died
## 138             0              0      &lt;NA&gt;        0.00     -1       Died
## 139           158            159       Age       37.50      1       &lt;NA&gt;
## 140             0              0      &lt;NA&gt;        0.00     -1       Died
## 141           160            161       Age       42.50      1       &lt;NA&gt;
## 142             0              0      &lt;NA&gt;        0.00     -1       Died
## 143             0              0      &lt;NA&gt;        0.00     -1   Survived
## 144             0              0      &lt;NA&gt;        0.00     -1       Died
## 145             0              0      &lt;NA&gt;        0.00     -1       Died
## 146             0              0      &lt;NA&gt;        0.00     -1       Died
## 147             0              0      &lt;NA&gt;        0.00     -1       Died
## 148             0              0      &lt;NA&gt;        0.00     -1   Survived
## 149           162            163       Age       18.50      1       &lt;NA&gt;
## 150           164            165       Age       28.50      1       &lt;NA&gt;
## 151             0              0      &lt;NA&gt;        0.00     -1   Survived
## 152             0              0      &lt;NA&gt;        0.00     -1   Survived
## 153             0              0      &lt;NA&gt;        0.00     -1   Survived
## 154           166            167       Age       20.25      1       &lt;NA&gt;
## 155           168            169       Age       21.50      1       &lt;NA&gt;
## 156           170            171       Age       26.50      1       &lt;NA&gt;
## 157           172            173       Age       30.75      1       &lt;NA&gt;
## 158             0              0      &lt;NA&gt;        0.00     -1   Survived
## 159           174            175       Age       38.50      1       &lt;NA&gt;
## 160             0              0      &lt;NA&gt;        0.00     -1       Died
## 161             0              0      &lt;NA&gt;        0.00     -1       Died
## 162           176            177       Age       17.50      1       &lt;NA&gt;
## 163             0              0      &lt;NA&gt;        0.00     -1   Survived
## 164             0              0      &lt;NA&gt;        0.00     -1   Survived
## 165             0              0      &lt;NA&gt;        0.00     -1   Survived
## 166             0              0      &lt;NA&gt;        0.00     -1       Died
## 167             0              0      &lt;NA&gt;        0.00     -1       Died
## 168             0              0      &lt;NA&gt;        0.00     -1       Died
## 169             0              0      &lt;NA&gt;        0.00     -1       Died
## 170           178            179       Age       25.50      1       &lt;NA&gt;
## 171           180            181       Age       28.50      1       &lt;NA&gt;
## 172           182            183       Age       30.25      1       &lt;NA&gt;
## 173             0              0      &lt;NA&gt;        0.00     -1       Died
## 174             0              0      &lt;NA&gt;        0.00     -1       Died
## 175             0              0      &lt;NA&gt;        0.00     -1       Died
## 176           184            185       Age       16.50      1       &lt;NA&gt;
## 177             0              0      &lt;NA&gt;        0.00     -1   Survived
## 178           186            187       Age       24.25      1       &lt;NA&gt;
## 179             0              0      &lt;NA&gt;        0.00     -1       Died
## 180           188            189       Age       27.50      1       &lt;NA&gt;
## 181             0              0      &lt;NA&gt;        0.00     -1       Died
## 182             0              0      &lt;NA&gt;        0.00     -1       Died
## 183             0              0      &lt;NA&gt;        0.00     -1       Died
## 184             0              0      &lt;NA&gt;        0.00     -1   Survived
## 185             0              0      &lt;NA&gt;        0.00     -1   Survived
## 186             0              0      &lt;NA&gt;        0.00     -1       Died
## 187           190            191       Age       24.75      1       &lt;NA&gt;
## 188             0              0      &lt;NA&gt;        0.00     -1       Died
## 189             0              0      &lt;NA&gt;        0.00     -1       Died
## 190             0              0      &lt;NA&gt;        0.00     -1       Died
## 191             0              0      &lt;NA&gt;        0.00     -1       Died</code></pre>
<p>Unfortunately there is no easy plotting mechanism for the result of <code>getTree</code>.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
<p>Yikes. Clearly this tree is pretty complicated. Not something we want to examine directly.</p>
</div>
<div id="variable-importance" class="section level3">
<h3>Variable importance</h3>
<p>Another method of interpreting random forests looks at the importance of individual variables in the model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">varImpPlot</span>(age_sex_rf$finalModel)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/rf_import-1.png" width="672" /></p>
<p>This tells us how much each variable decreases the average <em>Gini index</em>, a measure of how important the variable is to the model. Essentially, it estimates the impact a variable has on the model by comparing prediction accuracy rates for models with and without the variable. Larger values indicate higher importance of the variable. Here we see that the gender variable <code>Sexmale</code> is most important.</p>
</div>
<div id="prediction" class="section level3">
<h3>Prediction</h3>
<p>We can also use random forests to make predictions on an explicit validation set, rather than relying on OOB estimates. Let’s split our Titanic data into training and test sets, train the full random forest model on the training set, then use that model to predict outcomes in the test set. Instead of using a bootstrapped resampling method, again let’s use <code>&quot;oob&quot;</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_split &lt;-<span class="st"> </span><span class="kw">resample_partition</span>(titanic_rf_data,
                                    <span class="kw">c</span>(<span class="dt">test =</span> <span class="fl">0.3</span>, <span class="dt">train =</span> <span class="fl">0.7</span>))
titanic_train &lt;-<span class="st"> </span>titanic_split$train %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>()
titanic_test &lt;-<span class="st"> </span>titanic_split$test %&gt;%
<span class="st">  </span><span class="kw">tbl_df</span>()

rf_full &lt;-<span class="st"> </span><span class="kw">train</span>(Survived ~<span class="st"> </span>Pclass +<span class="st"> </span>Sex +<span class="st"> </span>Age +<span class="st"> </span>SibSp +
<span class="st">                   </span>Parch +<span class="st"> </span>Fare +<span class="st"> </span>Embarked,
                 <span class="dt">data =</span> titanic_train,
                 <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>,
                 <span class="dt">ntree =</span> <span class="dv">500</span>,
                 <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;oob&quot;</span>))
rf_full$finalModel</code></pre></div>
<pre><code>## 
## Call:
##  randomForest(x = x, y = y, ntree = 500, mtry = param$mtry) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 19.04%
## Confusion matrix:
##          Died Survived class.error
## Died      270       28  0.09395973
## Survived   67      134  0.33333333</code></pre>
<ul>
<li>We used 500 trees</li>
<li>At every potential branch, the model randomly used one of 2 variables to define the split</li>
<li>For OOB test observations, the model misclassifies the individual’s survival 19% of the time.</li>
<li>This model is somewhat better at predicting survivors compared to the age + gender model, but is still worse at predicting survivors from the deceased. This is not terribly surprising since our classes are <em>unbalanced</em>. That is, there were a lot fewer survivors (342) than deceased (549). Because of this, the model has more information on those that died than those that lived, so it is natural to have better predictions for those that died.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">varImpPlot</span>(rf_full$finalModel)</code></pre></div>
<p><img src="stat_learning02_files/figure-html/rf_validate_imp-1.png" width="672" /></p>
<p>Note that gender and age are important predictors in the random forest, but so too is the fare an individual paided. This is a proxy for socioeconomic status; recall that <a href="https://www.youtube.com/watch?v=NfDZO9QAiEM">the wealthy had better access to lifeboats</a>.</p>
<p>To make predictions for the validation set, we use the <code>predict</code> function. By setting <code>type = &quot;prob&quot;</code> we will get predicted probabilities for each possible outcome, rather than just a raw prediction of “Survived” or “Died”:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">titanic_pred &lt;-<span class="st"> </span>titanic_test %&gt;%
<span class="st">  </span><span class="kw">bind_cols</span>(<span class="kw">predict</span>(rf_full, <span class="dt">newdata =</span> titanic_test, <span class="dt">type =</span> <span class="st">&quot;prob&quot;</span>) %&gt;%
<span class="st">              </span><span class="kw">rename</span>(<span class="dt">prob_dead =</span> Died,
                     <span class="dt">prob_survive =</span> Survived))
titanic_pred</code></pre></div>
<pre><code>## # A tibble: 213 × 10
##    Survived Pclass    Sex   Age SibSp Parch     Fare Embarked prob_dead
##      &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;   &lt;fctr&gt;     &lt;dbl&gt;
## 1  Survived      3 female    26     0     0   7.9250        S     0.536
## 2      Died      3   male    35     0     0   8.0500        S     0.996
## 3  Survived      2 female    14     1     0  30.0708        C     0.110
## 4      Died      3   male    20     0     0   8.0500        S     0.998
## 5      Died      2   male    35     0     0  26.0000        S     0.940
## 6  Survived      2   male    34     0     0  13.0000        S     0.972
## 7  Survived      3 female    38     1     5  31.3875        S     0.910
## 8      Died      1   male    19     3     2 263.0000        S     0.584
## 9      Died      1   male    40     0     0  27.7208        C     0.734
## 10     Died      2   male    66     0     0  10.5000        S     0.864
## # ... with 203 more rows, and 1 more variables: prob_survive &lt;dbl&gt;</code></pre>
</div>
</div>
</div>
<div id="acknowledgments" class="section level1 toc-ignore">
<h1>Acknowledgments</h1>
<ul>
<li>For more information on statistical learning and the math behind these methods, see the awesome book <a href="http://link.springer.com.proxy.uchicago.edu/book/10.1007%2F978-1-4614-7138-7"><em>An Introduction to Statistical Learning</em></a></li>
</ul>
</div>
<div id="session-info" class="section level1 toc-ignore">
<h1>Session Info</h1>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools::<span class="kw">session_info</span>()</code></pre></div>
<pre><code>##  setting  value                       
##  version  R version 3.3.1 (2016-06-21)
##  system   x86_64, darwin13.4.0        
##  ui       X11                         
##  language (EN)                        
##  collate  en_US.UTF-8                 
##  tz       America/Chicago             
##  date     2016-11-16                  
## 
##  package      * version date       source                           
##  assertthat     0.1     2013-12-06 CRAN (R 3.3.0)                   
##  boot         * 1.3-18  2016-02-23 CRAN (R 3.3.1)                   
##  broom        * 0.4.1   2016-06-24 CRAN (R 3.3.0)                   
##  car            2.1-3   2016-08-11 CRAN (R 3.3.0)                   
##  caret        * 6.0-73  2016-11-10 CRAN (R 3.3.2)                   
##  class          7.3-14  2015-08-30 CRAN (R 3.3.1)                   
##  codetools      0.2-15  2016-10-05 CRAN (R 3.3.0)                   
##  colorspace     1.2-7   2016-10-11 CRAN (R 3.3.0)                   
##  DBI            0.5-1   2016-09-10 CRAN (R 3.3.0)                   
##  devtools       1.12.0  2016-06-24 CRAN (R 3.3.0)                   
##  digest         0.6.10  2016-08-02 CRAN (R 3.3.0)                   
##  dplyr        * 0.5.0   2016-06-24 CRAN (R 3.3.0)                   
##  e1071          1.6-7   2015-08-05 CRAN (R 3.3.0)                   
##  evaluate       0.10    2016-10-11 CRAN (R 3.3.0)                   
##  foreach        1.4.3   2015-10-13 CRAN (R 3.3.0)                   
##  foreign        0.8-67  2016-09-13 CRAN (R 3.3.0)                   
##  gapminder    * 0.2.0   2015-12-31 CRAN (R 3.3.0)                   
##  gganimate    * 0.1     2016-11-11 Github (dgrtwo/gganimate@26ec501)
##  ggplot2      * 2.2.0   2016-11-10 Github (hadley/ggplot2@f442f32)  
##  gtable         0.2.0   2016-02-26 CRAN (R 3.3.0)                   
##  highr          0.6     2016-05-09 CRAN (R 3.3.0)                   
##  htmltools      0.3.5   2016-03-21 CRAN (R 3.3.0)                   
##  htmlwidgets    0.8     2016-11-09 CRAN (R 3.3.1)                   
##  ISLR         * 1.0     2013-06-11 CRAN (R 3.3.0)                   
##  iterators      1.0.8   2015-10-13 CRAN (R 3.3.0)                   
##  jsonlite       1.1     2016-09-14 CRAN (R 3.3.0)                   
##  knitr          1.15    2016-11-09 CRAN (R 3.3.1)                   
##  labeling       0.3     2014-08-23 CRAN (R 3.3.0)                   
##  lattice      * 0.20-34 2016-09-06 CRAN (R 3.3.0)                   
##  lazyeval       0.2.0   2016-06-12 CRAN (R 3.3.0)                   
##  lme4           1.1-12  2016-04-16 cran (@1.1-12)                   
##  magrittr       1.5     2014-11-22 CRAN (R 3.3.0)                   
##  MASS           7.3-45  2016-04-21 CRAN (R 3.3.1)                   
##  Matrix         1.2-7.1 2016-09-01 CRAN (R 3.3.0)                   
##  MatrixModels   0.4-1   2015-08-22 CRAN (R 3.3.0)                   
##  memoise        1.0.0   2016-01-29 CRAN (R 3.3.0)                   
##  mgcv           1.8-16  2016-11-07 CRAN (R 3.3.0)                   
##  minqa          1.2.4   2014-10-09 cran (@1.2.4)                    
##  mnormt         1.5-5   2016-10-15 CRAN (R 3.3.0)                   
##  ModelMetrics   1.1.0   2016-08-26 CRAN (R 3.3.0)                   
##  modelr       * 0.1.0   2016-08-31 CRAN (R 3.3.0)                   
##  munsell        0.4.3   2016-02-13 CRAN (R 3.3.0)                   
##  nlme           3.1-128 2016-05-10 CRAN (R 3.3.1)                   
##  nloptr         1.0.4   2014-08-04 cran (@1.0.4)                    
##  nnet           7.3-12  2016-02-02 CRAN (R 3.3.1)                   
##  pbkrtest       0.4-6   2016-01-27 CRAN (R 3.3.0)                   
##  plyr           1.8.4   2016-06-08 CRAN (R 3.3.0)                   
##  profvis      * 0.3.2   2016-05-19 CRAN (R 3.3.0)                   
##  psych          1.6.9   2016-09-17 cran (@1.6.9)                    
##  purrr        * 0.2.2   2016-06-18 CRAN (R 3.3.0)                   
##  quantreg       5.29    2016-09-04 CRAN (R 3.3.0)                   
##  R6             2.2.0   2016-10-05 CRAN (R 3.3.0)                   
##  randomForest * 4.6-12  2015-10-07 CRAN (R 3.3.0)                   
##  rcfss        * 0.1.0   2016-10-06 local                            
##  Rcpp           0.12.7  2016-09-05 cran (@0.12.7)                   
##  readr        * 1.0.0   2016-08-03 CRAN (R 3.3.0)                   
##  reshape2       1.4.2   2016-10-22 CRAN (R 3.3.0)                   
##  rmarkdown      1.1     2016-10-16 CRAN (R 3.3.1)                   
##  rstudioapi     0.6     2016-06-27 CRAN (R 3.3.0)                   
##  scales         0.4.1   2016-11-09 CRAN (R 3.3.1)                   
##  SparseM        1.72    2016-09-06 CRAN (R 3.3.0)                   
##  stringi        1.1.2   2016-10-01 CRAN (R 3.3.0)                   
##  stringr        1.1.0   2016-08-19 cran (@1.1.0)                    
##  tibble       * 1.2     2016-08-26 cran (@1.2)                      
##  tidyr        * 0.6.0   2016-08-12 CRAN (R 3.3.0)                   
##  tidyverse    * 1.0.0   2016-09-09 CRAN (R 3.3.0)                   
##  tree         * 1.0-37  2016-01-21 CRAN (R 3.3.0)                   
##  withr          1.0.2   2016-06-20 CRAN (R 3.3.0)                   
##  yaml           2.1.13  2014-06-12 CRAN (R 3.3.0)</code></pre>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The actual value you use is irrelevant. Just be sure to set it in the script, otherwise R will randomly pick one each time you start a new session.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Because <code>modelr</code> is a work in progress, it doesn’t always play nicely with base R functions. Note how I refer directly to the test<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p><code>pretty = 0</code> cleans up the formatting of the text some.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Specifically passenger class, gender, age, number of sibling/spouses aboard, number of parents/children aboard, fare, and port of embarkation.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>Because behind the scenes, <code>caret</code> is simply using the <code>glm</code> function to train the model.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p><a href="https://topepo.github.io/caret/train-models-by-tag.html#random-forest">There are many packages that use algorithms to estimate random forests.</a> They all do the same basic thing, though with some notable differences. The <code>rf</code> method is generally popular, so I use it here.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>Remember that it was not generated by the <code>tree</code> library, but instead by a function in <code>randomForest</code>. Because of that we cannot just call <code>plot(age_sex_rf$finalModel)</code>.<a href="#fnref7">↩</a></p></li>
</ol>
</div>

<p>This work is licensed under the  <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 Creative Commons License</a>.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
