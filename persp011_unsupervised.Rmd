---
title: "Statistical learning: unsupervised learning"
author: "MACS 30100 - Perspectives on Computational Modeling"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Objectives

* Distinguish unsupervised learning from supervised learning
* Demonstrate how to estimate and interpret $K$-means clustering
* Demonstrate how to estimate and interpret hierarchical clustering
* Define key terms for unsupervised text analysis
* Explain the purpose of dimension reduction techniques
* Demonstrate how to estimate and interpret principal components analysis
* Implement latent semantic analysis using text data
* Introduce Latent Dirchlet allocation and apply it to example data

```{r packages, cache = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(forcats)
library(broom)
library(modelr)
library(stringr)
library(ISLR)
library(titanic)
library(rcfss)
library(grid)
library(gridExtra)
library(ggdendro)
library(tidytext)
library(tm)
library(topicmodels)

options(digits = 3)
set.seed(1234)
theme_set(theme_minimal())
```

# Unsupervised learning

**Supervised learning** methods are used in situations where you have a set of $p$ predictors $X_1, X_2, \dots, X_p$ measured on $n$ observations and you want to explain or predict a response $Y$ also measured on those $n$ observations. **Unsupervised learning** methods differ in that you have a set of $p$ predictors $X_1, X_2, \dots, X_p$ measured on $n$ observations, *but you do not have an associated response variable $Y$*. Instead you want to explore the structure and grouping of the observations. Typically unsupervised learning is a more **exploratory process** because you have no end result you are specifically looking for, and you have no measuring stick to decide if you have the "right" results. At least with supervised learning, you can assess the accuracy or fit of the model and decide how well it performs or compare it to other models. Techinques such as cross-validation and resampling methods can be used to ensure we are not overfitting the training data. But with unsupervised learning this is impossible, because you don't know the "true" answer.

There are a wide range of unsupervised learning methods, and their use is governed by the type of research question you have. Here we will examine three types of methods: clustering, dimension reduction, and topic modeling.

# Clustering methods

**Clustering** refers to a set of techniques for finding subgroups within a dataset, called **clusters**. The goal is to partition the dataset into similar and distinct groups so that observations in each group are similar to one another, while each group is distinctive and dissimilar to the other groups.

## $K$-means clustering

$K$-means clustering is one approach to identifying distinct clusters within data. First we specify the number of $K$ clusters we want to estimate in the data, then assign each observation to precisely one of those $K$ clusters.

```{r kmeans}
# generate data
x <- data_frame(x1 = rnorm(150) + 3,
                x2 = rnorm(150) - 4)

# estimate k clusters
x.out <- x %>%
  mutate(k2 = kmeans(x, 2, nstart = 20)$cluster,
         k3 = kmeans(x, 3, nstart = 20)$cluster,
         k4 = kmeans(x, 4, nstart = 20)$cluster)

# plot clusters
x.out %>%
  gather(K, pred, k2:k4) %>%
  mutate(K = parse_numeric(K),
         pred = factor(pred)) %>%
  ggplot(aes(x1, x2, color = pred)) +
  facet_grid(. ~ K, labeller = label_both) +
  geom_point() +
  theme(legend.position = "none")
```

Let $C_1, C_2, \dots, C_K$ denote sets containing the indicies of the observations in each cluster. $K$-means clustering defines a good cluster as one for which within-cluster variation is as small as possible. So we want to minimize the within-cluster variation defined by some function $W(C_K)$ that identifies variation:

$$\min_{C_1, C_2, \dots, C_K} \left\{ \sum_{k = 1}^K W(C_k) \right\}$$

so that the overall amount of within-cluster variation across all the clusters is as small as possible. We can define within-cluster variation in several different ways, but a standard approach uses **squared Euclidean distance**:

$$W(C_k) = \frac{1}{|C_k|} \sum_{i,i' \in C_k} \sum_{j = 1}^p (x_{ij} - x_{i'j})^2$$

where the within-cluster variation is the sum of all of the pairwise squared Euclidean distances between the observations in the $k$th cluster, divided by the number of observations in the $k$th cluster. Unfortunately we cannot evaluate every possible cluster combination because there are almost $K^n$ ways to partition $n$ observations into $K$ clusters. Instead, we will settle for a **good enough** approach; that is, rather than finding the global optimum for the optimization problem we will instead estimate the local optimum.

To do this we employ an iterative process. First we randomly assign each observation to one of the $K$ clusters. This will be the initial cluster assignment for each observation. Then we iterate over the cluster assignments:

1. For each of the $K$ clusters, compute the cluster **centroid**, or the vector of $p$ feature means for the observations in the $k$th cluster.
1. Assign each observation to the cluster whose centroid is closest as defined by Euclidean distance.

Each time we do this observations will move around and join different clusters because the initial assignments were made entirely at random. As we iterate over this process, the cluster assignments will become more stable and eventually stop entirely. This is when we reach the local optimum. Since the local optimum is based on the initial (random) assignments, we run this algorithm multiple times from different random starting configurations and select the best solution (the one with the lowest total within-cluster variation).

```{r kmeans-sim-start}
kmean.out <- rerun(6, kmeans(x, 3, nstart = 1))

kmean.out %>%
  map_df(~ as_tibble(.$cluster), .id = "id") %>%
  bind_cols(bind_rows(x,x,x,x,x,x)) %>%
  mutate(withinss = rep(map_chr(kmean.out, ~ .$tot.withinss), each = nrow(x)),
         value = factor(value)) %>%
  ggplot(aes(x1, x2, color = value)) +
  facet_wrap(~ id + withinss, ncol = 3, labeller = label_wrap_gen(multi_line = FALSE)) +
  geom_point() +
  theme(legend.position = "none")
```

## Hierarchical clustering

A drawback to $K$-means clustering is that it requires you to specify in advance the number of clusters in the data. Since this is unsupervised learning, you don't really know the actual number of clusters. Depending on the major features of the data, different values of $K$ could produce equally meaningful results. Imagine if your data contains observations on individuals, split between males and females as well as split between Americans, Canadians, and South Africans. $K=2$ would potentially cluster the observations based on gender, whereas $K=3$ could cluster based on nationality. Which is "right"? Well, both of them. It depends on the features of the data in which you are most interested.

**Hierarchical clustering** is an alternative approach that does not require us to fix the number of clusters *a priori*. It also produces a visual interpretation of the clusters using tree-based representations called **dendrograms**. Here let's review how to interpret dendrograms generated from **bottom-up** clustering.

## Interpreting dendrograms

```{r dendro}
# generate data
x <- data_frame(x1 = rnorm(50) + 3,
                x2 = rnorm(50) - 4,
                y = ifelse(x1 < 3, "1",
                           ifelse(x2 > -4, "2", "3")))

ggplot(x, aes(x1, x2, color = y)) +
  geom_point() +
  labs(title = "Simulated data") +
  theme(legend.position = "none")

# estimate hierarchical cluster
hc.complete <- hclust(dist(x), method = "complete")

# plot
ggdendrogram(hc.complete)
```

Here we plot a dendrogram using simulated data, consisting of `r nrow(x)` observations in two-dimensional space. We simulate three natural classes in the data, but in the real-world you would not know that. Suppose that we observe the data without class labels and want to perform hierarchical clustering on the data. The result is plotted above.

Like with decision trees, we have **leafs** and **branches**. Rather than reading the dendrogram from the top-down, we read it from the bottom-up. Each observation is represented by a leaf. As we move up the tree, leaves **fuse** into branches. These are observations that are similar to one another, similarity generally being defined by Euclidean distance. Observations that fuse together near the bottom of the tree are generally similar to one another, whereas observations that fuse near the top of the tree are dissimilar. The height on the graph where the fusion occurs defines how similar or dissimilar any two observations are. The larger the value, the more dissimilar they are. Rather than paying attention to the proximity of observations along the horizontal axis, we should instead focus on the location of observations relative to the vertical axis.

From this dendrogram we can assign observations to clusters. To generate clusters, we make a horizontal cut somewhere on the dendrogram, severing the tree into multiple subtrees. The height of the cut will dictate how many clusters are formed. For instance, cutting the tree at a height of 4 splits the dendrogram into two subtrees, and therefore two clusters:

```{r dendro-cut-4}
h <- 4
# extract dendro data
hcdata <- dendro_data(hc.complete)
hclabs <- label(hcdata) %>%
  left_join(data_frame(label = as.factor(seq.int(nrow(x))),
                       cl = as.factor(cutree(hc.complete, h = h))))

# plot dendrogram
ggdendrogram(hc.complete, labels = FALSE) +
  geom_text(data = hclabs,
            aes(label = label, x = x, y = 0, color = cl),
            vjust = .5, angle = 90) +
  geom_hline(yintercept = h, linetype = 2) +
  theme(axis.text.x = element_blank(),
        legend.position = "none")
```

Alternatively we could split it lower, for instance at 3:

```{r dendro-cut-3}
h <- 3
# extract dendro data
hcdata <- dendro_data(hc.complete)
hclabs <- label(hcdata) %>%
  left_join(data_frame(label = as.factor(seq.int(nrow(x))),
                       cl = as.factor(cutree(hc.complete, h = h))))

# plot dendrogram
ggdendrogram(hc.complete, labels = FALSE) +
  geom_text(data = hclabs,
            aes(label = label, x = x, y = 0, color = cl),
            vjust = .5, angle = 90) +
  geom_hline(yintercept = h, linetype = 2) +
  theme(axis.text.x = element_blank(),
        legend.position = "none")
```

Generating a larger number of clusters. Determining the optimal number of clusters is generally left to the discretion of the researcher based on the height of the fusions and desired number of clusters. Again, this is unsupervised learning **so there is no single correct number of clusters**.

## Estimating hierarchical clusters

The general procedure for estimating hierarchical clusters is relatively straightforward:

1. Assume each $n$ observation is its own cluster. Calculate the $\binom{n}{2} = \frac{n(n-1)}{2}$ pairwise dissimilarities between each observation.^[Again, using Euclidean distance.]
1. For $i=n, n-1, \dots, 2$:
    1. Compare all pairwise inter-cluster dissimilarities among the $i$ clusters and identify the pair of clusters that are least dissimilar (i.e. most dissimilar). Fuse these two clusters. The dissimilarity between these two clusters determines the height in the dendrogram where the fusion should be placed.
    1. Compute the new pairwise inter-cluster dissimilarities among the $i-1$ clusters

This process is continued until there is only a single cluster remaining. The only complication is how to measure dissimilarities between clusters once they contain more than one observation. Previously we used pairwise dissimilarities of the observations, but how do we proceed with multiple observations? There are four major approaches to defining dissimilarity between clusters, also called **linkage**:

1. **Complete** - compute all pairwise dissimilarities between observations in cluster A and cluster B and record the largest of these dissimilarities.
1. **Single** - compute all pairwise dissimilarities between observations in cluster A and cluster B and record the smallest of these dissimilarities.
1. **Average** - compute all pairwise dissimilarities between observations in cluster A and cluster B and record the average of these dissimilarities.
1. **Centroid** - compute the dissimilarity between the centroid (a mean vector of length $p$) for cluster A and cluster B.

```{r dendro-compare-linkage}
hc.complete <- hclust(dist(x), method = "complete")
hc.single <- hclust(dist(x), method = "single")
hc.average <- hclust(dist(x), method = "average")

# plot
ggdendrogram(hc.complete) +
  labs(title = "Complete linkage")
ggdendrogram(hc.single) +
  labs(title = "Single linkage")
ggdendrogram(hc.average) +
  labs(title = "Average linkage")
```

# Dimension reduction

Another possible application of unsupervised learning is to reduce the number of dimensions in a dataset. There are a couple reasons you might do this:

1. You want to visualize the data but you have a lot of variables. You could generate something like a scatterplot matrix, but once you have more than a handful of variables even these become difficult to interpret.
1. You want to use the variables in a supervised learning framework, but reduce the total number of predictors to make the estimation more efficient.

In either case, the goal is to reduce the dimensionality of the data by identifying a smaller number of representative variables that collectively explain most of the variability in the original dataset. There are several methods available for performing such a task. First we will examine an example of applying dimension reduction techniques to summarize roll-call voting in the United States

## Application: DW-NOMINATE

In the 1990s, dimension reduction techniques revolutionized the study of U.S. legislative politics. Measuring the ideology of legislators prior to this point was difficult because there was no method for locating legislators along an ideological spectrum (liberal-conservative) in a manner that allowed comparisons over time. That is, how liberal was a Democrat in 1870 compared to a Democrat in 1995? Additionally, supposed you wanted to predict how a legislator would vote on a given bill. Roll-call votes record individual legislator behavior, so you could use past votes to predict future ones. But there have been tens of thousands of recorded votes over the course of the U.S. Congress. Even in a given term of Congress, the Senate may cast hundreds of recorded votes. But there are only 100 senators (at present), and you cannot estimate a regression model when your number of predictors $p$ is larger than your number of observations $n$. We need some method for reducing the dimensionality of this data to a handful of variables which explain as much of the variation in roll-call voting as possible.

**Multidimensional scaling techniques** can be used to perform this feat. The technical details of this specific application are beyond the scope of this class, but Keith Poole and Howard Rosenthal developed a specific procedure called [NOMINATE](http://voteview.org/) to reduce the dimensionality of the data. Rather than using $p$ predictors to explain or predict individual legislator's roll-call votes, where $p$ is the total number of roll-call votes in the recorded history of the U.S. Congress, Poole and Rosenthal examined the similarity of legislator's votes in a given session of Congress and over time to identify two major dimensions to roll-call voting in the U.S. Congress. That is, roll-call votes in Congress can generally be explained by two variables that can be estimated for every past and present member of Congress. The two dimensions do not have any inherent substantive interpretation, but by graphically examining the two dimensions, it becomes clear that they represent two specific factors in legislative voting:

1. First dimension - political ideology. This dimension appears to represent political ideology on the liberal-conservative spectrum. Positive values on this dimension refer to increasingly conservative voting patterns, and negative values refer to increasingly liberal voting patterns.
1. Second dimension - "issue of the day". This dimension appears to pick up on attitudes that are salient at different points in the nation's history. They could be regional differences (Southern vs. non-Southern states), or attitudes towards specific policy issues (i.e. slavery).

This data can be used for a wide range of research questions. For example, we could use it to assess the degree of polarization in the U.S. Congress over time:

![[Voteview.org](http://voteview.org/political_polarization_2015.htm)](http://voteview.org/images/png/house_party_means_1879-2015.png)

![[Voteview.org](http://voteview.org/political_polarization_2015.htm)](http://voteview.org/images/png/house_party_means_1879-2015_2nd.png)

![[Voteview.org](http://voteview.org/political_polarization_2015.htm)](http://voteview.org/images/png/senate_party_means_1879-2015.png)

![[Voteview.org](http://voteview.org/political_polarization_2015.htm)](http://voteview.org/images/png/senate_party_means_1879-2015_2nd.png)


## Principal components analysis

**Principal components analysis** (PCA) is a basic technique for dimension reduction. The goal is to find a low-dimensional representation of the data that contains as much as possible of the variation. Each dimension is a linear combination of the $p$ variables.

The **first principal component** of a set of variables $X_1, X_2, \dots, X_p$ is the normalized linear combination of the features

$$Z_1 = \phi_{11}X_1 + \phi_{21}X_2 + \dots + \phi_{p1}X_p$$

that has the largest variance. By normalizing the features, we mean

$$\sum_{j=1}^p \phi_{j1}^2 = 1$$

The elements of $\phi_{11}, \dots, \phi_{p1}$ are known as the **loadings** of the first principal component, and combined together they form the principal component loading vector $\phi_1 = (\phi_{11}, \dots, \phi_{p1})^T$.

Estimating the first principal component follows the following procedure. Since we are only interested in variance, we assume each column in the $n\times p$ data set $\mathbf{X}$ has mean zero and look for the linear combination of the sample column values of the form

$$z_{i1} = \phi_{11}x_{i1} + \phi_{21}x_{i2} + \dots + \phi_{p1} x_{ip}$$

that has the largest sample variance, subject to the constraint $$\sum_{j=1}^p \phi_{j1}^2 = 1$$.

The result of this optimization problem is a loading vector $\phi_1$ with elements $\phi_{11}, \phi_{21}, \dots, \phi_{p1}$ that defines a direction in feature space along which the data vary the most. We can estimate the second, third, and $n$th principal components using a similar process. The second principal component is the linear combination of $X_1, X_2, \dots, X_p$ that has the maximum variance out of all linear combinations that are uncorrelated with $Z_1$.

Once we estimate the principal components, we can plot them against each other in order to produce a low-dimensional visualization of the data. Let's look at the use of PCA on the `USArrests` dataset, reproduced from **An Introduction to Statistical Learning**.

```{r pca-usarrests}
pr.out <- prcomp(USArrests, scale = TRUE)

pr.out$rotation
biplot(pr.out, scale = 0, cex = .6)
```

The principal component score vectors have length $n=50$ and the principal component loading vectors have length $p=4$. The biplot visualizes the relationship between the first two principal components for the dataset, including both the scores and the loading vectors. The first principal component places approximately equal weight on murder, assault, and rape. We can tell this because these vectors' length on the first principal component dimension are roughly the same, whereas the length for urban population is smaller. Conversely, the second principal component (the vertical axis) places more emphasis on urban population. Intuitively this makes sense because murder, assault, and rape are all measures of violent crime, and it makes sense that they should be correlated with one another (i.e. states with high murder rates are likely to have high rates of rape as well).

We can also interpret the plot for individual states based on their positions along the two dimensions. States with large positive values on the first principal component have high crime rates while states with large negative values have low crime rates; states with large positive values on the second principal component have high levels of urbanization while states with large negative values have low levels of urbanization.

## Latent semantic analysis

Text documents can be utilized in computational text analysis under the **bag of words** approach. Documents are represented as vectors, and each variable counts the frequency a word appears in a given document. While we throw away information such as word order, we can represent the information in a mathematical fashion using a matrix. Each row represents a single document, and each column is a different word:

```
 a abandoned abc ability able about above abroad absorbed absorbing abstract
43         0   0       0    0    10     0      0        0         0        1
```

These vectors can be very large depending on the dictionary, or the number of unique words in the dataset. These bag-of-words vectors have three important properties:

1. They are **sparse**. Most entries in the matrix are zero.
1. A small number of words appear frequently across all documents. These are typically uninformative words called **stop words** that inform us nothing about the document (e.g. "a", "an", "at", "of", "or").
1. Other than these words, the other words in the dataset are correlated with some words but not others. Words typically come together in related bunches.

Considering these three properties, we probably don't need to keep all of the words. Instead, we could reduce the dimensionality of the data by projecting the larger dataset into a smaller feature space with fewer dimensions that summarize most of the variation in the data. Each dimension would represent a set of correlated words. Principal component analysis can be used for precisely this task.

In a textual context, this process is known as **latent semantic analysis**. By identifying words that are closely related to one another, by searching for just one of the terms we can find documents that use not only that specific term but other similar ones.

### Interpretation: `NYTimes`

```{r nytimes}
# get NYTimes data
load("data/pca-examples.Rdata")
```

Let's look at an application of LSA. `nyt.frame` contains a document-term matrix of a random sample of stories from the New York Times: 57 stories are about art, and 45 are about music. The first column identifies the topic of the article, and each remaining cell contains a frequency count of the number of times each word appeared in that article.^[Actually it contains the [term frequency-inverse document frequency](http://cfss.uchicago.edu/text001_tidytext.html#assessing_word_and_document_frequency) which downweights words that appear frequently across many documents. This is one method for guarding against any biases caused by stop words.] The resulting data frame contains `r nrow(nyt.frame)` rows and `r ncol(nyt.frame)` columns.

Some examples of words appearing in these articles:

```{r nytimes-words}
colnames(nyt.frame)[sample(ncol(nyt.frame),30)]
```

We can estimate the LSA using the standard PCA procedure:

```{r nytimes-pca}
# Omit the first column of class labels
nyt.pca <- prcomp(nyt.frame[,-1])

# Extract the actual component directions/weights for ease of reference
nyt.latent.sem <- nyt.pca$rotation
```

Because of the number of variables in the data set, it would be difficult to visualize this data using a biplot. Instead, we can extract the biggest components for the first principal component:

##### First PCA: positive loadings

```{r nytimes-1pos}
signif(sort(nyt.latent.sem[,1],decreasing=TRUE)[1:30],2)
```

##### First PCA: negative loadings

```{r nytimes-1neg}
signif(sort(nyt.latent.sem[,1],decreasing=FALSE)[1:30],2)
```

These are the 30 words with the largest positive and negative loadings on the first principal component. The words on the positive loading seem associated with music, whereas the words on teh negative loading are more strongly associated with art.

##### Second PCA: positive loadings

```{r nytimes-2pos}
signif(sort(nyt.latent.sem[,2],decreasing=TRUE)[1:30],2)
```

##### Second PCA: negative loadings

```{r nytimes-2neg}
signif(sort(nyt.latent.sem[,2],decreasing=FALSE)[1:30],2)
```

Here the positive words are about art, but more focused on acquiring and trading ("collections", "dealers", "donations"). We could perform similar analysis on each of the `r ncol(nyt.latent.sem)` principal components.

```{r nytimes-plot-dim}
cbind(type = nyt.frame$class.labels, as_tibble(nyt.pca$x[,1:2])) %>%
  mutate(type = factor(type, levels = c("art", "music"),
                       labels = c("A", "M"))) %>%
  ggplot(aes(PC1, PC2, label = type, color = type)) +
  geom_text() +
  theme(legend.position = "none")
```

Even after throwing away the vast majority of information in the original data set, the first two principal components still strongly distinguish the two types of articles.


# Topic modeling


## Latent Direchet allocation

## Interpretation: Associated Press


# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```




