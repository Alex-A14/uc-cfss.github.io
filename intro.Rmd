---
title: "Introduction to Computing for the Social Sciences"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

# Objectives

* Introduce myself
* Identify major course objectives
* Identify course logistics
* Introduce basic principles of data science workflow and programming
* Explain how to get started in R

# Introduction to the course

## Me (Dr. Benjamin Soltoff)

I am a lecturer in the [Masters in Computational Social Science](http://macss.uchicago.edu) program. I earned my PhD in political science from [Penn State University](http://polisci.la.psu.edu/) in 2015. [My research interests](https://www.bensoltoff.com) focus on judicial politics, state courts, and agenda-setting. Methodologically I am interested in statistical learning and text analysis. I was first drawn to programming in grad school, starting out in [Stata](http://www.stata.com/) and eventually making the transition to [R](https://www.r-project.org/) and [Python](https://www.python.org/). I learned these programming languages out of necessity - I needed to process, analyze, and code tens of thousands of judicial opinions and extract key information into a tabular format. I am not a computer scientist. I am a social scientist who uses programming and computational tools to answer my research questions.

## Joshua Masoulf

* PhD student in sociology
* TA for the course
* Office hours available on the course site

## Course website

Go to https://uc-cfss.github.io for the course site. This contains the course objectives, required readings, schedules, slides, etc.

## Enrollment in the course

Identifying demand for a course at UChicago appears to be a challenge. I experienced significant attrition last term - of 36 interested students for 25 seats, I ended the term with 16 enrolled students. Because matching supply with demand was so difficult, this term I am implementing an easier policy: **anyone who wishes to take the course may take it**. If you were unable to register because of the seat limit, email me to request an override.

## Course objectives

**The goal of this course is to teach you basic computational skills and provide you with the means to learn what you need to know for your own research.** I start from the perspective that you want to analyze data, and *programming is a means to that end*. You will not become an expert programmer - that is a given. We will cover many different topics in this course, including:

* Elementary programming techniques (e.g. loops, conditional statements, functions)
* Writing reusable, interpretable code
* Problem-solving - debugging programs for errors
* Obtaining, importing, and munging data from a variety of sources
* Performing statistical analysis
* Visualizing information
* Creating interactive reports
* Generating reproducible research

## How we will do this

> Teach a (wo)man to fish

This is a hands-on class. You will learn by writing programs and analysis. Don't fear the word "program". A program can be as simple as:

```{r}
print("Hello world")
```

> One line of code, and it performs a very specific task (print the phrase "Hello world" to the screen)

More typically, your programs will perform statistical and graphical analysis on data of a variety of forms. But we will start small to build our way up to there.

Class sessions will include a combination of lecture and live-coding. **You need to bring a laptop to class to follow along**, but all class materials (including slides and notes) will be made available before/after class for your review. The emphasis of the class is on application and learning how to implement different computational techniques. However we will sometimes read and discuss examples of interesting and relevant scholarly research that demonstrates the capabilities and range of computational social science.

Lab sessions will be held each Monday. I strongly encourage you to attend these sessions. Myself or the TA will be available in the lab to assist you as you practice specific skills or encounter problems completing the homework assignments.

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">15 min rule: when stuck, you HAVE to try on your own for 15 min; after 15 min, you HAVE to ask for help.- Brain AMA <a href="https://t.co/MS7FnjXoGH">pic.twitter.com/MS7FnjXoGH</a></p>&mdash; Rachel Thomas (@math_rachel) <a href="https://twitter.com/math_rachel/status/764931533383749632">August 14, 2016</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

We will follow the *15 minute rule* in this class. If you encounter a problem in your assignments, spend 15 minutes troubleshooting the problem on your own. Make use of [Google](https://www.google.com) and [StackOverflow](http://stackoverflow.com/) to resolve the error. However, if after 15 minutes you still cannot solve the problem, **ask for help**. We will use [GitHub to ask and answer class-related questions](https://github.com/uc-cfss/Discussion).

    * That said, [Google](https://www.google.com) is going to be your strongest ally. Also [StackOverflow](http://stackoverflow.com/). Please attempt to troubleshoot your problem on your own before asking for advice.
* Plagiarism - I am trying to balance two competing perspectives:
    1. Collaboration is good - researchers usually collaborate with one another on projects. Developers work in teams to write programs. Why reinvent the wheel when it has already been done?
    2. Collaboration is cheating - this is academia. You are expected to complete your own work. If you copy from someone else, how do I know you actually learned the material?
    * The point is that collaboration in this class is good -- *to a point*. You are always, unless otherwise noted, expected to write and submit your own work. You should not blindly copy from your peers. You should not copy large chunks of code from the internet.
    * That said, using the internet to debug programs is fine. Asking a classmate to help you debug your program is fine (the key phrase is *help you*, not do it for you).
    * Bottom line - if you don't understand what the program is doing and are not prepared to explain it in detail, you should not submit it.

### Evaluations

* Weekly programming assignments (70%)
    * Practicing what we discuss in class each week
    * Assignments distributed on Wednesday, due by the beginning of class the following Monday
    * Assignments will initially come with starter code, or an initial version of the program where you need to fill in the blanks to make it work. As the quarter moves on and your skills become more developed, I will provide less initial help.
    * Lab sessions will be held on Wednesdays at 5pm
* Final project (30%)
    * Culmination of coding skills
    * A computationally-driven research project on a topic of your choosing
    * Completely reproducible
    * Web-based presentation
    
# Computational social science workflow

Drawn heavily from [@hadley2016]

![Data science workflow](../images/data-science.png)

Computationally driven research follows a specific workflow. This is the ideal - in this course, I want to illustrate and explain to you why each stage is important and how to do it.

## Import

First you need to get your data into whatever software package you will use to analyze it. Most of us are familiar with data stored in flat files (e.g. spreadsheets). However a lot of interesting data cannot be obtained in a single specific and simple format. Information you need could be stored in a database, or a web API, or even (god forbid) *printed books*. You need to know how to convert/extract information into your software package of choice.

## Tidy

Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored. In brief, when your data is tidy, each column is a variable, and each row is an observation. Tidy data is important because the consistent structure lets you focus your struggle on questions about the data, not fighting to get the data into the right form for different functions. Contrary to what you might expect, much of a researcher's time is spent wrangling and cleaning data into a tidy format for analysis. While not glamorous, tidying is an important stage.

## Transform

Once you have tidy data, a common first step is to transform it. You may zero in on a subset of data, add new variables that are functions of existing variables, or calculate a set of summary statistics.

## Visualize

Humans love to visualize information. A good visualisation will show you things that you did not expect, or raise new questions about the data. A good visualisation might also hint that you’re asking the wrong question, or you need to collect different data. Visualisations can surprise you, but don’t scale particularly well because they require a human to interpret them.

## Model

Models are complementary tools to visualisation. Once you have made your questions sufficiently precise, you can use a model to answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. Even when they don’t, it’s usually cheaper to buy more computers than it is to buy more brains! But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise you.

## Communicate

The last step of data science is communication, an absolutely critical part of any data analysis project. It doesn’t matter how well your models and visualisation have led you to understand the data unless you can also communicate your results to others.

## Programming

Surrounding all these tools is programming. Programming is a cross-cutting tool that you use in every part of the project. You don’t need to be an expert programmer to be a data scientist, but learning more about programming pays off because becoming a better programmer allows you to automate common tasks, and solve new problems with greater ease.

# Basic principles of programming

A *program* is a series of instructions that specifies how to perform a computation [@downey2012].

Major components to programs:

* *Input* - what is being manipulated/utilized. Typically data files from your hard drive or the internet.
* *Output* - display data or analysis on the computer, include in a paper/report, publish on the internet, etc.
* *Math* - basic or complex mathematical and statistical operations. Could be as simple as addition or subtraction, or complicated like estimating a linear regression or machine learning model.
* *Conditional execution* - check for certain conditions and only perform operations when conditions are met.
* *Repetition* - perform some action repeatedly, usually with some variation.

Virtually all programs are built using these fundamental components. Obviously the more components you implement, the more complex the program will become. The skill is in breaking up a problem into smaller parts until each part is simple enough to be computed using these basic instructions.

## GUI software

* *Graphical user interface (GUI)* - a visual way of interacting with a computer using elements such as a mouse, icons, and menus
* Think Windows and Mac OS X, or any smartphone (iOS or Android)
* Software runs using all the basic programming elements, but the end user is not aware of any of this.
    * Instructions in GUI software are *implicit* to the user
    * Programming requires the user to make instructions *explicit*

## Benefits to programming vs. GUI software [@gentzkow2014]

### Automation

#### Jane: Typical workflow of an undergraduate writing a research paper

0. Assignment -> write a report analyzing the relationship between ice cream consumption and crime rates in Chicago.
1. Jane finda data files online with total annual ice cream sales in the 50 largest U.S. cities from 2001-2010 and total numbers of crimes (by type) for the 50 largest U.S. cities from 2001-2010. She gets them as spreadsheets and downloads them to her computer, saving them in her main Downloads folder which includes everything she's downloaded over the past three years. Probably looks something like this:
![](../images/downloads_folder.png)

2. Jane opens the files in Excel.
    * Ice cream sales - frozen yogurt is not ice cream. She delete the column for frozen yogurt sales.
    * Crime data - only interested in violent crime. She delete all rows pertaining to non-violent offenses.
    * Jane saves these modified spreadsheets in a new folder created for this paper.
3. Jane opens SPSS/Stata.
    * First she imports the ice cream data file using the drop-down menu
    * Next she merges this with the crime data using the drop-down menu. There are some settings she tweaks when she does this, but Jane doesn't bother to write them down anywhere.
    * Then she creates new variables for per capita ice cream sales and per capita crime rates
    * After that, she estimates a linear regression model for the relationship between the two per capita variables
    * Finally she creates a graph plotting the relationship between these two variables
4. Jane write's her report in Google Docs
    * Huzzah! She finds a correlation between the two variables. Jane writes up her awesome findings and prepares for her A+.
    * The professor wants her results in the paper itself? Darn it. Okay, she copies and pastes her regression results and graph into the paper.
5. Jane prints her report and turns it in to the professor. Done!
6. The professor returns Jane's report a week later. He's impressed with the findings, but wants her to verify the results using new data files for ice cream sales and crime rates for 2011-2015 before he will determine her grade. Jane now has to repeat steps 1-5 all over again, but she forgot how she defined violent vs. non-violent crimes. What is she going to do?

#### Sally: How a student with computational skills would have written this paper

0. Assignment -> write a report analyzing the relationship between ice cream consumption and crime rates in Chicago.
1. Sally creates a folder specifically for this project and divides it into subfolders (e.g. `data`, `graphics`, `output`)
2. Next she finds data files online with total annual ice cream sales in the 50 largest U.S. cities from 2001-2010 and total numbers of crimes (by type) for the 50 largest U.S. cities from 2001-2010. She writes a program to download these files to the `data` subfolder.
2. Then Sally writes a program in R that opens each data file and filters/cleans the data to get the necessary variables. She saves the cleaned data as new files in the `data` folder.
3. Sally writes a separate program which imports the cleaned data files, estimates a regression model, generates a graph, and saves the regression results and graph to the output subfolder.
4. Sally creates an RMarkdown document for her report and analysis. Because RMarkdown combines both code and text, the results from step 3 are automatically added into the final report.
5. Sally submits the report to the professor.
6. The professor returns her report a week later. He's impressed with the findings, but wants her to verify the results using new data files for ice cream sales and crime rates for 2011-2015 before he will determine her grade.
    a. Sally adds the updated data files to the `data` subfolder, then rewrites her program in step 2 to combine the old and new data files.
    b. Next Sally re-runs steps 3 and 4. The analysis program accepts the new data files without issue and generates the updated regression model estimates and graph. The RMarkdown document automatically includes these revised results without any need to modify the underlying document.
    
* By automating her workflow, Sally can quickly update her results. Jane has to do all the same work again.
* Because Jane forgot how she initially filtered the data files, she cannot replicate her original results, much less update them with the new data. There is no way to definitively prove how she got her initial results.
* Even if Jane does remember, she still has to do the work of cleaning the data all over again. The more data files in a project, the more work that has to be done. Sally's program makes cleaning the data files repeatedly trivial - if she want to clean the data again, she simply runs the program again.

### Reproducibility

* Previously researchers focused on *replication* - can the results be duplicated in a new study with different data?
    * Difficult to replicate articles and research - authors don't provide enough information to easily replicate experiments and studies
    * Biases against replication - no one wants to publish it
    * Authors don't like to have their results challenged
* Reproducibility - *the idea that data analyses, and more generally, scientific claims, are published with their data and software code so that others may verify the findings and build upon them*^[[Coursera: Reproducible Research](https://www.coursera.org/learn/reproducible-research)]
    * Can quickly and easily reproduce the original results and trace back to determine how they were derived
    * More easily enables verification and replication
* Also allows the researcher to precisely replicate his/her analysis
    * Important when writing a paper, submiting it to a journal, then coming back months later for a revise and resubmit
    * You won't remember how all the code/analysis works together

### Version control

* Research projects involve lots of edits and revisions, not just in the final paper. Researchers make lots of individual decisions when writing programs and conducting analysis. Why filter this set of rows and not this other set? Do I compute traditional or robust standard errors?
* To keep track of all of these decisions and modifications, you could save multiple copies of the same file. But this is bad for two reasons.
    1. When do you decide to create a new version of the file? What do we name this file?
    2. Why did you create this new version? How can we include this information in a short file name?
* Why not cloud storage like Dropbox or Google Drive?
    * Cannot edit simultaneously - how do you combine changes?
    * No record of who made what changes
    * Cannot keep annotations describing the changes and why you made them
* Version control software (VCS) allows us to track all these changes in a detailed and comprehensive manner without resorting to 50 different copies of a file floating around.
* How VCS works:
    * Create a *repository* on a computer or server which contains all files relevant to a project.
    * Any time you want to modify a file or directory, check it out. When you are finished, check it back in.
    * The VCS tracks all changes, when the changes were made, and who made the changes.
    * If you make a change and realize you don't want to keep it, you can rollback to a previous version of the repository - or even an individual file - without hassle because the VCS already contains a log of every change.
* VCS on a local computer
![](https://git-scm.com/book/en/v2/book/01-introduction/images/local.png)

* VCS with a server
![](https://git-scm.com/book/en/v2/book/01-introduction/images/distributed.png)

### Documentation

* Programs include *comments* which are ignored by the computer but are intended for humans reading the code to understand what it does. So if you decide to ignore frozen yogurt sales, you can include this comment in your code to explain why the program drops that column from the data.
    * Comments are the *what* - what is the program doing?
    * Code is the *how* - how is the program going to do this?
* Computer code should also be *self-documenting*. That is, the code should be comprehensible whenever possible. For example, if you are creating a scatterplot of the relationship between ice cream sales and crime, don't store it in your code as `graph`. Instead, give it an intelligible name that intuitively mean something, such as `icecream_crime_scatterplot` or even `ic_crime_plot`.
* These records are included directly in the code and should be updated whenever the code is updated.
* Comments are not just for other people reading your code, but also for yourself. The goal here is to future-proof your code. That is, future you should be able to open a program and understand what the code does. If you do not include comments and/or write the code in an interpretable way, you will forget how it works.

#### Badly documented code

```{r eval=FALSE}
library(twitteR)
source("keys.R")
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
data <- userTimeline("realdonaldtrump", n = 1000)
data2 <- twListToDF(data)
write.csv(data2, "data2.csv")
```

#### Good code

```{r eval=FALSE}
# get_tweets.R
# Program to get Donald Trump tweets using Twitter API

# access Twitter API functions
library(twitteR)

# setup API authentication
source("keys.R")    # store keys privately in separate file

setup_twitter_oauth(consumer_key,
                    consumer_secret,
                    access_token,
                    access_secret)

# get 1000 most recent tweets
username <- "realdonaldtrump"
tweets <- userTimeline(username, n = 1000)

# convert to data frame
tweets_df <- twListToDF(tweets)

# write to disk
write.csv(tweets_df, "tweets_trump.csv")
```

# Get started in R

Brief demonstration of RStudio

* Open a blank window it from the dock
* Describe four main panes
    * Console - run code interactively and show output
    * Source - write code to file, then run in chunks
    * Environment - show what data or objects are currently in memory
    * Bottom-right
        * Files - show files/folders in current directory
        * Plots - show current plot
        * Help - look up documentation for different functions

# Session Info {.toc-ignore}

```{r sessioninfo}
devtools::session_info()
```

# References {.toc-ignore}
