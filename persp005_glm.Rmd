---
title: "Statistical learning: generalized linear models"
author: "MACS 30100 - Perspectives on Computational Modeling"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(cache = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Objectives

* Define the generalized linear model (GLM)
* Identify the three elements of a GLM
    * Probability distribution
    * Linear predictor
    * Link function
* Explain ordinary least squares regression as a GLM
* Explain logistic regression as a GLM
* Introduce ordinal and multinomial logistic regression for discrete variables with more than two classes
* Introduce poisson regression for count data

```{r packages, message = FALSE, warning = FALSE, cache = FALSE}
library(tidyverse)
library(modelr)
library(broom)
library(haven)
library(MASS)

options(na.action = na.warn)
set.seed(1234)

theme_set(theme_minimal())
```

# Generalized linear models

**Generalized linear models** are a flexible class of models that allow us to estimate linear regression for response variables that have error distribution models other than the normal distribution. GLMs are typically estimated via maximum likelihood estimation, though can also be estimated via generalized method of moments as well as Bayesian procedures.

# Elements of a GLM

A GLM consists of three components

1. A **random component** specifying the conditional distribution of the response variable, $Y_i$, given the values of the predictor variables in the model. Typically these distributions are a member of the [**exponential family**](https://en.wikipedia.org/wiki/Exponential_family), a set of related probability distributions.
1. A **linear predictor** that is a linear function of regressors:

    $$\eta_i = \alpha + \beta_{1}X_{i1} + \beta_{2}X_{i2} + \dots + \beta_{k}X_{ik}$$
    
    The regressors are prespecified functions of the explanatory variables. This is exactly like the form you've seen for [linear](persp003_linear_regression.html) and [logistic](persp004_logistic_regression.html) regression, because in fact linear and logistic regression are types of GLMs.
1. A **link function** $g(\cdot)$ which transforms the expectation of the response variable, $\mu_i \equiv E(Y_i)$ to the linear predictor:

    $$g(\mu_i) = \eta_i = \alpha + \beta_{1}X_{i1} + \beta_{2}X_{i2} + \dots + \beta_{k}X_{ik}$$
    
    Because the link function must also be **invertible**, we can also write it as:
    
    $$\mu_i = g^{-1}(\eta_i) = g^{-1}(\alpha + \beta_{1}X_{i1} + \beta_{2}X_{i2} + \dots + \beta_{k}X_{ik})$$

    The inverted link function is also known as the **mean function**. The purpose of the link function is to relate the linear predictor to the mean of the distribution function.
    
For any given probability distribution, there are common link functions called **canonical link functions** that are typically used in conjunction with the probability distribution in GLMs.

# GLMs and ordinary least squares regression

Previously we have discussed ordinary least squares regression in the context of **minimizing the sum of the squared errors**. This approach has a closed-form solution in the form of linear algebra:

$$\beta = (X^{'}X)^{-1}X^{'}Y$$

However we can also treat OLS as a special case of a generalized linear model.

We presume the response variable $Y$ is drawn from a Gaussian (normal) distribution with mean $\mu$ and variance $\sigma^2$:

$$Pr(Y_i = y_i) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[\frac{(Y_i - \mu)^2}{2\sigma^2}\right]$$

This is the density, or probability density function (PDF) of the variable $Y$.

* The probability that, for any one observation $i$, $Y$ will take on the particular value $y$.
* This is a function of $\mu$, the expected value of the distribution, and $\sigma^2$, the variability of the distribution around the mean.

We want to generate estimates of the parameters $\hat{\mu}$ and $\hat{\sigma^2}$ based on the data. How do we do this? Maximum likelihood estimation of course. We need to find the parameter values that maximize the **log-likelihood function**. For the normal distribution, the log-likelihood function is:

$$\ln L(\hat{\mu}, \hat{\sigma}^2 | Y) = \ln \prod_{i = 1}^{N}{\frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[\frac{(Y_i - \mu)^2}{2\sigma^2}\right]}$$

Which can also be written as:

$$\ln L(\hat{\mu}, \hat{\sigma}^2 | Y) = \sum_{i=1}^{N}{\ln\left(\frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[\frac{(Y_i - \mu)^2}{2\sigma^2}\right]\right)}$$
$$\ln L(\hat{\mu}, \hat{\sigma}^2 | Y) = -\frac{N}{2} \ln(2\pi) - \left[ \sum_{i = 1}^{N} \ln{\sigma^2 - \frac{1}{2\sigma^2}} (Y_i - \mu)^2 \right]$$

Now right now the mean of the distribution $\mu$ is a constant. We are testing different values for $\mu$ to see what optimizes the function. But more typically we want $\mu$ to be a function of some other variable $X$. We can write this as:

$$E(Y) \equiv \mu = \beta_0 + \beta_{1}X_{i}$$
$$\mathrm{Var}(Y) = \sigma^2$$

Now we just substitute this equation for the systematic mean part ($\mu$) in the previous equations:

$$\ln L(\beta_0, \beta_1, \sigma^2 | Y) = \ln \prod_{i = 1}^{N}{\frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[\frac{(Y_i - \beta_0 - \beta_{1}X_{i})^2}{2\sigma^2}\right]}$$
$$\ln L(\beta_0, \beta_1, \sigma^2 | Y) = -\frac{N}{2} \ln(2\pi) - \left[ \sum_{i = 1}^{N} \ln{\sigma^2 - \frac{1}{2\sigma^2}} (Y_i - \beta_0 - \beta_{1}X_{i})^2 \right]$$

## Connecting MLE estimator to OLS

With respect to the parameters $\{\beta_0, \beta_1, \sigma^2\}$, only the last term is important. The first one ($-\frac{N}{2} \ln(2\pi)$) is invariant with respect to the parameters of interest, and so it can be dropped using the *Fisher-Neyman Factorization Lemma*. Thus the **kernal** of the log-likelihood is:

$$-\sum_{i = 1}^{N} \ln{\sigma^2 - \frac{1}{2\sigma^2}} (Y_i - \beta_0 - \beta_{1}X_{i})^2$$

Which is eeriely familiar to the sum-of-squared-errors term, merely scaled by the variance parameter $\sigma^2$:

$$RSS = \sum_{i = 1}^{N} (Y_i - \beta_0 - \beta_{1}X_{i})^2$$

This proves the least-squares estimator of OLS $\beta$s **is the maximum likelihood estimator as well**.

## Completing the elements

So far we have the random component (a normal distribution) and a linear predictor ($\beta_0 + \beta_{1}X_{i}$). Where is the link function? We actually have it already too. Because the normal distribution already supports an infinite range of real numbers $(-, +\infty)$, the data naturally scaled to the linear predictor. What we use here is an **identity link function** that returns its argument unaltered:

$$\eta_i = g(\mu_i) = \mu_i$$
$$\mu_i = g^{-1}(\eta_i) = \eta_i$$

Therefore with ordinary least squares linear regression, we have now demonstrated that it is merely one type of generalized linear model.

# GLMs and logistic regression

The normal distribution does not properly describe  binary outcome variables which take on values of $[0,1]$, because the normal distribution allows for values along the entire real number line. This was the problem last class in trying to apply OLS to the Titanic dataset.

Instead, we assume our outcome variable $Y$ is drawn from the **binomial distribution** with probability $\pi$:

$$Pr(Y_i = y_i | \mu) = \mu^{y_i} (1 - \mu)^{(1 - y_i)}$$

This is a probability mass function (PMF):

* The probability that, for any one observation $i$, $Y$ will take on the particular value $y$.
* This is a function of $\mu$, the expected value of $Y_i$ that takes on the value 1 across repeated experiments. So $Y_i$ takes on the expected value of 1 with probability $\pi$ and 0 with probability $1 - \pi$.

Of course, the probability of the outcome $Y$ may vary systematically given known predictors. To incorporate that into the model, we need to choose a linear predictor:

$$\pi \equiv \eta = g(\mu)$$

Now in the linear context we would write something like this:

$$g(\mu_i) = \beta_0 + \beta_{1}X_i$$

However remember the problem with that approach using the Titanic data. We need to constrain the linear predictor to the probability range $[0,1]$. So in logistic regression, we cannot use the identity link function. Instead, we use the **logit link function**:

$$g(\mu_i) = \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}}$$

So to recap, for logistic regression:

* The random component is the Bernoulli distribution

    $$Pr(Y_i = y_i | \mu) = \mu^{y_i} (1 - \mu)^{(1 - y_i)}$$

* The linear predictor is:

    $$\eta_i = \beta_0 + \beta_{1}X_i$$

* The link function is the logit function:

    $$\mu_i = \frac{e^{\eta_i}}{1 + e^{\eta_i}}$$

So the likelihood for a given observation $i$ is:

$$L_i = \left( \frac{e^{\eta_i}}{1 + e^{\eta_i}} \right) ^ {Y_i} \left[ 1 - \left( \frac{e^{\eta_i}}{1 + e^{\eta_i}} \right) \right]^{1 - Y_i}$$

$$L_i = \left( \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}} \right) ^ {Y_i} \left[ 1 - \left( \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}} \right) \right]^{1 - Y_i}$$

The product of which yields the likelihood function:

$$L = \prod_{i = 1}^{N} \left( \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}} \right) ^ {Y_i} \left[ 1 - \left( \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}} \right) \right]^{1 - Y_i}$$

And therefore the log-likelihood function:

$$\ln L = \sum_{i = 1}^{N} Y_i \ln \left( \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}} \right) + (1 - Y_i) \ln \left[ 1 - \left( \frac{e^{\beta_0 + \beta_{1}X_i}}{1 + e^{\beta_0 + \beta_{1}X_i}} \right) \right]$$

Which we maximize with respect to the $\beta$s to obtain our estimated parameters, just as we would linear regression.

# Poisson regression

The **Poisson distribution** is a discrete probability function defined by:

$$P(Y_i = y_i | \mu) = \frac{\mu^{k} e^{-y_i}}{y_i!}$$

where $\mu$ is the event rate (average number of events per interval), $e$ is Euler's number, $y_i$ is an integer with range $[0, \infty]$, and $y_i!$ is the factorial of $y_i$. The mean $\mu$ and variance $\sigma$ of a Poisson distribution are the same parameterm and hence are both defined by $\mu$.

The Poisson distribution is ideal for explaining **count variables**, where the variable contains frequency counts of events occurring. This could include things such as:

* Number of persons killed by mule or horse kicks in the Prussian army per year (this was one of the first applications of the Poisson distribution)
* Number of terrorist attacks in a country per year
* Number of times an individual consumes ice cream per month

Count variables can take on non-negative integer values $\{ 0,1,2,3,\dots \}$. To estimate a poisson regression model, we need the following elements:

* The random component is the Poisson distribution:

    $$P(Y_i = y_i | \mu) = \frac{\mu^{k} e^{-y_i}}{y_i!}$$

* The linear predictor is

    $$\eta_i = \beta_0 + \beta_{1}X_i$$

* The canonical link function for the Poisson distribution is the **log function**

    $$\mu_i = \ln(\eta_i)$$

So after substituting terms, we need to estimate the parameters that maximize the log-likelihood of:

$$P(Y_i = y_i | \beta_0, \beta_1) = \frac{\ln(\beta_0 + \beta_{1}X_i)^{k} e^{-y_i}}{y_i!}$$

Let's test this method using some real-world data.

## Internal armed conflict in Africa

```{r africa-data}
africa <- read_dta("data/internal-conflict.dta")
africa
```

`africa` contains annual measures for 50 African countries from 1995-2007. The response variable `INTERNAL` contains a count of internal armed conflicts (aka civil wars and insurgencies) for each country in each year. Additional covariates include:

* Measures of latitude and longitude
* Adult literacy rate
* Number of refugees living in the country
* Lagged natural log of GDP and trade (in constant dollars)
* Lagged polity score (ranging from -10 to +10, with 10 = democracy)

```{r africa-hist}
ggplot(africa, aes(INTERNAL)) +
  geom_histogram(binwidth = 1)
```

To estimate a Poisson regression model, we use the `glm()` function with `family = "poisson"`.

```{r africa-model}
africa_mod <- glm(INTERNAL ~ literacy, family = "poisson", data = africa)
summary(africa_mod)
```

### Interpreting estimated parameters

In linear regression, the parameters tell us the estimated linear relationship between the predictor and the response variable. In logistic regression, the parameters tell us the estimated linear relationship between the predictor and the log-odds of the response variable. In Poisson regression, the parameters tell us the estimated linear relationship between the predictor and the log-count of the response variable. So the parameter for `literacy` suggests that for every one-unit increase in adult literacy, we expect the log of the number of internal armed conflicts to decrease by `r abs(coef(africa_mod)[[2]])`.

```{r africa-mod-plot}
africa %>%
  data_grid(literacy) %>%
  add_predictions(africa_mod) %>%
  ggplot(aes(literacy, pred)) +
  geom_line() +
  labs(x = "Literacy",
       y = "Predicted log-count of internal armed conflicts")
```

If we want to discuss the results in terms of the predicted count of the response variable, we need to **exponentiate** the parameters and predicted values.

```{r africa-exp}
exp(coef(africa_mod))

africa %>%
  data_grid(literacy) %>%
  add_predictions(africa_mod) %>%
  mutate(pred = exp(pred)) %>%
  ggplot(aes(literacy, pred)) +
  geom_line() +
  labs(x = "Literacy",
       y = "Predicted count of internal armed conflicts")
```

And so like with logistic regression, the relationship between predictors and the predicted log-count is linear, whereas the relationship between predictors and the predicted count is non-linear.

### Multiple predictors

Like with other GLMs, we can add multiple predictors to the model.

```{r africa-mod-big}
africa_big <- glm(INTERNAL ~ literacy + PolityLag,
                  data = africa, family = "poisson")
summary(africa_big)

africa %>%
  data_grid(literacy, PolityLag = seq(-10, 10, 4)) %>%
  add_predictions(africa_big) %>%
  mutate(pred = exp(pred),
         PolityLag = factor(PolityLag)) %>%
  ggplot(aes(literacy, pred, color = PolityLag)) +
  geom_line() +
  labs(x = "Literacy",
       y = "Predicted count of internal armed conflicts")
```

## Over or underdispersion

Recall that the Poisson distribution assumes that the mean and variance are identical. What happens if this assumption is violated? Sometimes this is the result of omitted variable bias, and we are just not accounting for all the variables that are included in the underlying data generating process. However if the variable truly has a conditional variance different from the conditional mean, then the Poisson distribution is not the most appropriate random component for the data.

There are a couple ways to test for overdispersion (variance larger than the mean) or underdispersion (variance smaller than the mean). One method is to estimate a **quasi-poisson model**. It still relies on the Poisson distribution, but introduces a dispersion parameter:

$$V(Y_i | \eta_i) = \phi \mu_i$$

If $\phi > 1$, then the conditional variance of $Y$ increases more rapidly than its mean (overdispersion). Estimating this dispersion parameter requires **quasi-likelihood** estimation which combines aspects of maximum-likelihood and method-of-moments and is beyond the scope of this class. However it yields virtually identical estimates of the parameters, and in the case of overdispersion will also yield larger (and more realistic) standard errors.

Let's estimate the original African regression model using the quasi-poisson method:

```{r africa-quasi}
africa_quasimod <- glm(INTERNAL ~ literacy, family = "quasipoisson", data = africa)
summary(africa_quasimod)
```

Here the disperson parameter is actually quite close to 1, so underdispersion doesn't appear to be a significant problem. What about for the full model?

```{r africa-quasi-big}
africa_quasibig <- glm(INTERNAL ~ literacy + PolityLag,
                  data = africa, family = "quasipoisson")
summary(africa_quasibig)
```

Still quite similar to 1.

# Ordinal logistic regression

Consider the case where your response variable contains more than 2 possible values and they are inherently ordered. Survey research typically contains answers measured using a Likert scale (strongly agree, agree, neither agree nor disagree, disagree, strongly disagree). What method should you use? Linear regression is not appropriate because the variable is discrete. Depending on the number of levels, some researchers may attempt to treat it as a continuous variable but when you only have 3-5 values this doesn't work very well. Logistic regression sounds like it would work but so far we have only used it in situations where there are two possible outcomes. Can we generalize it to a larger number of response values?

Yes we can. This is called **ordinal logistic regression**. It works in situations where you have more than two possible outcomes and they contain an inherent ordering. Let's see how this works using a simulated dataset of individuals applying to graduate school.

```{r grad-data}
dat <- read_csv("http://www.ats.ucla.edu/stat/data/ologit.csv") %>%
  mutate(apply = factor(apply, levels = 0:2,
                        labels = c("Unlikely", "Somewhat unlikely", "Very likely")))
dat
```

Each observation is an individual asked if they are going to apply to graduate school. The variables are:

* `apply` - whether or not the student intends to apply. Possible levels are "Unlikely", "Somewhat unlikely", and "Very likely", coded as 0, 1, 2 respectively.
* `pared` - binary indicator of whether at least one parent has a graduate degree.
* `public` - binary indicator of whether their undergraduate institution is a public university.
* `gpa` - student's grade point average.

We can use the `polr` function from the `MASS` library to estimate the ordered logistic regression model:

```{r grad-model}
m <- polr(apply ~ pared + public + gpa, data = dat, Hess=TRUE)
summary(m)
```

The model results give use estimated parameters and standard errors. However now we have two separate intercepts. These are the cut points used to determine 

```{r}
dat_grid <- dat %>%
  data_grid(pared, public, gpa)

dat_pred <- bind_cols(dat_grid,
                      predict(m, dat_grid, type = "probs") %>%
                        as_tibble()) %>%
  gather(outcome, prob, Unlikely:`Very likely`)

ggplot(dat_pred, aes(gpa, prob, color = outcome)) +
  geom_line() +
  facet_grid(pared ~ public, labeller = "label_both")
```



# Session Info {.toc-ignore}

```{r child='_sessioninfo.Rmd'}
```

